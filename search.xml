<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>论文精读：PhotoRealistic Style Transfer系列</title>
      <link href="/2020/02/17/photorealistic-style-transfer/"/>
      <url>/2020/02/17/photorealistic-style-transfer/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Prerequisite-Knowledge"><a href="#1-Prerequisite-Knowledge" class="headerlink" title="1. Prerequisite Knowledge"></a>1. Prerequisite Knowledge</h1><p> 此部分为预备知识，主要涉及内容如下：</p><ul><li><p>Upsampling, Uppooling, Transpose Convolution（上采样，上池化，转置卷积）</p></li><li><p>Whitening and Coloring Transformations（白化与上色）</p></li><li>Wavelet Transforms（小波变换）</li></ul><p>若是熟悉这几块内容的童鞋可以直接跳过~</p><h2 id="1-1-Upsampling-UpPooling-Transpose-Convolution"><a href="#1-1-Upsampling-UpPooling-Transpose-Convolution" class="headerlink" title="1.1 Upsampling, UpPooling, Transpose Convolution"></a>1.1 Upsampling, UpPooling, Transpose Convolution</h2><ul><li><p>Upsampling（上采样）</p><p><img src="/2020/02/17/photorealistic-style-transfer/1-1-1.jpg" alt></p><p>可以看出，上采样相当于做了一个均值池化操作的反操作。但会有一个问题，恢复的特征图中只保留了低频信息，丢失了高频信息。也就是说，生成的图像会比较平滑，严重的话会有些模糊的纹理。</p></li><li><p>Uppooling（反池化）</p><p><img src="/2020/02/17/photorealistic-style-transfer/1-1-2.jpg" alt></p></li><li><p>Transpose Convolution</p><p>转置卷积，也有叫反卷积的。这是一个比较深的学问，而不是直接掉个包就觉得懂了。为什么一副较小的特征图能变大？这里就讲个转置卷积的大概流程：首先对原始特征图进行插值得到放大后的特征图，再对插值后的特征图进行卷积得到最终的结果。那怎么插值？插什么值比较好？这都是学问，大家可以查找相关资料，也建议阅读源码。</p></li></ul><h2 id="1-2-Whitening-and-Coloring-Transformations"><a href="#1-2-Whitening-and-Coloring-Transformations" class="headerlink" title="1.2 Whitening and Coloring Transformations"></a>1.2 Whitening and Coloring Transformations</h2><blockquote><p>前言</p></blockquote><p>假设<script type="math/tex">X</script>是一个均值为0的向量，那么我们可以得到其协方差矩阵:</p><script type="math/tex; mode=display">\sum = E(XX^T)</script><p>如果说<script type="math/tex">X</script>中的向量是互相关联的，那么其协方差矩阵肯定不会是一个对角矩阵。为什么这么说？假设<script type="math/tex">X^T = [x_1 ~ x_2]</script>那么<script type="math/tex">\sum = \left[ \begin{matrix} x_1x_1 & x_1x_2 \\ x_1x_2 & x_2x_2 \end{matrix} \right]</script>。由于<script type="math/tex">X</script>均值为0，所以其该协方差举证的对角线元素为方差，而其他的元素都为向量中每个元素之间的协方差。所以，一旦<script type="math/tex">X</script>中的向量相互都不关联，那么所有的协方差都为0，那么<script type="math/tex">\sum = \left[ \begin{matrix} x_1x_1 & 0 \\ 0 & x_2x_2 \end{matrix} \right]</script>,这就是一个对角矩阵。</p><p><strong>而白化的目的就是去除原始向量中各维度的相关性，且每个维度的方差为1。</strong></p><blockquote><p>Whitening Transformation（白化）</p></blockquote><p>白化的过程分为两步：</p><ol><li><p>去除向量<script type="math/tex">X</script>中各维度之间的相关性，具体步骤如下：</p><ul><li><p>首先通过下面的办法，找到向量<script type="math/tex">X</script>协方差矩阵<script type="math/tex">\Sigma</script>的特征值和特征向量</p><script type="math/tex; mode=display">\Sigma ~ \Phi = \Phi \Lambda</script><p>其中，<script type="math/tex">\Sigma</script>是对角矩阵，其对角线上的元素都是特征值，<script type="math/tex">\Phi</script>是协方差矩阵的特征向量。推导一下，可以得到如下等式：</p><script type="math/tex; mode=display">\Phi^T ~ \Sigma ~ \Phi = \Lambda</script></li><li><p>那么，若是想得到一个新的<script type="math/tex">X</script>，其协方差矩阵为对角矩阵的话，进行如下操作</p><script type="math/tex; mode=display">Y = \Phi^TX</script></li><li><p>最后我们检验一下，<script type="math/tex">Y</script>的协方差矩阵是否为对角阵</p><script type="math/tex; mode=display">\sum_Y = E(YY^T) = \Phi^TXX^T\Phi = \Phi^T ~ \Sigma ~ \Phi = \Lambda</script></li></ul></li><li><p>使得向量<script type="math/tex">X</script>中每个维度的方差为1，即使得协方差矩阵的对角线上的元素相同，这就是完整的白化过程。</p><ul><li><p>我们定义进行第二步操作之后的<script type="math/tex">Y</script>为<script type="math/tex">W</script>，那么：</p><script type="math/tex; mode=display">W = \Lambda^{-\frac{1}{2}} ~ Y = \Lambda^{-\frac{1}{2}} ~ \Phi^T ~ X</script></li><li><p>那么最后<script type="math/tex">X</script>就被转化成了<script type="math/tex">W</script>，<script type="math/tex">W</script>的协方差真的是一个对角元素相同的对角阵了吗？我们检查一下：</p><script type="math/tex; mode=display">\begin{eqnarray} E(WW^T) &=& E(\Lambda^{-\frac{1}{2}T} ~ YY^T ~ \Lambda^{-\frac{1}{2}}) \\ &=& \Lambda^{-\frac{1}{2}T} ~ E(YY^T) ~ \Lambda^{-\frac{1}{2}} \\ &=& \Lambda^{-\frac{1}{2}T} ~ \Lambda ~ \Lambda^{-\frac{1}{2}} \\ &=& \Lambda^{-\frac{1}{2}T} ~ \Lambda^{\frac{1}{2}} ~ \Lambda^{\frac{1}{2}} ~ \Lambda^{-\frac{1}{2}} \\ &=& II \\ &=& I\end{eqnarray}</script></li></ul><p>以上就是白化的所有操作，我们借用参考资料<strong>[3]</strong>中的图像再解释一下，如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/1-2-3.jpg" alt></p><ul><li>假设<script type="math/tex">S</script>是从<script type="math/tex">X</script>中多次采样获得的n个样本<script type="math/tex">X_i,i=1,2,..., n.</script>构成列向量的矩阵</li><li>那么上图(a)显示了原始采样数据<script type="math/tex">X_1, X_2</script>这两个变量的关系散点图。可以看到散点图中的点云斜向下，那么这两个变量成负相关。</li><li>图(b)显示了，经过第一步操作后获得的变量<script type="math/tex">Y_1, Y_2</script>的关系散点图。此时，散点图中的点云垂直于<script type="math/tex">Y_1</script>轴</li><li>图(c)显示了，经过第二步操作后获得的变量<script type="math/tex">W_1, W_2</script>的关系散点图。此时，散点图中的点云形成一个圆，这说明了<script type="math/tex">W_1, W_2</script>的相关性接近于0。也就达到了白化的目的，去相关性。</li></ul></li></ol><blockquote><p>Coloring  Transformations</p></blockquote><p>如果白化懂了，那么剩下的上色就很容易理解了。因为上色本质上就是白化操作的逆操作。承接上文中的矩阵<script type="math/tex">S</script>经白化后得到的样本<script type="math/tex">W_i,i=1,2,..., n.</script>，现在想把<script type="math/tex">S</script>恢复为协方差矩阵为<script type="math/tex">\Sigma</script>，且均值为<script type="math/tex">\mu</script>的矩阵。那么也需要两步操作</p><ol><li><p>化协方差矩阵为非单位阵</p><ul><li><p>首先，对协方差矩阵<script type="math/tex">\Sigma</script>进行特征值分解</p><script type="math/tex; mode=display">\Sigma = \Phi ~ \Lambda ~\Phi^T = \Phi ~ \Lambda^{\frac{1}{2}} ~ \Lambda^{\frac{1}{2}} ~ \Phi^T</script><p>其中，<script type="math/tex">\Lambda</script>是一个具有特征值<script type="math/tex">\lambda_i</script>的对角矩阵，<script type="math/tex">\Phi</script>是相对应的特征向量矩阵。由于<script type="math/tex">\Phi^T</script>中的特征向量互为正交，所以<script type="math/tex">\Phi^{-1} = \Phi^T</script></p></li><li><p>进行白化中第二步的逆操作，得到<script type="math/tex">Y</script></p><script type="math/tex; mode=display">Y = \Lambda^{\frac{1}{2}} ~ S</script></li></ul></li><li><p>化协方差矩阵为非对角阵，进行白化第一步操作的逆操作</p><script type="math/tex; mode=display">X = \Phi Y = \Phi ~ \Lambda^{\frac{1}{2}} ~ S</script></li></ol><p>至此，上色操作就结束了~</p><h2 id="1-3-Wavelet-Transforms"><a href="#1-3-Wavelet-Transforms" class="headerlink" title="1.3 Wavelet Transforms"></a>1.3 Wavelet Transforms</h2><p>这一段内容，主要是对参考文献<strong>[1]</strong>中部分相关资料的整理。由于非通信专业，对信号学的知识理解不多，感谢原作者分享的知识~。假设现在有一个信号<script type="math/tex">X= [90 ~~ 70 ~~ 100 ~~ 70]</script>，我想要对其进行压缩该怎么办呢？</p><ul><li><p>记<script type="math/tex">x_0=90 ~~ x_1=70 ~~ x_2=100 ~~ x_3=70</script>，现在我们通过如下计算重新获取它们的值</p><script type="math/tex; mode=display">\begin{cases} x_0^{'} = \frac{x_0 + x_1}{2} = 80 \\ x_1^{'} = \frac{x_0 - x_1}{2} = 10 \end{cases}</script><p>其中，80是平均数，而10则是它们的波动范围。同理，</p><script type="math/tex; mode=display">\begin{cases} x_2^{'} = \frac{x_2 + x_3}{2} = 85 \\ x_3^{'} = \frac{x_2 - x_3}{2} = 15 \end{cases}</script><p>其中，85是平均数，而15是它们的波动范围</p></li><li><p>通过上述计算得到的<script type="math/tex">\{80 ~~ 10 ~~ 85 ~~ 15\}</script>四个值</p><ul><li>其中80和85是局部平均值，反映该信号大的总体状态，是相对平缓的值；可以认为它们是<strong>信号的低频部分，记作<script type="math/tex">L</script></strong>。</li><li>而10和15是小范围波动的值，局部变化较快，可以认为<strong>它们是信号的高频部分，记作<script type="math/tex">H</script></strong>。</li></ul></li><li><p>现在，我们将<script type="math/tex">\{80 ~~ 10 ~~ 85 ~~ 15\}</script>重新排列，得到<script type="math/tex">[80 ~~ 85 ~~ 10 ~~ 15]</script>，记作<script type="math/tex">[L ~~ L ~~ H ~~ H]</script>。然后，我们重复第一步的操作，得到</p><script type="math/tex; mode=display">\begin{cases} x_0^{''} = \frac{x_0^{'} + x_1^{'}}{2} = 82.5 \\ x_1^{''} = \frac{x_0^{'} - x_1^{'}}{2} = -2.5 \\ x_2^{''} = \frac{x_2^{'} + x_3^{'}}{2} = 12.5 \\ x_3^{''} = \frac{x_2^{'} - x_3^{'}}{2} = -2.5 \end{cases}</script></li><li><p>通过上述计算得到的<script type="math/tex">\{82.5 ~~ -2.5 ~~ 12.5 ~~ -2.5\}</script>四个值，将它们重新排列，记作<script type="math/tex">[LL ~~ LH ~~ HL ~~ HH]</script>。</p></li></ul><p>现在，我们使用线性代数的知识，利用矩阵运算完成上述操作</p><ul><li><p>对于<script type="math/tex">x_0=90 ~~ x_1=70</script>，若要得到<script type="math/tex">x_0^{'}=80 ~~ x_1^{'}=10</script>，那么需要做如下初等变换</p><script type="math/tex; mode=display">[90 ~~ 70]\left[ \begin{matrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & -\frac{1}{2} \end{matrix} \right] = [80 ~~ 10]</script><p>这个右乘的矩阵<script type="math/tex">\frac{1}{2}\left[ \begin{matrix} 1 & 1 \\ 1 & -1 \end{matrix} \right]</script>是不是很熟悉，有点接近于<script type="math/tex">Haar ~~ Wavelet</script>了。</p></li><li><p>那么</p><script type="math/tex; mode=display">[L ~~ L ~~ H ~~ H] = [90 ~~ 70 ~~ 100 ~~ 70]\left[ \begin{matrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & -\frac{1}{2} \end{matrix} \right] = [80 ~~ 85 ~~ 10 ~~ 15]</script></li></ul><p>以此类推，大家可以通过熟悉的线性代数得到最后的<script type="math/tex">[LL ~~ LH ~~ HL ~~ HH]</script>。注意，<strong>为了得到<script type="math/tex">[LL ~~ LH ~~ HL ~~ HH]</script>，每次右乘的矩阵都是正交方阵，也就是说其可逆且<script type="math/tex">AA^T=I</script>，这个性质在后面的论文中会用到</strong>。</p><h1 id="2-Universal-Style-Transfer-via-Feature-Transforms-WCT"><a href="#2-Universal-Style-Transfer-via-Feature-Transforms-WCT" class="headerlink" title="2. Universal Style Transfer via Feature Transforms(WCT)"></a>2. Universal Style Transfer via Feature Transforms(<script type="math/tex">WCT</script>)</h1><p>pass</p><h1 id="3-A-Closed-form-Solution-to-Photorealistic-Image-Stylization-PhotoWCT"><a href="#3-A-Closed-form-Solution-to-Photorealistic-Image-Stylization-PhotoWCT" class="headerlink" title="3. A Closed-form Solution to Photorealistic Image Stylization(PhotoWCT)"></a>3. A Closed-form Solution to Photorealistic Image Stylization(<script type="math/tex">PhotoWCT</script>)</h1><p>pass</p><h1 id="4-Photorealistic-Style-Transfer-viaWavelet-Transforms-WCT-2"><a href="#4-Photorealistic-Style-Transfer-viaWavelet-Transforms-WCT-2" class="headerlink" title="4. Photorealistic Style Transfer viaWavelet Transforms(WCT^2)"></a>4. Photorealistic Style Transfer viaWavelet Transforms(<script type="math/tex">WCT^2</script>)</h1><blockquote><p>针对什么问题？</p></blockquote><p>从<script type="math/tex">WCT</script>再到<script type="math/tex">PhotoWCT</script>，生成的图片效果是一次一次刷新眼界。但<script type="math/tex">WCT</script>和<script type="math/tex">PhotoWCT</script>都包含了多个Decoder，虽然会使得生成的图像保留更多的内容细节，但也会引入伪信号。此外，无论是最大池化还是上池化都会丢失原始图像的空间信息及细节。</p><blockquote><p>提出什么方法？</p></blockquote><p>本文使用Wavelet Pooling和Wavlet Unpooling取代最大池化和上池化，并构建了单一的Encoder和Decoder来取代多个Decoder的网络结构。</p><blockquote><p>效果怎么样？</p></blockquote><p>引用该文章摘要中的一句话：这是第一个也是唯一一个，可以在4.7秒内风格化<script type="math/tex">1024 \times 1024</script>分辨率图像的端到端模型，无需任何处理就能获得令人愉悦的真实感。</p><h2 id="4-1-Haar-wavelet-pooling-and-unpooling"><a href="#4-1-Haar-wavelet-pooling-and-unpooling" class="headerlink" title="4.1 Haar wavelet pooling and unpooling"></a>4.1 Haar wavelet pooling and unpooling</h2><blockquote><p>平均池化，最大池化出了什么问题？</p></blockquote><p>$WCT^2$的精髓就在于引入小s波变换充当池化和反池化层。</p><ul><li><p>从作者的提供材料里可以了解到，若将图像看作信号，若是想要恢复一个完整的信号<script type="math/tex">f</script>，那么就必须满足下列条件：</p><script type="math/tex; mode=display">\begin{cases} \tilde{\Phi}\Phi^T = I \\ z = \Phi f \\ f = \tilde{\Phi}z = \tilde{\Phi}\Phi f = f \end{cases}</script><p>​    这里面的<script type="math/tex">f</script>可以看作原始图像，<script type="math/tex">\Phi</script>看作某种新的池化操作，<script type="math/tex">\tilde{\Phi}</script>看作该新池化操作的反池化。从公式中可以看出，一旦满足<script type="math/tex">\tilde{\Phi}\Phi^T = I</script>，那么原始图像就可以被完整的恢复。</p></li><li><p>若从线性代数的角度来解释的话就是池化操作<script type="math/tex">\Phi</script>可逆，且该操作的逆为<script type="math/tex">\tilde{\Phi}</script>；或者说原始图像<script type="math/tex">f</script>经过两次基变换后能够回到原始的特征空间。</p></li><li>那么问题来了，<strong>我们熟悉的平均池化操作(Mean Pooling)或者最大池化操作(Max Pooling)存在逆操作吗？答案是不存在</strong>。一旦进行平均池化或者最大池化，新的特征空间无法保留原先特征空间的所有信息。一种更容易理解的方式就是，以平均池化为例，平均池化的逆操作只会得到数值一摸一样的特征点。</li></ul><blockquote><p>小波池化就能做到恢复所有的原始特征吗？</p></blockquote><p>答案是Yes。在<script type="math/tex">WCT^2</script>中主要使用的是<script type="math/tex">Haar ~ wavelets</script>来做池化和反池化。</p><ul><li><p>$Haar ~ wavelets$一共有四个滤波器，分别为$\{LL^T ~ LH^T HL^T HH^T \}$，其中</p><script type="math/tex; mode=display">L^T = \frac{1}{\sqrt[]{2}}[1 ~~~ 1], ~~~ H^T = \frac{1}{\sqrt[]{2}}[-1 ~~~ 1]</script><p>​    其中低通滤波器专门用来捕获图像中光滑的表面和纹理，这其实也就是图像中的大部分信息了。高通滤波器用来提取垂直、水平和对角的边缘类信息，这就对对应了图像中的细节信息。</p></li><li><p>这里，我们可以先计算出所有的滤波器，为后文滤波器的运用做铺垫。计算的结果如下（该数值经过作者源码的验证，有兴趣可以自己去查看一下源码，写的很干净）：</p><script type="math/tex; mode=display">LL^T=\frac{1}{2}\left[ \begin{matrix} 1 & 1 \\ 1 & 1 \end{matrix} \right] ~~~~~~~~~~~~~ LH^T=\frac{1}{2}\left[ \begin{matrix} -1 & 1 \\ -1 & 1 \end{matrix} \right]</script><script type="math/tex; mode=display">HL^T=\frac{1}{2}\left[ \begin{matrix} -1 & -1 \\ 1 & 1 \end{matrix} \right] ~~~~~~~~ HH^T=\frac{1}{2}\left[ \begin{matrix} 1 & -1 \\ -1 & 1 \end{matrix} \right]</script></li><li><p>作者在文中说到：<strong>通过对原始信号的镜像操作，就可以重建原信号，具体操作就是先进行转置卷积，再将所有转置卷积的结果求和</strong>。这个是小波分解运用的核心在将它之前，我们需要看一下模型的结构</p></li></ul><h2 id="4-2-Model-Architecture"><a href="#4-2-Model-Architecture" class="headerlink" title="4.2 Model Architecture"></a>4.2 Model Architecture</h2><p>本文具体的模型结构如下：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-model-1.jpg" alt></p><ul><li><p>上图清晰的展示了<script type="math/tex">WCT^2</script>的模型结构，可以发现它有以下几个特点</p><ul><li>使用<script type="math/tex">Wavelet ~ Pooling</script>以及<script type="math/tex">Wavelet ~ Unpooling</script>取代了最大池化，根据<script type="math/tex">Haar ~  Wavelet</script>小波4个滤波器的性质，所有的池化窗口应该为<script type="math/tex">2 \times 2</script>。</li><li>取消了多Decoder的模式，而分别在Encoder和Decoder中对称的做了多次的<script type="math/tex">WCT</script>操作。</li><li>因为<script type="math/tex">Haar ~ wavelets</script>有四个滤波器，那么输出的通道应该为4层。其中<script type="math/tex">LL</script>对应低频信息，<script type="math/tex">LH ~ HL ~ HH</script>对应高频信息。在Encoder中，仅有低频特征<script type="math/tex">LL</script>前传，而剩余的高频特征<script type="math/tex">LH ~ HL ~ HH</script>都先保留下来，并传递给了Decoder中与Encoder对称的地方。</li></ul></li><li><p>再结合我们上一小结提到的：通过对原始信号的镜像操作，就可以重建原信号，具体操作就是先进行转置卷积，再将所有转置卷积的结果求和。在理解这句话之前，我们先提几个问题？<strong>为什么小波变换能够保留更多的原始信息？</strong>，<strong>为什么转置之后求和，求和之后真的能恢复原始图像中的所有信息吗？</strong>现在我们进行解答</p><ul><li><p>定义原始特征层为<script type="math/tex">f</script>，那么经过池化后可以得到<script type="math/tex">LL ~ LH ~ HL ~ HH</script>的表达式如下</p><script type="math/tex; mode=display">\begin{cases} LL = LL^Tf \\ LH = LH^Tf \\ HL = HL^Tf \\ HH = HH^Tf \end{cases}</script></li><li><p>若是我们暂不考虑图像特征在前向传播中的改变，即图像在Decoder中对称的<script type="math/tex">LL</script>不变，那么进行转置卷积之后，再求和的结果是什么呢？</p><script type="math/tex; mode=display">\begin{cases} LL^{'} = LL^TLL^Tf = \frac{1}{2}\left[ \begin{matrix} 1 & 1 \\ 1 & 1 \end{matrix} \right]f \\ LH^{'} = LH^TLH^Tf = 0 \\ HL^{'} = HL^THL^Tf = 0 \\ HH^{'} = HH^THH^Tf = \frac{1}{2}\left[ \begin{matrix} 1 & -1 \\ -1 & 1 \end{matrix} \right]f \end{cases}</script><p>那么，<script type="math/tex">LL^{'} + LH^{'} + HL^{'} + HH^{'} = \frac{1}{2}\left[ \begin{matrix} 2 & 0 \\ 0 & 2 \end{matrix} \right]f = If = f</script>，没错就是这么神奇，原始信号经过两次小波变换再求和得到的就是原始信号。所以说<script type="math/tex">Haar ~ Wavelet</script>作为池化层和反池化层的卷积核能够完整的重建信号。</p></li><li><p>即使在前向传播的过程中，原始特征图的特征改变了，但原始图像的低频信息和高频信息都很好的被保留了下来，这使得生成的图像更加细腻、更加逼真。</p></li></ul><p>至此，本文中关于小波变换的部分讲解就到此结束了，下面我们还需要考虑模型的其他信息。</p></li><li><p>虽然说在Decoder中，进行转置卷积并求和有着理论上的正确性，但在实际操作过程中，作者并未采取求和的办法，而是类似于<script type="math/tex">U-Net</script>的拼接操作。这虽然会增加训练参数，但是作者说生成的图像会更加的清晰，如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-model-2.jpg" alt></p><p>不过说实话，我感觉区别不大，可能是区别大的图像没有放出来。如果能在保证理论上的正确性又能取得很好的效果就更加完美了。</p></li><li><p>在上面的模型图中，大家可以看到在Encoder和Decoder中做了3次对称的WCT操作。如果再增加WCT的次数，可能会有更好的效果，效果如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-model-3.jpg" alt></p><p>可以看出WCT次数的增加会使得生成图像的风格更加鲜艳（会发现风格图的排版未对齐，这是作者留给观众的彩蛋吗？），但是也会涉及到更多的<script type="math/tex">SVD</script>操作，因为要分解出特征图的特征向量和特征对角阵，这会导致时间的增加。此外，按道理来说，Encoder中通过跳级连接传递到Decoder中的<script type="math/tex">LH ~ HL ~ HH</script>应该也要做WCT操作，但是为了降低计算量就取消了。</p></li></ul><h2 id="4-3-Experiments"><a href="#4-3-Experiments" class="headerlink" title="4.3 Experiments"></a>4.3 Experiments</h2><ul><li><p>由于作者认为：图像中的低频部分代表的是光滑的表面和纹理，高频部分代表的是边缘。故<script type="math/tex">LL</script>应该会影响到生成图像的整体纹理或者表面，而<script type="math/tex">LH ~ HL ~ HH</script>应该影响边缘信息。故作者做了一个实验，即在恢复图像的过程中仅保留<script type="math/tex">LL</script>的信息，效果如下：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-exp-1.jpg" alt></p><ul><li>可以看到，丢失了高频信息的图像中建筑的边缘并没有被风格化，而保留了低高频信息的建筑都被风格化了，这就验证了作者的观点。</li><li>此外，这还证明了其他研究人员提出来的观点：在风格迁移中使用平均池化会使得生成的图像更加有吸引力，因为仅保留<script type="math/tex">LL</script>的操作就类似于平均池化，只不过对于<script type="math/tex">2 \times 2</script>平均池化除4取平均，而<script type="math/tex">LL^T</script>滤波器除2取平均。</li></ul></li><li><p>作者为了验证<script type="math/tex">Wavelet ~ Pooling</script>的有效性，将其和<script type="math/tex">Split ~ Pooling</script>，<script type="math/tex">Learning~ Pooling</script>做对比，效果如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-exp-2.jpg" alt></p><p>这个<script type="math/tex">Learning~ Pooling</script>的效果就惨不忍睹了，不过这个<script type="math/tex">Split ~ Pooling</script>看起来还不错。<script type="math/tex">Split ~ Pooling</script>也是使用<script type="math/tex">2 \times 2</script>的滤波器进行池化，且也能捕获全局信息，不过其表达的能力差了点，所以陆地上的草效果看起来就比较差劲。</p></li><li><p>当然，本文中最值得一提的实验就是使用<script type="math/tex">SSIM</script>以及<script type="math/tex">Gram ~ loss</script>作为验证指标，如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-exp-3.jpg" alt></p><ul><li>首先解释一下这幅图的横纵坐标。横坐标代表的是<script type="math/tex">SSIM</script>，越高说明生成的图像越好。纵坐标代表的是<script type="math/tex">Gram ~ Loss</script>，肯定是越小越好，越小就说明生成的图像和风格图的风格越接近。但注意<strong>纵轴坐标的值是递减而不是递增的</strong>，这样图中代表方法的点如果越靠近右上角，说明该方法越好。</li><li>从图中可以看到，靠近右上角基本都是<script type="math/tex">WCT^2</script>。其中<script type="math/tex">DPST</script>的闪光点在于其风格损失很小，因为它直接优化了风格损失。然后，作者在论文中加粗了一句话打脸了<script type="math/tex">PhotoWCT</script>提出使用<script type="math/tex">Unpooling</script>可以提升生成图像的质量。因为从图上来看，使用最大池化的<script type="math/tex">WCT</script>效果反而比使用<script type="math/tex">Unpooling</script>的好。所以，作者明确指出<script type="math/tex">PhotoWCT</script>生成图像质量好的主要原因还是第二阶段中经过“流形排序”处理了。</li></ul></li><li><p>最后再看一下，<script type="math/tex">WCT^2</script>的速度</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-exp-4.jpg" alt></p><p>可以看出基于迭代过程的<script type="math/tex">DBSP</script>和<script type="math/tex">WCT</script>系列完全不是一个量级的。而<script type="math/tex">PhotoWCT</script>的第二步处理需要占用大量的内存和时间。所以<script type="math/tex">WCT^2</script>的速度优势很明显。</p></li></ul><h1 id="5-References"><a href="#5-References" class="headerlink" title="5. References"></a>5. References</h1><ol><li>小波变换轻松入门（<a href="https://blog.csdn.net/jtxhe/article/details/42005685?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task）" target="_blank" rel="noopener">https://blog.csdn.net/jtxhe/article/details/42005685?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task）</a></li><li>反卷积、上采样、上池化（<a href="https://blog.csdn.net/a_a_ron/article/details/79181108）" target="_blank" rel="noopener">https://blog.csdn.net/a_a_ron/article/details/79181108）</a></li><li>白化和上色（<a href="https://www.projectrhea.org/rhea/images/1/15/Slecture_ECE662_Whitening_and_Coloring_Transforms_S14_MH.pdf）" target="_blank" rel="noopener">https://www.projectrhea.org/rhea/images/1/15/Slecture_ECE662_Whitening_and_Coloring_Transforms_S14_MH.pdf）</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文泛读2：基于Adjustable Parameters的Style Transfer</title>
      <link href="/2020/02/15/extensive-reading2/"/>
      <url>/2020/02/15/extensive-reading2/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Adjustable-Real-Time-Style-Transfer-ICLR-2020"><a href="#1-Adjustable-Real-Time-Style-Transfer-ICLR-2020" class="headerlink" title="1. Adjustable Real-Time Style Transfer (ICLR 2020)"></a>1. Adjustable Real-Time Style Transfer (ICLR 2020)</h1><blockquote><p>针对什么问题</p></blockquote><p>​    本文指出基于现有风格迁移算法训练得到的模型，只能生成固定内容结构及风格模式的Stylized Images(风格化图)。此外，选取不同的风格图作为输入，总是得调整损失函数中每一层卷积特征的权重$w$，一旦使用者对生成的结果不满意，就要重新训练该模型。</p><blockquote><p>提出什么方法</p></blockquote><p>​    本文提出一种可以在训练和测试阶段自动调整关键参数的方法，使得训练得到的风格迁移模型可以通过调整参数，生成风格迥异的Stylized Images。</p><blockquote><p>效果怎么样</p></blockquote><p>​    本文的方法通过训练一个单一的模型，和多次训练得到的不同风格迁移模型，在生成的Style Images上达到相似的效果。</p><h2 id="1-1-Model"><a href="#1-1-Model" class="headerlink" title="1.1 Model"></a>1.1 Model</h2><p>本文的模型如下图所示：</p><p><img src="/2020/02/15/extensive-reading2/1-model.jpg" alt></p><ul><li><p>模型的构造很简单，和快速风格迁移的模型类似，最大的区别在于新加入的可调节损失权重$\alpha_c, \alpha_s$，用于替代原始的快速风格迁移模型中计算内容损失$L_c$和风格损失$L_s$的的权重。</p></li><li><p>此外, <script type="math/tex">\alpha_c, \alpha_s</script>不仅加入到损失函数中，还通过训练前馈神经网络$\Lambda$并以CIN(Conditional Instance Normalization)的方式得到<script type="math/tex">\gamma_\alpha, \gamma_\alpha</script>以改变激活层的输出（如下面公式所示）。因此，在测试的时候可以通过调节<script type="math/tex">\alpha_c, \alpha_s</script>以调节生成器的输出。</p><script type="math/tex; mode=display">z = \gamma_{\alpha}(\frac{x - \mu}{\sigma}) + \beta_{\alpha}</script><script type="math/tex; mode=display">\gamma_{\alpha}, \beta_{\alpha} = \Lambda(\alpha_c, \alpha_s)</script></li><li><p>本文的实验提到了训练中$\alpha_c, \alpha_s$的取值。</p><ul><li><p>本文采用预训练的Vgg19提取特征。</p></li><li><p>本文使用$conv3$提取内容特征，故$\alpha_c = 1$，也就是说$\alpha_c$的取值固定不变。</p></li><li>本文使用$conv2, conv3, conv4$提取风格特征，所以这三层中$\alpha_s$的取值随即从$U(0, 1)$中采样。</li></ul></li></ul><h2 id="1-2-Optimization"><a href="#1-2-Optimization" class="headerlink" title="1.2 Optimization"></a>1.2 Optimization</h2><p>本文采用的就是常规的风格迁移算法损失：</p><script type="math/tex; mode=display">L_c(p) = \underset{l \in C}\sum\alpha_c^lL_c^l(p)</script><script type="math/tex; mode=display">L_s(p) = \underset{l \in S}\sum\alpha_s^lL_s^l(p)</script><p>最大的区别就是，将原始的损失权重$w$替换成了可调节的$\alpha$</p><h2 id="1-3-Experiment"><a href="#1-3-Experiment" class="headerlink" title="1.3 Experiment"></a>1.3 Experiment</h2><p>本文做的实验主要有以下几方面</p><ul><li>验证可调节参数的有效性</li><li>通过可调节参数，生成随机化的Stylized Images</li><li>和已有的BaseLine作比较</li></ul><p>下面挑选几个实验做讲解</p><p><img src="/2020/02/15/extensive-reading2/1-exp1.jpg" alt></p><ul><li>由于实验中，$\alpha_c$固定为1， $\alpha_s$的三个值可调节，故作者分别固定$\alpha_s$中其他两个值不变，剩余的一个值从0至1调节，得到上图的结果。可以发现，当调节不同层所对应的$\alpha_s$时，生成的Stylized Image风格变化的模式是不同的。这也凸显了作者方法的有效性。</li></ul><p><img src="/2020/02/15/extensive-reading2/1-exp2.jpg" alt></p><ul><li>在上面的实验中，作者通过常规的方法调节参数训练单一模型作为base，并调节关键参数使得自己的模型权重和单一模型一致。可以看出，作者的方法生成的图像和多个单一训练的模型生成的图像效果较为接近，这也说明了通过本文的方法，训练单一的模型在一定程度上能取代多个模型。</li></ul><p><img src="/2020/02/15/extensive-reading2/1-exp3.jpg" alt></p><ul><li>在上面的实验中，作者通过改变$\alpha$值和往内容图中填充高斯噪音以获得不同效果的Stylized Images。图中第一行是改变$\alpha$值，图中第二行是往内容图田中噪音，图中第三行是两种方法的结合。可以看到，生成的Stylized Images的确取得了不同的生成效果。</li></ul><h1 id="2-Sym-Parameterized-Dynamic-Inference-for-Mixed-Domain-Image-Translation-ICCV-2019"><a href="#2-Sym-Parameterized-Dynamic-Inference-for-Mixed-Domain-Image-Translation-ICCV-2019" class="headerlink" title="2. Sym-Parameterized Dynamic Inference for Mixed-Domain Image Translation(ICCV 2019)"></a>2. Sym-Parameterized Dynamic Inference for Mixed-Domain Image Translation(ICCV 2019)</h1><blockquote><p>针对什么问题</p></blockquote><p>​    本文指出现有的图像翻译方法在没有数据集的情况下，难以学习到领域知识。比如说跨领域知识。如有晴天、阴天、雨天三种数据集，现有的方法只能生成各自领域的知识，却无法学习到混合领域的知识。</p><blockquote><p>提出什么方法</p></blockquote><p>​    本文提出将多领域的概念扩展到损失领域，并可以动态的生成混合领域的图像。说白了，就是通过在网络中加入参数，动态调节生成图像中3种领域的程度以生成混合领域的图像。</p><blockquote><p>效果怎么样</p></blockquote><p>​    生成的图像可通过参数Sym-parameters的调节，动态生成混合领域的图像。</p><h2 id="2-1-Model"><a href="#2-1-Model" class="headerlink" title="2.1 Model"></a>2.1 Model</h2><p>本文的模型如下图所示：</p><p><img src="/2020/02/15/extensive-reading2/2-model.jpg" alt></p><ul><li><p>可以看出，该模型的创新指出在于</p><ul><li>三种领域的数据集混合训练</li><li>引入Sym-parameters的CCAM模块</li></ul></li><li><p>我们先来了解以下这三种领域数据集的训练方法</p><ul><li>单独一个生成网络获得生成图像</li><li>生成图像分别和三种领域的数据集做以下损失<ul><li>Reconstruction Loss</li><li>Perceptual Loss</li><li>Adversarial Loss</li></ul></li><li>然后三种损失通过引入Sym-parameters联合起来更新整个模型</li></ul></li><li><p>然后，我们看看Sym-parameters的含义</p><ul><li>由于该模型被三种数据集以不同的损失进行训练，故可以通过加入Sym-parameters <script type="math/tex">S=(s_1, s_2, s_3)</script>同步在模型的网络和模型的损失中控制生成图像占据的领域知识比例。</li><li><script type="math/tex">s_1 + s_2 + s_3 = 1</script>，其维度和损失的个数相同。</li><li>S基于狄利克雷分布采样</li><li>模型训练完毕后，可以通过调整S来改变生成图像中的混合领域知识的程度</li></ul></li><li><p>最后，我们讲解一下CCAM模块，如下图所示</p><p><img src="/2020/02/15/extensive-reading2/2-model-ccam.jpg" alt></p><ul><li>CCAM模块接收上一层的图像特征和Sym-parameters向量作为输入</li><li>Sym-parameters通过MLP升维再和AvgPool拼接在一起，最后通过全连接层的方式转换为通道注意力权重，和上一层原始特征相乘，得到输出特征</li></ul></li></ul><h2 id="2-2-Optimization"><a href="#2-2-Optimization" class="headerlink" title="2.2. Optimization"></a>2.2. Optimization</h2><p>本文的损失很简单，如下：</p><script type="math/tex; mode=display">L_G = s_1L_{rec} + s_2L_{adv} + s_3L_{per}</script><script type="math/tex; mode=display">L_D = -s_2L_{adv}</script><p>为了保持一致性，判别器损失也要加上相应的Sym-parameter。</p><h2 id="2-3-Experiment"><a href="#2-3-Experiment" class="headerlink" title="2.3 Experiment"></a>2.3 Experiment</h2><p>本文做的实验主要体现以下几方面内容</p><ul><li>证明Sym-parameters的有效性</li><li>验证是否能够学到混合领域知识</li></ul><p>下面挑选几个实验讲解</p><p><img src="/2020/02/15/extensive-reading2/2-exp1.jpg" alt></p><ul><li>在上面的实验中，作者通过1-D的玩具模型同时训练一个MLP网络完成分类和回归的任务。左图是数据集可视化后的结果，右边三张图是在1-D玩具模型中引入Sym-parameters。通过训练单一的基于Sym-parameters的模型，以及基于Hyper-parameters的多个模型，得到以下结论：<ul><li>当Sym-parameters和Hyper-parameters一致时，模型的损失值相同，说明Sym-parameters训练的单一模型的确能够取代多个模型。</li><li>若固定损失中的Sym-parameters不变，却动态的改变模型中的Sym-parameters，会导致最后的损失值和Hyper-parameters的不一致，说明模型和损失函数中的Sym-parameters需要同步变化。</li></ul></li></ul><p><img src="/2020/02/15/extensive-reading2/2-exp2.jpg" alt></p><ul><li>在上面的实验中，作者展示了在Sym-parameter控制下，<script type="math/tex">A, B, C</script>三个领域互相转化的效果。第一行是<script type="math/tex">A\to B</script>，以此类推。最后一行是随机调整Sym-parameters的值，生成图像的情况。值得一提的是，右下角的图像将中间一项设为1.5，使得该领域知识被大大加强。</li></ul><p><img src="/2020/02/15/extensive-reading2/2-exp3.jpg" alt></p><ul><li>在上图的实验中，作者将本文提出的SGAN和其他多领域转换方法SingleGAN在Sym-parameter于<script type="math/tex">(1,0,0), (0,1,0), (0,0,1)</script>的情况下进行比较。可以看出，SingGAN的输出连原始领域的图像都无法转换，但SGAN因为有重构损失的参与，可以更多的还原原始领域的细节。</li></ul><p><img src="/2020/02/15/extensive-reading2/2-exp4.jpg" alt></p><ul><li>最后，作者还研究了Sym-parameter的不同注入方法，结果如上图所示。可以看出，CCAM方法生成的图像在风格模式和内容结构上都优于其他方法。</li></ul><p>最后，感谢前辈们的付出，Respect!</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Seeing What a GAN Cannot Generate</title>
      <link href="/2020/01/03/gan-can-not-generate/"/>
      <url>/2020/01/03/gan-can-not-generate/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1. Abstract"></a>1. Abstract</h1><ul><li>虽然GAN很成功，但是模式崩塌(Mode Collapse)的问题依然存在，而现在少有论文去理解并量化GAN到底抛弃了哪些模式。</li><li>故本文在两个级别的基础上可视化了模式崩塌：分布级(Distribution Level)和实例级(Instance Level)。<ul><li>首先，对生成数据集和目标数据集使用语义分割网络，统计两个数据集中被分割出来的物体的分布。分布的不同将会透露出GAN在生成过程中所遗漏的物体。</li><li>其次，当确定被遗忘的物体类别后，直接可视化GAN遗忘的过程。即比较特定图像的原始版本和通过GAN反演后的版本</li></ul></li><li>为了达到可视化的目的，本文放宽了反演问题，解决了GAN某一层反演的问题来替代整个GAN结构的反演。</li><li>最后，本文使用所提出的框架来分析最近的一些GAN，鉴定他们典型失败的例子。</li></ul><h1 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h1><ul><li>本文提出一个很有意思的问题：How can we know what a GAN is unable to generate?<ul><li>也即，我们怎样才能知道GAN不能生成什么。</li><li>虽然GAN很强大，但是模式崩塌(Model Collapse)和模式下降(Model Dropping)的问题依然存在，这表明GAN在生成图像的过程中遗忘了某些目标分布。</li></ul></li><li>在下面的描述中，该文章会逐渐将：GAN不能生成什么这个问题转化，然后逐渐提出自己的方法<ul><li>首先，作者表明该文章不是为了判断生成分布和真实分布之间的距离有多远，而是为了理解目标数据集中的真实图像和生成的假图像到底哪里不同。</li><li>其次，引出问题：Does a GAN deviate from the target distribution by ignoring difficult images altogether? 也即，GAN是否会去忽略比较难生成的图像，从而偏离目标分布。<strong>这里我想插一句，学习过GAN的人都应该知道，生成多样性的图像也是GAN的一大挑战。而GAN生成的图像几乎都是数据集中出现概率比较高的图像，所以特殊的复杂的图像的确比较难生成。这个问题问的…感觉挺一般。</strong></li><li>最后，再将问题转化为：how can we detect and visualize these missing concepts that a GAN does not generate? 也即，我们如何去检测和可视化这些GAN未能生成的概念呢？</li></ul></li><li>为此，作者提出在：分布级(Distribution Level)和实例级(Instance Level)上，通过分析场景生成器，来直接理解模式下降。<ul><li>首先，分割生成图像和真实数据集中的图像，比较分割出来的类别的分布。</li><li>其次，一旦被遗忘的类别找到了，生成这些被遗忘的类别特例的图像，来验证这些类别在生成的过程中是否的确会被GAN所遗忘。</li></ul></li><li>最后，本文基于自己的结论做检测，发现<ul><li>被GAN遗忘的类别并不会以扭曲形变的结果展示在生成图上，而是直接被忽略，就好像它们不是场景的一部分。</li><li>GAN的确会忽略比较难以生成的类别，同时输出高平均视觉质量的图像</li></ul></li></ul><p>本文的结论没有想象中惊艳，在不做实验得情况下，通过分析已有GAN的生成结果也可以猜测得出。所以本文的亮点不在于结论，而在于他提出的如何反演GAN的过程，也就是下面要讲的Method。</p><h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h1><p>​    根据上文，本文使用两步来可视化和理解GAN无法生成的语义概念</p><h2 id="3-1-在分布级上量化模式崩塌"><a href="#3-1-在分布级上量化模式崩塌" class="headerlink" title="3.1 在分布级上量化模式崩塌"></a>3.1 在分布级上量化模式崩塌</h2><ul><li><p>本文利用图像的场景层次结构来分析GAN的误差，举个例子：使用LSUN bedrooms数据集，GAN若是渲染卧室，也会渲染一些窗帘。若是GAN生成的图像中窗帘的统计数据与真实数据不符合，那么窗帘就是GAN的一个缺陷，即无法生成。</p></li><li><p>为此，本文使用统一的语义分割网络分割真实图像和生成图像，测量每个对象类的总面积（以像素为单位）和平均值以协方差，并图示了出现次数较高的类别，如下图所示</p><p><img src="/2020/01/03/gan-can-not-generate/3-method1.jpg" alt></p><ul><li>mean area表明的是类别的面积，红色是生成图像中类别的平均面积，蓝色是真实数据集中各个类别的平均面积。可以看到，WGAN-GP生成的图像中，某几个类别如：chest，canopy等平均面积极其稀少，说明GAN遗忘了它们。StyleGAN的效果比WGAN-GP好很多，但还是出现了遗忘的情况。</li><li>relative delta表明的是相关偏差，即比较生成图像和真实图像中类别的比例，图中的纵坐标也给出了相关的计算公式。可以看到，这里的值和mean area相互照应，部分类别的确生成的过于稀少，被GAN所遗忘</li></ul></li><li><p>同时，本文仿照<strong>FID</strong>，提出了<strong>FSD</strong>(Frechet Segmentation Distance)这一指标</p><script type="math/tex; mode=display">FSD = ||\mu_g - \mu_t||^2 + Tr(\sum_g + \sum_t - 2(\sum_g\sum_t)^{\frac{1}{2}})</script><ul><li>其中，$\mu_t$是每个物体类别的平均像素值</li><li>$\sum_t$是这些像素数量的协方差</li></ul><p>FSD被用来衡量生成图像和真实图像中分割出来的物体的差异，越低说明差异越小，越好。</p></li></ul><h2 id="3-2-在实例级上量化模式崩塌"><a href="#3-2-在实例级上量化模式崩塌" class="headerlink" title="3.2 在实例级上量化模式崩塌"></a>3.2 在实例级上量化模式崩塌</h2><p>​    虽然上述的FSD能够统计出GAN在生成的过程中，遗漏了哪些特定的类别，却没有指出：GAN在生成图像时，哪里需要物体而它并未去生成。故本文又提出了一种方法来可视化GAN在生成图像时，所遗漏的类别对象。那具体怎么做呢？下面是我在自己阅读论文并理解后的概括。</p><ul><li><p>首先，我们得把问题定义好，所谓的反演就是</p><ul><li>当我们通过分布级的统计了解到一个训练好的GAN在生成图像时，会忽略哪些类别后</li><li>我们选定几张存在这些类别的图像，并希望使用GAN生成这些图像时，这些类别是否的确被遗忘了？</li><li>一旦这些类别在生成的图像中的确不存在，那么就可以得到结论：GAN难以生成什么。</li></ul></li><li><p>但可能有人会说，让GAN生成图像不是很简单的事情吗，为何需要后面的长篇大论。这也是我一开始困惑的，当我仔细理解后豁然开朗，原因如下</p><ul><li>让GAN生成图像的确很简单，但是<strong>让GAN生成指定内容的图像很困难，因为你不知道指定内容的图像所对应的输入向量是怎么样的。</strong></li><li>再换个角度解释一下，当一个GAN训练完后，你只能随机采样噪音向量作为输入，生成随机的图像，当然这些图像理论上是符合真实数据集分布的。但是，你无法指挥GAN生成指定内容的图像，你能给的生成条件仅是作为输入的噪音向量。</li></ul></li><li><p>那么可能就会有同学会想了：我们可以通过反编码的方式得到GAN生成图像的输入向量呀，的确有论文这样去做。当GAN网络层较少的时候是行得通的，但是一旦层数变多，那么效果就很差，文中后面的实验部分也证明了这一观点。</p></li><li><p>于是，本文定义了一个易于处理的反演问题</p><ul><li><p>将生成器分解成以下形式</p><script type="math/tex; mode=display">G = G_f(g_n(...((g_1(z)))))</script><p>其中$g_1, …, g_n$是生成器的前几层，$G_f$是生成器后面所有层的集合</p></li><li><p>作者指出，任何G能够生成的图像，$G_f$也能够生成，并做了如下定义</p><script type="math/tex; mode=display">range(G) \subset range(G_f)</script><p>也就是说，G能生成的图像是$G_f$生成图像的子集。若是$G_f$难以生成的图像，G同样也生成不了。(为什么？可能是因为$G_f$包含了更多的变化，输入向量的维度远超$z$)</p></li><li><p>所以问题就变成反演$G_f$，而不是$G$</p><script type="math/tex; mode=display">x^{'} = G_f(r^*),\\ where \quad r* = \underset{r}{argmin}L(G_f(r), x).</script></li><li><p>即最终不是寻找输入向量$z$，而是一个中间表示$r$来作为$G_f$的输入，得到指定内容图像的重构</p></li></ul></li><li><p>围绕着上面的定义，作者提出了自己的方法，一共分为三步，如下图所示</p><p><img src="/2020/01/03/gan-can-not-generate/3-method2.jpg" alt></p><ul><li><p>第一步，训练一个编码器E，使得$E(G(z)) \rightarrow z$。那么同学们可能就有疑惑了，不是说直接获得$z$很难吗，那为什么还要得到$z$，岂不画蛇添足。文中是这样解释的，虽然作者的目的是找到一个中间表示$r$来作为$G_f$的输入，但是一个较好的初始$z$将会对寻找$r$有很大的帮助，即得到一个更有和$z$相关的$r$。这一块的关键是如何训练编码器</p><ul><li><p>编码器的训练不是采用端到端的反向传播方式，而是$layer-wise$的方法。专门有相关的论文研究该方法，简单来说就是独立每一层网络的训练，而不是一起训练。</p></li><li><p>为此，作者对应着$g_i \in \left\{ g_1, g_2, …, g_n, G_f \right\}$将编码器$E$分解为$\left\{ e_1, e_2, … \right\}$，并定义了如下表示</p><script type="math/tex; mode=display">r_i = g_i(r_{i - 1}), \\ r_{i-1} = e_i(r_i)</script><p>即$e_i$能够还原$g_i$生成的特征，为了使得$e_i$较好的保留$g_i$的输出特征，又定义</p><script type="math/tex; mode=display">r_i \approx g_i(e_i(r_i))</script></li><li><p>训练$e_i$的损失函数如下所示，能够保证双向输出的特征被很好的保留</p><script type="math/tex; mode=display">L_L \equiv E_z[||r_{i-1} - e(g_i(r_{i-1}))||_1],\\ L_R \equiv E_z[||r_i - g_i(e(r_i))||_1], \\ e_i = \underset{e}{argmin}\quad L_L + \lambda_RL_R</script><p>其中$||.||<em>1$是$L_1$损失，$\lambda_R = 0.01$来强调$r</em>{i - 1}$的重构能力。</p></li></ul><p>那么，编码器$E$就可以表示为</p><script type="math/tex; mode=display">E^* = e_1(e_2(\dots(e_n(e_f(x))))).</script></li><li><p>第二步，在得到$z_0$之后，就可以得到初始化的$r_0 = g_n(…(g_1(z_0)))$。</p></li><li><p>第三步，不断的更新$r$找到最适合的那个$r^*$，最后输入到GAN的$G_f$部分中来恢复对指定生成图像的还原。</p><ul><li><p>寻找到$r^*$的方式是，对生成器早些层不断地做扰动，学习扰动参数$\delta$，如下所示</p><script type="math/tex; mode=display">z_0 \equiv E(x), \\ r \equiv \delta_n + g_n(\dots(\delta_2 + g_2(\delta_1 + g_1(z_0)))),\\ r^* = \underset{r}{argmin}(l(x, G_f(r)) + \lambda_{reg}\underset{i}{\sum}||\delta_i||^2)</script></li><li><p>其中，$l$是基于$VGG$的Perceptual  Loss。</p></li><li><p>根据文中描述，需要训练的参数就是$\delta$。</p></li></ul></li></ul></li></ul><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p>在讲解实验之前，说一下实验得配置</p><ul><li>作者在三个GAN模型上验证了自己的方法，分别是：WGAN-GP，Progressive GAN以及StyleGAN。</li><li>数据集是:LSUN bedroom images</li><li>使用：Unified Perceptual Parsing network 来分割图像</li></ul><h2 id="4-1-生成图的分割统计"><a href="#4-1-生成图的分割统计" class="headerlink" title="4.1 生成图的分割统计"></a>4.1 生成图的分割统计</h2><p>这个在前文的方法中讲过了，作者在实验中直接使用上文提出的$FSD$来衡量真实数据集和生成数据集类别统计的差异，如下表所示：</p><p><img src="/2020/01/03/gan-can-not-generate/4-exp1.jpg" alt></p><h2 id="4-2-敏感度测试"><a href="#4-2-敏感度测试" class="headerlink" title="4.2 敏感度测试"></a>4.2 敏感度测试</h2><p>这个敏感度测试，应该是为了验证自己的统计分布的方法并只针对整个数据集有意义，随机采样数据并统计信息也会得到类似的结果。表明了自己方法的泛化性。</p><h2 id="4-3-确认抛弃的模式"><a href="#4-3-确认抛弃的模式" class="headerlink" title="4.3 确认抛弃的模式"></a>4.3 确认抛弃的模式</h2><p>根据分布级统计到的数据，GAN总是选择性跳过比较困难的子任务，如在生成分布中，人出现的概率很低。通过反演的方法得到的包含被遗漏对象的图像的重构图，也都表明了模型难以生成这些对象。也就是说，GAN并不是以低质量的结果展示这些类别，而是完全遗忘了这些类别，所以这些类别在生成的图像中完全不存在。</p><h2 id="4-4-比较Layer-wise和其他方法"><a href="#4-4-比较Layer-wise和其他方法" class="headerlink" title="4.4 比较Layer-wise和其他方法"></a>4.4 比较Layer-wise和其他方法</h2><p>这是文中比较大型的实验，主要通过和其他方法对比，以及消融实验，来阐明自己设计的方法是比较合理有效的。整个实验效果图图下，</p><p><img src="/2020/01/03/gan-can-not-generate/4-exp2.jpg" alt></p><p>图中的前三列是对比试验，后三列是消融实验。第一行是重构图；由于是对GAN的生成图进行重构，存在$z$的ground-truth，所以第一行是$z$的协相关系数以及图示；第二行是第四层特征的协相关系数；第三行是整个生成图像的协相关系数。下面将对每一列进行详细的解释</p><ul><li>(a)中的方法通过梯度下降优化$z$来最小化重构损失，适用于网络层数较少的生成器，一旦用于层数较多的如15层的Progressive GAN，效果就很差。</li><li>(b)中构造了编码器E来反演生成器但是没有使用layer-wise的优化方法，虽然比起(a)好了很多，但是效果还是很差。</li><li>(c)中先通过方法(b)来初始化$z$，再通过方法(a)对$z$进行梯度更新，效果是好了很多，但是重构的结果仍然不如人意。</li><li>(d)是消融实验，它先通过layer-wise的优化方法得到编码器E，得到对于$z$的初始猜测$z_0 = E(x)$；再直接通过$G(z_0)$的方式得到重构后的图像$x^{‘}$，而未去学习扰动参数$\delta$来得到中间层$r$。实验效果表明，通过layer-wise得到的$z_0$是比较好的，生成的图像比较令人满意，这证明了layer-wise的有效性。</li><li>(e)的实验是在(d)之后，对$z$继续优化来最小化图像的重构损失。虽然定性的结果，重建的图很好，但是$z$的协相关系数很差，这导致我们难以判断重构的错误出在哪里。</li><li>(f)是完整的一套方法，可以接近完美的重建图像，而且各个协相关系数也都很好。所以一旦重建的图像缺斤少两了，并不是重建效果的不好，而是GAN本身问题，是GAN遗漏了那些类别物体。</li></ul><p>除了使用生成的图像来做实验，本文还采用了真实图像来测试，以获得定性的结果，如下图所示：</p><p><img src="/2020/01/03/gan-can-not-generate/4-exp3.jpg" alt></p><p>当然，结果也可以证明，本文提出的方法在协相关系数上最高，也就是说重构图最贴近原图。</p><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h1><p>最后，我总结以下本文的主要内容</p><ul><li>首先，作者提出问题：GAN存在model dropping的问题，那么GAN到底难以生成哪些东西呢？或者说，GAN到底生成不了什么。</li><li>为此，作者从分布级和实例级对这个问题进行了回答<ul><li>在分布级上，作者对生成图像和真实图像进行实力分割，统计各类物体的总面积以及相关系数，找到了生成图像中出现次数较少的类别</li><li>在实例级上，由于找到了出现次数较少的类别，可否再通过GAN重构某些存在这个类别物体的图像，来判断GAN是否遗漏了这些目标</li></ul></li><li>故本文提出了基于layer-wise的反演方法，并通过一系列实验证明了其有效性，这也是本文的两点，特别是使用协相关系数。</li><li>但文中的结论过于简单，那就是：当类别太难了，GAN就不生成了。我觉得是否可以将其归结于多样性的问题，比如数据集中床和窗帘比较多，而人比较少，所以生成的人的概率就小了</li></ul><p>最后，感谢本文作者的贡献，respect! </p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文泛读1：基于Disentangle的Image-to-Image Translation</title>
      <link href="/2019/12/11/extensive-reading1/"/>
      <url>/2019/12/11/extensive-reading1/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Unsupervised-Image-to-Image-Translation-Networks"><a href="#1-Unsupervised-Image-to-Image-Translation-Networks" class="headerlink" title="1. Unsupervised Image-to-Image Translation Networks"></a>1. Unsupervised Image-to-Image Translation Networks</h1><blockquote><p>针对什么问题？</p></blockquote><p>图像翻译旨在将图像从一个领域映射到另一个领域。在监督的设定下，数据集是成对的，领域之间的映射是可行的。但在非监督的情况下，即数据集不是成对的，而是分成两个彼此独立的领域，图像翻译的任务将会变得艰难。而现实生活中，成对的数据是很难获得的，故无监督的图像翻译更实在。</p><blockquote><p>提出什么方法？</p></blockquote><p>本文指出<strong>图像翻译的主要挑战是学习两个不同领域之间的联合分布</strong>。在无监督条件下，两个数据集合分别包括在不同领域中来自两个边缘分布的图像，而图像翻译的目的正是从这两个边缘分布得到联合分布。而耦合理论指出：<strong>一般情况下，存在无穷多的联合分布可以到达给定的边缘分布</strong>。因此，从边缘分布推断联合分布是一个不适合的方法。为了解决这个问题，本文对联合分布提出了额外的假设。</p><ul><li>本文提出一个共享隐藏空间的假设，即不同领域中的一对相关图像可以被映射到共享隐藏空间中的相同隐藏表达。</li><li>基于上述假设，本文提出了结合了GAN和VAE的UNIT框架</li></ul><blockquote><p>效果怎么样？</p></blockquote><p>本文将提出的框架应用于多种无监督的图像翻译问题并取得了高质量的翻译结果，这也从侧面反映了共享隐藏空间暗含了Cycle-Consistency Constraint（循环一致损失）。</p><h2 id="1-1-Model"><a href="#1-1-Model" class="headerlink" title="1.1 Model"></a>1.1 Model</h2><p>本文提出的UNIT框架如下图所示：</p><p><img src="/2019/12/11/extensive-reading1/1-model.jpg" alt></p><ul><li>其中，(a)图示了隐藏空间的含义，即Domain$X_1, X_2$中的图像$x_1, x_2$都被映射到了相同的隐藏空间$z$中</li><li>(b)图示了整个框架的结构<ul><li>Domain$X_1, X_2$中的图像$x_1, x_2$分别通过$E_1, E_2$映射到隐藏空间$z$中，再分别通过$G_1, G_2$还原成自我重构图像$\tilde{x}_1^{1 \rightarrow 1}, \tilde{x}_2^{2 \rightarrow 2}$或者领域翻译后的图像$\tilde{x}_1^{1 \rightarrow 2}, \tilde{x}_2^{2 \rightarrow 1}$。<ul><li>$E_1, E_2$共享了最后几层网络，$G_1, G_2$共享了前面几层网络</li><li>$\left\{ E_1, G_1 \right\}$, $\left\{ E_2, G_2 \right\}$对各自的领域$X_1, X_2$都构成了$VAE$的结构</li></ul></li><li>判别器$D_1, D_2$负责验证翻译后的图像是否真实</li></ul></li><li>本文指出，权重共享的约束并不能保证两个领域中相关的图像能够得到相同的隐藏空间编码。因为在无监督条件下，这两个领域中没有对应的图像能够训练网络输出相同的隐藏编码。即使隐藏编码是相同的，它们在不同的领域将会具有不同的语义意义。但话又说回来，通过对抗损失的训练，两个领域中对应的图像可以被映射到相同的隐藏空间，并可以被反射回不同的领域。</li></ul><h2 id="1-2-Optimization"><a href="#1-2-Optimization" class="headerlink" title="1.2 Optimization"></a>1.2 Optimization</h2><p>本文通过联合寻来你解决了$VAE_1, VAE_2, GAN_1, GAN_2$的学习问题，损失如下</p><script type="math/tex; mode=display">\underset{E_1, E_2, G_1, G_2}{min} \underset{D_1}{max}L_{VAE_1}(E_1, G_1) + L_{GAN_1}(E_1, G_1, D_1) + L_{cc_1}(E_1, G_1, E_2, G_2)</script><script type="math/tex; mode=display">\underset{E_1, E_2, G_1, G_2}{min} \underset{D_2}{max}L_{VAE_2}(E_2, G_2) + L_{GAN_2}(E_2, G_2，D_2) + L_{cc_2}(E_2, G_2, E_1, G_1)</script><ul><li>其中$L_{VAE}$旨在最小化变量的上界</li><li>$L_{GAN}$确保了翻译后图像的真实性</li><li>$L_{CC}$类似于循环一致性约束，确保了两次翻译后的图像与输入图像一致</li></ul><p>以上损失的具体形式请参考原论文</p><h2 id="1-3-Experiment"><a href="#1-3-Experiment" class="headerlink" title="1.3 Experiment"></a>1.3 Experiment</h2><ul><li>验证指标<ul><li>average pixel accuracy over the images in the test set</li><li>…</li></ul></li><li>对比模型<ul><li>DANN</li><li>DTN</li><li>CoGAN</li></ul></li><li>数据集<ul><li>map dataset</li><li>Synthetic to real</li><li>Dog breed conversion</li><li>…</li></ul></li></ul><p>下面展示本文部分的实验结果</p><p><img src="/2019/12/11/extensive-reading1/1-exp1.jpg" alt></p><p><img src="/2019/12/11/extensive-reading1/1-exp2.jpg" alt></p><h1 id="2-Multimodal-Unsupervised-Image-to-Image-Translation"><a href="#2-Multimodal-Unsupervised-Image-to-Image-Translation" class="headerlink" title="2. Multimodal Unsupervised Image-to-Image Translation"></a>2. Multimodal Unsupervised Image-to-Image Translation</h1><blockquote><p>针对什么问题？</p></blockquote><p>本文指出，在许多场景中，对于交叉领域的映射是多模态的，而现有的Image-to-Image Translation方法，如Pix2Pix, CycleGAN经仅仅只能得到一个确定性的映射。这是什么意思呢？</p><ul><li>举个例子，一个冬天的场景可能会有多个状态，天气、时间、光线等因素都会使得冬天的场景看起来不一样。</li><li>而现有的方法，若是做 夏天-&gt;冬天，一张夏天的场景仅能转换出到一种冬天领域下的场景，无法做到多种不同的输出。</li><li>特别是，本文指出，有些方法<strong>通过注入噪音完成图像中信息的改变，但是经过训练后的网络会忽略这些噪音，从而使得噪音的注入无效。</strong></li></ul><blockquote><p>提出什么方法？</p></blockquote><p>故本文针对以上问题，提出了<strong>MUNIT</strong>模型，并做了以下的假设</p><ul><li><p>首先，假设图像的潜在空间(Latent Space)可以被分解为<strong>内容空间</strong>(Content Space)和<strong>风格空间</strong>(Style Space)。</p><ul><li>内容空间编码了在图像翻译时需要保留的内容</li><li>风格空间代表了剩余那些输入图片中不需要保留的变量</li></ul></li><li><p>其次，假设处于不同领域中的图像共享一个共同的内容空间，但是不共享风格空间。如下图所示，其中$X_1, X_2$共享同一个内容空间，但是风格空间彼此独立。</p><p><img src="/2019/12/11/extensive-reading1/2-explain.jpg" alt></p></li></ul><p>那么，在进行领域转换的时候，将内容编码和不同的随机采样的风格编码相结合，就可以得到多样化和多模态的输出。</p><blockquote><p>效果怎么样?</p></blockquote><p>实验表明该方法在多模态输出建模中的有效性，且能生成质量更好的图像。</p><h2 id="2-1-Model"><a href="#2-1-Model" class="headerlink" title="2.1 Model"></a>2.1 Model</h2><p><img src="/2019/12/11/extensive-reading1/2-model.jpg" alt></p><p>上图为模型的概述。整个模型包含了两个<strong>auto-encoders</strong>，也就是模型中的红线和蓝线，各自代表一个domain。</p><ul><li>在图(a)中，两个auto-encoders的作用如下<ul><li>对于$x_1$，针对domain1的auto-encoders将其分解成风格空间$s_1$和内容空间$c_1$，再通过$L_1$损失重构成原图$\hat{x_1}$</li><li>对于$x_2$，针对domain2的auto-encoders将其分解成风格空间$s_2$和内容空间$c_2$，再通过$L_1$损失重构成原图$\hat{x_2}$</li></ul></li><li>在图(b)中，对于拆分后的$c_1, c_2$操作如下<ul><li>通过采样获取风格编码$x_1$, 将其和$c_2$一起输入到domain1的auto-encoders中，使其能够重构为$\hat{s_1},\hat{c_2}$。</li><li>通过采样获取风格编码$s_2$, 将其和$c_1$一起输入到domain2的auto-encoders中，使其能够重构为$\hat{s_2},\hat{c_1}$。</li></ul></li></ul><p>其中，auto-encoders的构造如下图</p><p><img src="/2019/12/11/extensive-reading1/2-autoencoder.jpg" alt></p><ul><li>可以看到，内容编码和风格编码都有各自的Encoder才获得<ul><li>对于Content Encoder，它由几个跨步卷积紧跟着几个残差块获得</li><li>对于Style Encoder，它由几个跨步卷积紧跟着几个全局的池化，并接上了全连接层</li></ul></li><li>之后，作者通过使用AdaIN方法将Content Code和Style Code结合了在一起，具体操作如下<ul><li>对于Content Code，它继续接几个残差块来不断地传播语义特征</li><li>对于Style Code，它通过MLP获得AdaIN的参数，在Content Code传播的过程中结合AdaIN参数</li></ul></li><li>最后，再通过上采样获得最后的重构图像</li></ul><p>其中，AdaIN的公式如下</p><script type="math/tex; mode=display">AdaIN(z, \gamma, \beta) = \gamma(\frac{z - \mu(z)}{\sigma(z)}) + \beta</script><ul><li>z代表卷积后输出的激活值</li><li>$\mu$代表通道均值</li><li>$\sigma$代表通道标准差</li><li>$\gamma, \beta$代表MLP生成的参数</li></ul><h2 id="2-2-Optimization"><a href="#2-2-Optimization" class="headerlink" title="2.2 Optimization"></a>2.2 Optimization</h2><ul><li><p>首先，是图像重构损失(Image Reconstruction)，如下</p><script type="math/tex; mode=display">L_{recon}^{x_1} = E_{x_1 \thicksim p(x_1)}[||G_1(E_1^c(x_1), E_1^s(x_1)) - x_1||_1]</script><ul><li>即从Domain1中采样数据$x_1$，通过Domain1的Auto-Encoders来提取内容空间和风格空间，再通过Domain1的$G_1$转换为重构后的图像，和原始图像$x_1$做$L_1$损失。</li><li>同理，可以推出$L_{recon}^{x_2}$损失。</li></ul></li><li><p>其次，是隐藏重构损失（Latent Reconstruction）</p><script type="math/tex; mode=display">L_{recon}^{c_1} = E_{c_1 \thicksim p(c_1),s_2 \thicksim q(s_2)}[||E_2^c(G_2(c_1, s_2)) - c_1||]</script><script type="math/tex; mode=display">L_{recon}^{s_2} = E_{c_1 \thicksim p(c_1),s_2 \thicksim q(s_2)}[||E_2^c(G_2(c_1, s_2)) - s_2||]</script><ul><li>其中$q(s_2)$是先验分布$N(0, I)$中采样的，$p(c_1)$由$c_1 = E_1^c(x_1)$得到，$x_1 \thicksim p(x_1)$。</li><li>$L_{recon}^{c_1}$通过$G_2$将$c_1, s_2$转化为Domain2中的图像，并通过Domain2中的内容解码器抽取内容空间，得到重构后的$c_1$的内容空间和原$c_1$做$L_1$损失</li><li>$L_{recon}^{s_2}$通过$G_2$将$c_1, s_2$转化为Domain2中的图像，并通过Domain2中的风格解码器抽取风格空间，得到重构后的风格空间和原$s_2$做$L_1$损失</li><li>同理，可以推出<script type="math/tex">L_{recon}^{x_2}, L_{recon}^{c_2}, L_{recon}^{s_1}</script>。</li></ul></li><li><p>最后，是对抗损失(Adversarial Loss)</p><script type="math/tex; mode=display">L_{GAN}^{x_2} = E_{c_1 \thicksim p(c_1), s_2 \thicksim p(s_2)}[log(1 - D_2(G_2(c_1, s_2)))] + E_{x_2 \thicksim p(x_2)}[logD_2(x_2)]</script><ul><li>判别器$D_2$尝试区分由$x_1$翻译到$x_2$中的图像与$X_2$中的真实图像</li><li>同理，可以推出$D<em>1$的损失$$L</em>{GAN}^{x_1}$$</li></ul></li></ul><p>根据以上的单一损失描述，可以得到本文的总损失如下：</p><script type="math/tex; mode=display">\underset{E_1, E_2, G_1, G_2}{min} \underset{D_1, D_2}{max}L(E_1, E_2, G_1, G_2, D_1, D_2) = L_{GAN}^{x_1} + L_{GAN}^{x_2} + \\ \lambda_x(L_{recon}^{x_1} + L_{recon}^{x_2}) + \lambda_c(L{recon}^{c_1} + L_{recon}^{c_2}) + \lambda_s(L_{recon}^{s_1} + L_{recon}^{s_2})</script><p>其中，$\lambda_x, \lambda_c, \lambda_s$都是超参数。</p><h2 id="2-3-Experiment"><a href="#2-3-Experiment" class="headerlink" title="2.3 Experiment"></a>2.3 Experiment</h2><ul><li>验证指标<ul><li>Human Preference，即人们对图像的欣赏度</li><li>LPIPS Distance，验证翻译后的多样性</li><li>Inception Score，验证多模态翻译后的真实度</li></ul></li><li>对比模型<ul><li>UNIT</li><li>CycleGAN</li><li>CycleGAN*</li><li>BicycleGAN</li></ul></li><li>数据集<ul><li>Edges &lt;-&gt; Shoes/handbags</li><li>Animal Image Translation</li><li>Street Scene Images</li><li>Summer &lt;-&gt; Winter</li></ul></li></ul><p>下面是挑选的实验中的几张效果图，总之就是比其他方法真实，且多样性好</p><p><img src="/2019/12/11/extensive-reading1/2-exp1.jpg" alt></p><p><img src="/2019/12/11/extensive-reading1/2-exp2.jpg" alt></p><p><img src="/2019/12/11/extensive-reading1/2-exp4.jpg" alt></p><h1 id="3-Diverse-Image-to-Image-Translation-via-Disentangled-Representations"><a href="#3-Diverse-Image-to-Image-Translation-via-Disentangled-Representations" class="headerlink" title="3. Diverse Image-to-Image Translation via Disentangled Representations"></a>3. Diverse Image-to-Image Translation via Disentangled Representations</h1><p>在写这篇论文泛读之前想说的话：<strong>本文是本次所有泛读文章中，文笔最好，实验最多的；若是对Image-to-Image Translation中Disentangle做法感兴趣，可以先从这篇读起。</strong></p><blockquote><p>针对什么问题？</p></blockquote><p>本文指出Image-to-Image Translation旨在学习两个视觉域中的映射关系，但存在以下两个挑战</p><ul><li>缺乏对齐的训练数据</li><li>输入单一的图像，缺乏多样化的输出结果</li></ul><blockquote><p>提出什么方法？</p></blockquote><p>为了生成多样化的输出且不依靠对齐的训练数据，本文将图像嵌入到两个空间中</p><ul><li>领域不变的内容空间(Domain-Invariant Content Space)，来捕捉共享信息</li><li>领域专有的属性空间(Domain-Specific Attribute Space)</li></ul><p>如下图所示</p><p><img src="/2019/12/11/extensive-reading1/3-method.jpg" alt></p><ul><li>其中，CycleGAN将$x, y$映射到分离的隐层空间</li><li>UNIT将$x, y$映射到共享的隐层空间</li><li>MUNIT和DRIT将$x, y$分别映射到共享的内容空间，和专有的属性空间</li></ul><blockquote><p>效果怎么样？</p></blockquote><p>大量的实验表明本文提出的方法能够生成多样化和真实的图像，并且能够被运用于多领域的图像翻译来生成多样性的输出。</p><h2 id="3-1-Model"><a href="#3-1-Model" class="headerlink" title="3.1 Model"></a>3.1 Model</h2><p>本文的目标是在没有成对训练数据的情况下，学习两个视觉领域之间的多模态映射。其中，本文完整的模型如下图所示</p><p><img src="/2019/12/11/extensive-reading1/3-model.jpg" alt></p><ul><li><p>针对领域$x, y$，存在各自的内容编码器$E_x^c, E_y^c$和属性编码器$E_x^a, E_y^a$，以及各自的解码器$G_x, G_y$。</p></li><li><p>在训练过程中，我们以领域$x$中的图像为例。第一步，对于领域$x$中的图像使用$E_x^a$抽取专有属性，再使用$E_y^c$抽取领域$y$中的内容信息，并通过领域$x$的解码器$G_x$将映射到领域$x$中。第二步，使用$E_x^a, E_x^c$抽取映射后图像的内容信息和专有属性，再使用$E_y^c$抽取领域$y$中的内容信息，最后通过领域$x$的解码器$G_x$将其映射回领域$x$，即得到重构后的$\hat{x}$。领域$y$中的图像同理。</p></li><li><p>在测试过程中，分为两种情况</p><ul><li>一是使用随机属性进行测试。输入图像$x$，使用$E_x^c$抽取图像内容，并采样属性信息，通过$G_y$将其映射到领域$y$中。</li><li>二是使用给定的图像抽取专有属性。输入图像$x$，使用$E_x^c$抽取领域$x$中的图像信息，再使用$E_y^a$抽取领域$y$中的图像专有属性，最后通过解码器$G_y$生成领域$y$中的图像。</li></ul></li><li><p>为了取得更好的分解效果，本文还采用了两个策略</p><ul><li><p>参数共享(Weight-Sharing)。基于两个领域共享相同的隐层空间，本文共享了$E_X^c, E_Y^c$的最后几层和$G_X, G_Y$的第一层。在参数共享的策略下，内容表达能够被强制映射到同一个空间，即使得判别器无法判断学习到的内容表达到底属于领域$x$还是领域$y$。</p></li><li><p>内容判别器(Content Discriminator)。内容的对抗损失如下</p><script type="math/tex; mode=display">L_{adv}^{content}(E_X^c, E_Y^c, D^c) = E_x[\frac{1}{2}logD^c(E_X^c(x)) + \frac{1}{2}log(1 - D^c(E^c_X(x))) + \\ E_y[\frac{1}{2}logD^c(E_Y^c(y)) + \frac{1}{2}log(1 - D^c(E^c_Y(y)))]</script></li></ul></li></ul><p>本文除了双领域之间的转换，还尝试了多领域之间的转化，我们这里也提一下，模型如下图</p><p><img src="/2019/12/11/extensive-reading1/3-multiI2I.jpg" alt></p><p>模型和双领域转换类型，存在两方面的不同</p><ul><li>多了领域编码(Domain Code)，其实就是$One-hot$向量，领域编码将会在$E_a$抽取领域的专有属性时嵌入到特征里。</li><li>内容判别器不仅需要判别图像是否真实，还需要判别来自哪一个领域，应该就是条件GAN判别器的做法，输入分为图像内容和条件。</li></ul><h2 id="3-2-Optimization"><a href="#3-2-Optimization" class="headerlink" title="3.2 Optimization"></a>3.2 Optimization</h2><p>除了上述的内容判别器损失，本文还使用了交叉循环一致性损失(Cross-cycle Consistency Loss)，使得模型可以将任意图像的内容和目标领域中另一个图像的属性表达结合在一起。</p><ul><li><p>交叉循环一致性损失分为两个步骤：前向翻译和后向翻译，也就是上文模型图的内容，即映射和重构。损失如下</p><script type="math/tex; mode=display">L_1^{cc}(G_X, G_Y, E_X^c, E_Y^c, E_X^a, E_Y^a) = E_{x,y}[||G_X(E_Y^c(v), E_X^a(u)) - x||_1 + \\ ||G_Y(E_X^c(u), E_Y^a(v)) - y||_1]</script></li></ul><p>除了内容对抗损失和交叉循环一致性损失，本文还使用了其他损失来确保网络的训练，如下图所示</p><p><img src="/2019/12/11/extensive-reading1/3-loss.jpg" alt></p><ul><li>$L_{1}^{recon}$在训练中完成自我重构</li><li>$L_{KL}$从先验高斯分布中采样，对齐属性的表达</li><li>$L_{adv}^{domain}$鼓励生成器G在每个领域生成真实的图像</li><li>$L_1^{latent}$对隐藏属性进行重构</li><li>$L_{ms}$进一步提升多样性</li></ul><p>损失的详情请看原文。</p><h2 id="3-3-Experiment"><a href="#3-3-Experiment" class="headerlink" title="3.3 Experiment"></a>3.3 Experiment</h2><ul><li>验证指标<ul><li>FID</li><li>LPIPS</li><li>JSD and NDB</li><li>User Performance</li></ul></li><li>对比模型<ul><li>DRIT</li><li>MUNIT</li><li>Cycle/Bicycle GAN</li></ul></li><li>数据集<ul><li>Winter -&gt; Summer</li><li>Cat -&gt; Dog</li></ul></li></ul><p>本文的实验极其丰富，此外还尝试了生成高分辨率的图像，建议大家阅读原文，我下面就稍微贴几张。</p><p><img src="/2019/12/11/extensive-reading1/3-exp1.jpg" alt></p><p><img src="/2019/12/11/extensive-reading1/3-exp2.jpg" alt></p><p><img src="/2019/12/11/extensive-reading1/3-exp3.jpg" alt></p><h1 id="4-Image-to-Image-Translation-for-Cross-Domain-Disentanglement"><a href="#4-Image-to-Image-Translation-for-Cross-Domain-Disentanglement" class="headerlink" title="4. Image-to-Image-Translation-for-Cross-Domain-Disentanglement"></a>4. Image-to-Image-Translation-for-Cross-Domain-Disentanglement</h1><p>本文的主要思想和上述论文类似，主要不同点在于引入了$GRL$模块使得编码器能分别提取共享信息和专有属性。</p><blockquote><p>针对什么问题？</p></blockquote><p>从场景的固有属性出发，将光照、阴影、视点、物体方向等场景事件分离出来，一直是计算机视觉长期追求的目标。当应用于深度学习时，这允许模型了解实体独立的变量因素，若信息与当前任务无关，那么模型可以沿着特定的变化因素边缘化信息。这样的分解过程对于基于表示学习的任务提供了更加精准的控制。故本文旨在将分解表示和图像翻译结合在一起，以达到更好的效果。</p><blockquote><p>提出什么方法？</p></blockquote><p>本文将分解目标和图像翻译结合，引入了跨域分解的概念，其目的是将域内专有的属性和跨域共享的属性分开。为了做到这一点，本文将交叉领域中图像的表示分解为三部分：交叉领域共有的信息，以及各自领域专有的信息，如下图</p><p><img src="/2019/12/11/extensive-reading1/4-concept.jpg" alt></p><ul><li>图中两个数字领域共享的是没有颜色信息的数字</li><li>专有的信息是，数字的背景信息等</li></ul><blockquote><p>效果怎么样？</p></blockquote><ul><li>多样性样本(Sample diversity)。可以基于给定的输入图片输出多样化的结果</li><li>跨域检索(Cross Domain Retrieval)。可以根据域之间共享的表示部分在两个域中检索相似的图像</li><li>专有域的图像迁移(Domain-specific Image Transfer)。领域专有的特征可以在图像间传输</li><li>专有域的插值(Domain-specific Interpolation)。可以在两个图像间插入领域专有的特征。</li></ul><h2 id="4-1-Model"><a href="#4-1-Model" class="headerlink" title="4.1 Model"></a>4.1 Model</h2><p>本文的模型如下图，左边是图像翻译模块，右边是跨域自动编码器。</p><p><img src="/2019/12/11/extensive-reading1/4-model.jpg" alt></p><ul><li>首先要分清楚图中各模块的作用，本文对模块的命名不是很友好，建议先看右边的跨域编码器。我们以右图中对于领域$X$的操作为例，领域$X$的编码器$G_e$将图像分解为共享的信息部分$S^x$和专有的属性部分$E^x$；再将$E^x$和来自领域$Y$中由编码器$F_e$抽取的图像的共享信息$E^y$相结合，输入到领域$X$的解码器$F_d$，可以得到重构后的图像$x$。同理对于领域$Y$的操作。</li><li>我们再看图中左边的图像翻译。我们以领域$X$操作为例，依然通过$G_e$提取共享信息$S_x$以及专有特征$E^x$。<ul><li>首先讲一下GRL操作。在黄色虚线那部分，作者认为由于$E^x$中体现的是专有属性，必不能包含领域$Y$中的信息，所以仅仅使用$E^x$不可能生成领域$Y$中的图像。为了强化这种认知，作者用了我看起来很“奇葩”的行为，他尝试使用$E^x$生成领域$Y$中的图像，却有积极的引导特征学习来防止这种情况发生。故作者专门设置了一个GRL(Gradient Reversal Layer)模块来学习与域无关的特性。在网络的前向传播过程中，它作为指示函数；在反向传播的时候，返回相应分支的梯度。</li><li>剩下的就是常规操作，将$S^x$和采样得到的属性结合，并通过解码器$G_d$将$x$翻译到领域$Y$中，再通过领域$Y$的编码器提取共享信息和专有属性。</li></ul></li></ul><h2 id="4-2-Optimization"><a href="#4-2-Optimization" class="headerlink" title="4.2 Optimization"></a>4.2 Optimization</h2><ul><li><p>重构隐层空间。用于翻译的解码器接收共享的表示$S$以及用于充当专有属性的随机输入噪音作为输入，在这里本文从8维的噪音向量$z$中采样，这也要求专有属性表示的能够满足合理的分布。为此，作者在这里加入了原始对抗损失，训练使得能从$N(0, I)$中采样接近于$E^x$分布的特征。</p></li><li><p>重构后的图像必须与解码器的输入匹配，因此需要在原图像和重构后的图像$S^X$中加入$L_1$损失。</p><script type="math/tex; mode=display">L_{recon}^X = E_{x \thicksim X}[||G_e(G_d(S^X, z)) - (S^X, z)||]</script></li><li><p>为了使得翻译后的图像接近于真实数据分布，还需要引入对抗损失，本文作者采用了$WGAN-GP$的损失，以获得稳定的训练和高质量的输出</p><script type="math/tex; mode=display">L_{Disc}^X = E_{\tilde{x} \thicksim \tilde{X}}[D(\tilde{x})] - E_{x \thicksim X}[D(x)] + \lambda \cdot E_{\hat{x} \thicksim \hat{X}}[(||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2]</script><script type="math/tex; mode=display">L_{Gen}^X = -E_{\tilde{x} \thicksim \tilde{X}}[D(\tilde{x})]</script></li><li><p>最后在跨域自动编码器中，重建的图像需要接近于原图，故有以下损失</p><script type="math/tex; mode=display">L_{auto}^X = E_{x \thicksim X}[||x^{'} - x||]</script></li></ul><p>损失的详情请查看原文</p><h2 id="4-3-Experiment"><a href="#4-3-Experiment" class="headerlink" title="4.3 Experiment"></a>4.3 Experiment</h2><ul><li>验证指标<ul><li>引入某篇论文中相关的读量协议</li></ul></li><li>对比方法<ul><li>BicycleGAN</li><li>Pix2Pix</li></ul></li><li>数据集<ul><li>MNIST-CD</li><li>MNIST-CB</li><li>3D car models </li><li>3D chair models</li></ul></li></ul><p>最后，给一些实验截图，有兴趣的朋友可自行阅读原文。</p><p><img src="/2019/12/11/extensive-reading1/4-exp.jpg" alt></p><h1 id="5-Conditional-Image-to-Image-translation"><a href="#5-Conditional-Image-to-Image-translation" class="headerlink" title="5. Conditional Image-to-Image translation"></a>5. Conditional Image-to-Image translation</h1><blockquote><p>针对什么问题？</p></blockquote><p>现有的图像翻译方法缺乏控制翻译结果的能力，从而导致生成的结果缺乏多样性。</p><blockquote><p>提出什么方法？</p></blockquote><p>本文提出了条件图像翻译的概念，即可以在给定的图像条件下，将源于转换为目标域中的图像，如下图示</p><p><img src="/2019/12/11/extensive-reading1/5-concept.jpg" alt></p><ul><li>图a以人脸作为条件，生成指定领域的人脸</li><li>图b以背包作为条件，生成指定领域的背包</li></ul><p>该模型要求生成的图像必须从目标域中继承条件图像的特定域特征。</p><blockquote><p>效果怎么样？</p></blockquote><p>本文进行了人脸转换、轮廓到背包等实验，并证明了该方法的有效性。</p><h2 id="5-1-Model"><a href="#5-1-Model" class="headerlink" title="5.1 Model"></a>5.1 Model</h2><p>本文提出的模型如下图所示</p><p><img src="/2019/12/11/extensive-reading1/5-model.jpg" alt></p><ul><li>上图中有两个编码器$e_A, e_B$以及两个解码器$g_A, g_B$。<ul><li>编码器用来抽取特征，输入图像后输出两类特征：领域无关特征(Domain-Independent Features)及领域专有特征(Domain-Specific Features)。（感觉名字取得很不友好）<ul><li>在翻译过程中，领域无关特征会保留。如将男人脸转为女人脸时，会保留脸的边缘，眼睛，鼻子</li><li>在翻译过程中，领域专有特征会改变，如脸的头发及风格。（例子也举得不友好）</li></ul></li><li>解码器充当生成器，将源域中图像的领域无关特征和目标域中图像的领域专有特征作为输入，输出属于目标域中的生成图像</li></ul></li><li>在编码器中，编码器网络会被切分成两个分支：一个接上卷积网络用来抽取领域无关特征，另一个接上全连接层用来抽取领域专有特征，通过损失函数的更新实现两分支不同的功能。</li><li>现在以领域$X$为例，讲一下模型的流程。<ul><li>输入图像$x_A$，编码器$e_A$提取领域不变特征$x_A^i$以及领域专有特征$x_A^S$，编码器$e_B$提取图像$x_B$的领域专有特征$x_B^s$。</li><li>接着，将<script type="math/tex">s_A^i</script>和<script type="math/tex">x_A^S</script>结合并输入解码器<script type="math/tex">g_B</script>得到映射到领域<script type="math/tex">B</script>中的图像<script type="math/tex">x_{AB}</script>。</li><li>判别器<script type="math/tex">d_B</script>接收<script type="math/tex">x_{AB}</script>及<script type="math/tex">x_B</script>以判断生成数据是否真实。</li><li>最后，将<script type="math/tex">x_{AB}</script>通过编码器<script type="math/tex">e_B</script>拆分出<script type="math/tex">x_{AB}</script>的领域不变重构特征<script type="math/tex">\hat{x}_A^i</script>以及领域专有重构特征<script type="math/tex">\hat{x}_B^s</script>，并将<script type="math/tex">\hat{x}_A^i</script>和<script type="math/tex">x^s_A</script>结合输入解码器<script type="math/tex">g_A</script>，以得到重构后的图像<script type="math/tex">\hat{x}_A</script>。</li></ul></li><li>领域$Y$同理。</li></ul><h2 id="5-2-Optimization"><a href="#5-2-Optimization" class="headerlink" title="5.2 Optimization"></a>5.2 Optimization</h2><ul><li><p>对抗损失(GAN Loss)。为了保证<script type="math/tex">x_{AB},x_{BA}</script>都能映射到相关的领域中，本文设计了判别器<script type="math/tex">d_A,d_B</script>损失如下</p><script type="math/tex; mode=display">l_{GAN} = log(d_A(x_A)) + log(1 - d_A(x_{BA})) + log(d_B(x_B)) + log(1 - d_B(x_{AB}))</script></li><li><p>双重学习损失(Dual Learning Loss)，使得模型最小化重构损失，如下</p><p><script type="math/tex">l_{dual}^{im}(x_A, x_B) = ||x_A - \hat{x}_A||^2 + ||x_B - \hat{x}_B||^2</script>,</p><p><script type="math/tex">l_{dual}^{di}(x_A, x_B) = ||x_A^i - \hat{x}_A^i|| + ||x_B^i - \hat{x}_B^i||^2</script>,</p><p><script type="math/tex">l_{dual}^{ds}(x_A, x_B) = ||x_A^s, \hat{x}_A^s||^2 + ||x_B^s - \hat{x}_B^s||^2</script>.</p></li></ul><h2 id="5-3-Experiment"><a href="#5-3-Experiment" class="headerlink" title="5.3 Experiment"></a>5.3 Experiment</h2><ul><li>验证指标<ul><li>本文的都是视觉性实验，通过观赏性来验证模型，且输入图像为64x64大小，怀疑是经费不够</li></ul></li><li>对比模型<ul><li>DualGAN</li><li>DualGAN-c</li><li>GAN-c</li></ul></li><li>数据集<ul><li>men-&gt;women</li><li>edges-&gt;shoes</li></ul></li></ul><p>下面贴几张模型的效果图</p><p><img src="/2019/12/11/extensive-reading1/5-exp.jpg" alt></p><p>最后，感谢前辈们的付出，Respect!</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Self-Ensembing with GAN-based Data Augmentation for Domain Adaption in Sematic Segmentation</title>
      <link href="/2019/12/10/self-ensembing/"/>
      <url>/2019/12/10/self-ensembing/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1. Abstract"></a>1. Abstract</h1><ul><li>基于深度学习的语义分割有着固有的缺陷：需要大量的数据</li><li>本文引入self-ensembling，想基于无监督下的领域自适应来解决数据问题，但通过self-ensembling中微调过的人工数据难以缩减语义分割中巨大的领域距离(Domain Gap)</li><li>为此，本文提出一个由两部分组成的框架<ul><li>首先，基于GAN提出一个数据增强方法，能有效促进领域的对齐(Domain Alignment)</li><li>其次，基于增强后的数据，将self-ensembling运用到分割网络中以提升模型的能力</li></ul></li></ul><h1 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h1><ul><li><p>语义分割的任务：给图像中的每个像素点都分类。</p></li><li><p>大量基于深度学习的算法能够获得较好的效果，但过于依赖数据。为避免人工标注的繁琐和耗时，研究人员利用计算机图形学得到合成数据及所对应的分割标签。而合成数据训练得到的分割模型难以媲美使用真实数据训练得到的，因为存在了称之为领域迁移(Domain Shift)的分布不同(Distribution Difference)。</p></li><li><p>Unsupervised domain adaptation通过将标记过的数据集中的知识迁移到未标记过的数据集，来解决Domain Shift的问题。</p><ul><li>最近的方法多集中在对齐源数据和目标数据中抽取到的特征。如基于对抗训练，通过域混肴(Domain Confusion)来最小化领域之间的差异(Domain Discrepancy)。</li><li>然而，对抗方法也有缺陷，为了对齐两个不同领域的全局分布可能会造成负迁移(Negative Transfer)，即将目标特征对齐到了源特征中错误的语义分类。<strong>通俗点说，就是将天空的特征或者风格，迁移到了马路上</strong>。特别是，数据集中的某个类别比较稀少，更容易产生负迁移。</li></ul></li><li><p>为解决负迁移，本文引入了<strong>self-ensembling</strong>。</p><ul><li>self-ensembling由学生网络和教师网络组成<ul><li>学生网络被迫基于老师提供的目标数据，做出协同的预测</li><li>教师网络的参数是学生网络的均值，所以教师在目标数据上做的预测可以看作是学生的伪标签</li></ul></li><li>虽然self-ensembling在图像分类上有很好的效果，若想用于成功对齐领域，它需要大力调节过的人工增强数据。此外，虽然self-ensembling得到的几何变化较大的数据能有效用于分类，它并不适合于语义分割中的Domain Shift。</li></ul></li><li><p>为改进self-ensembling，本文设计了新的数据增强方法</p><ul><li>基于GAN，生成能够保留语义内容的增强图像。因为未保留语义内容的图像，将会破坏分割的性能，由于<strong>增强后的图像和作为源标签的图像在像素上的不匹配</strong>。</li><li>为解决上述问题，该方法于生成器中加入了语义约束(Semantic Constraint)，来保留全局和局部的结构。</li><li>此外，本文提出了目标导向的生成器，能够基于目标领域抽取的风格信息来生成图片。这样，该方法生成的图像又能保留语义信息，又能只迁移目标图像的风格。</li></ul></li><li><p>大多数Image-to-Image Translation都是依赖于不同形式的Cycle-Consistency</p><ul><li>有两个限制<ul><li>需要多余的模块，比如两个生成器</li><li>若是数据不平衡的话，源领域和目标领域的约束过于强烈。<strong>也就是说，不管怎么生成，就那几种图像，类似于Model Collapse吧</strong></li></ul></li><li>而本文的方法由于它的设计，就不需要考虑Cycle-consistency了</li></ul></li><li><p>整个模型的框架如下图所示</p><p><img src="/2019/12/10/self-ensembing/Figure1.jpg" alt="Figure1"></p><ul><li>步骤一，给定有标签的合成数据及无标签的真实数据，生成带有标签的增强数据(我认为，这里的标签就是源标签)</li><li>步骤二，作者将关键信息写到了这里，使用两个分割网络作为教师和学生，以使用Self-ensembling算法。这两个网络都使用了合成数据、增强后的数据以及真实数据。在训练过程中，教师网络会将知识迁移到学生网络中。</li></ul></li></ul><h1 id="3-Proposed-Method"><a href="#3-Proposed-Method" class="headerlink" title="3. Proposed Method"></a>3. Proposed Method</h1><p>​    提升self-ensembling用于语义分割的兼容性的方法是，基于GAN增强后的数据来对齐源领域和目标领域之间的表达，而不是self-ensembling用于分类中的几何转变。为了这个目的，本文设计了以下的模型。</p><h2 id="3-1-Target-guided-generator"><a href="#3-1-Target-guided-generator" class="headerlink" title="3.1 Target-guided generator"></a>3.1 Target-guided generator</h2><ul><li><p>TGCF-DA的模型图如下所示</p><p><img src="/2019/12/10/self-ensembing/Figure2.jpg" alt="Figure2"></p><ul><li>基于假设，图像表达可被分为两部分：内容和图像，设计了以上结构<ul><li>使用Souce Encoder来抽取源图像的内容表达</li><li>再使用Target Encoder来抽取目标图像的风格表达</li></ul></li><li>为了结合源图像的内容和目标图像的风格，使用AdaIN算法</li></ul></li><li><p>根据上面的设计，生成器G得到的图像将会在保留源图像内容的同时，迁移目标图像的风格。<strong>也就是改变源图像，如GTA5的虚拟风格，变成现实风格</strong>。最后，将生成图像作为fake data，目标领域作为real data，输入判别器。</p></li></ul><h2 id="3-2-Semantic-constraint"><a href="#3-2-Semantic-constraint" class="headerlink" title="3.2 Semantic constraint"></a>3.2 Semantic constraint</h2><ul><li>由于没有使用Cycle-consistency，本文使用Semantic Constriant来约束生成图像的语义信息。具体的做法如下<ul><li>设定一个预训练好的分割模型，本文使用的是FCN-8s</li><li>将生成的图像输入得到分割后的掩码</li><li>分割后的掩码和源图像的标签做交叉熵</li></ul></li><li>这个做法类似于相当于风格迁移中，计算内容损失的做法。</li></ul><h2 id="3-3-Target-guided-and-cycle-free-data-augmentation"><a href="#3-3-Target-guided-and-cycle-free-data-augmentation" class="headerlink" title="3.3 Target-guided and cycle-free data augmentation"></a>3.3 Target-guided and cycle-free data augmentation</h2><ul><li><p>本文对于GAN框架的构建如下</p><ul><li><p>Discriminator的构建是基于Hpix2pix的，详见原文中的参考文献</p></li><li><p>使用LSGAN的损失作为对抗损失，并基于spectral normalization的方法稳定GAN的训练</p><script type="math/tex; mode=display">L_{GAN}(G, D) = E_{(x_s, s_t)\thicksim(P_S, P_T)}[D(G(s_s, s_t))^2] + E_{x_t\thicksim P_t}[(D(x_t) - 1)^2].</script></li></ul></li><li><p>对抗损失能够保证G生成的新图像在视觉上和目标图像相似。由于分割模型$f_{seg}$固定了，可以联合训练生成器和判别器来优化总损失</p><script type="math/tex; mode=display">L_{TGCF-DA} = L_{GAN} + \lambda L_{sem}</script></li><li><p>经过上述损失预训练后的生成器，将被用来合成增强数据，为后续的self-ensembling做准备。</p></li></ul><h2 id="3-4-Self-ensembling"><a href="#3-4-Self-ensembling" class="headerlink" title="3.4 Self-ensembling"></a>3.4 Self-ensembling</h2><ul><li><p>构建了教师网络$f_T$和学生网络$f_S$。步骤t中，教师网络的参数$t_i$由学生网络根据下列公式计算得到</p><script type="math/tex; mode=display">t_i = \alpha t_{i-1} + (1 - \alpha)s_i</script></li><li><p>在训练的过程中，每个mini-batch都会包含以下数据</p><ul><li>source samples  源样本</li><li>augmented samples  增强样本</li><li>target samples  目标样本</li></ul></li><li><p>源样本和增强样本将会被用来计算监督损失$L_{sup}$，即对于语义分割的交叉熵。这个损失函数能使得学生网络，对于源数据和增强数据，都产生语义上更加精准的预测。</p></li><li><p>一致性损失$L_{con}$是学生网络和教师网络生成的预测图的均方差</p><script type="math/tex; mode=display">L_{con}(f_S, f_T) = E_{x_t \thicksim P_T}[||\sigma (f_S(x_t)) - \sigma (f_T(x_t))||^2]</script><p>这里的$\sigma$是softmax函数，用来计算预测图的概率</p></li><li><p>总体的损失如下</p><script type="math/tex; mode=display">L_{total} = L_{sup} + \delta_{con} L_{con}</script><p>这里的$\delta_{con}$是一致性损失的权重</p></li></ul><h2 id="3-5-Data-augmentation-for-target-samples"><a href="#3-5-Data-augmentation-for-target-samples" class="headerlink" title="3.5 Data augmentation for target samples"></a>3.5 Data augmentation for target samples</h2><p>​    看到这一块内容，稍微有些迷糊，特别是作者的第一句话：这里的对于目标样本的数据增强和TGCF-DA并不相关，差点就被带偏了，不知道理解到什么地方了。不过仔细看了一下后面的内容，得到如下理解</p><ul><li>这里对target samples增强是为了在self-ensembling中计算consistency loss。对于目标样本的随机数据增强，是为了强迫学生网络针对相同的目标样本得到不同的预测，以更好的训练学生和教师网络。</li><li>根据前文，常规self-ensembling中的几何变换对于像素级别的预测任务并无帮助。因此，本文<strong>在目标样本中注入高斯噪声，并分别喂给学生和教师网络</strong>。此外，对于网络参数还是用了Dropout。</li><li>因此，学生网络在目标样本有扰动的情况下，还必须得产生和教师网络一致的预测，这也变相了提升了模型的性能。</li></ul><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p>​    在做实验之前，作者略微详细的介绍了用到的数据集如：GTA5，Cityscapes等。对于实验的配置，作者说的很详细，建议详细阅读。比如TGCF-DA的具体构造，生成器和判别器都是挑了比较好的结构，一起一些超参数的设置。此外，还讲了self-ensembling中，对于分割网络的选择，选择哪一层计算损失等。</p><h2 id="4-1-Experimental-results"><a href="#4-1-Experimental-results" class="headerlink" title="4.1 Experimental results"></a>4.1 Experimental results</h2><ul><li><p>作者将自己的方法与CycleGAN，MCD，DCAN等一堆方法进行比较。该实验首先在GTA5或者SYNTHIA数据集上，训练分割网络，并在Cityscapes的验证集上验证。实验得结果如下表所示</p><p><img src="/2019/12/10/self-ensembing/Table1.jpg" alt="Table1"></p><ul><li>表中的<strong>Self-Ensembling</strong>，代表着由源数据和目标数据(未加入增强数据)训练得到的分割网络的性能</li><li>表中的<strong>TGCF-DA</strong>表明由源数据、目标数据、TGCF-DA生成的增强数据，共同训练的分割网络</li><li>表中的<strong>Ours(TGCF-DA + SE)</strong>表明结合了TGCF-DA和Self-Enembling方法，得到的分割网络</li><li>表中的<strong>mIoU*</strong>代表的是，13个常见类别的mIoU，因为数据集中有些类别出现次数不多。但具体是哪13个类并没有讲，可能会在作者提供的附件里</li><li>表中的<strong>Source Only</strong>声明了只在源数据集上训练的分割模型的效果</li><li>表中的<strong>Target Only</strong>声明了在监督设定下训练的分割模型的效果</li></ul></li><li><p>作为刚接触Domain Adaption的小白，对于这个实验一直耿耿于怀。首先是对于表中的Baseline(Source Only)的不理解，其次是对Target Only的不了解。经反复思考后，理解如下</p><ul><li><p><strong>Baseline(Source Only)</strong>既然都说了是Source Only，而且文中又把增强后的数据叫做Augmented Data。对应上面的表格，Baseline应该指的就是GTA5和SYNTHIA中的原始训练集，训练得到的分割模型，那效果肯定差。</p></li><li><p>接下来一系列的方法，CycleGAN，MCD，DCAN等，应该都是使用进行Domain Shift后的数据配合原始的mask标签，来训练分割网络，那为什么效果会差呢？根据文中的描述，原因有二</p><ul><li>首先是，经过Domain Shift图像的内容结构可能被破坏了，导致原始的mask标签不匹配，从而产生语义上的损失。<strong>如，车子经过domain shift之后形状变了，但对应的mask掩码标签却没变，那效果肯定差</strong>。</li><li>其次是，负迁移造成的影响，有些类别过于稀疏，在Domain Shift的时候导致转换有误，<strong>如，前面的例子，将马路的知识迁移到了天空上</strong>。</li></ul></li><li><p>然后就是纯Self-Ensembling方法了，效果有一定提升，但是没本文方法提升的多。但作者在前文就指出原始的Self-Ensembling适用于分类网络，但却不太适用于分割网络。<strong>我理解是：由于Self-Ensembling是通过形变原始数据得到增强数据，来提升分类模型的性能；而在分割网络中，形变原始数据会导致其和mask标签不匹配，破坏了内容结构，所以效果变差</strong>。</p></li><li><p>最后就是文中提到的两个方法</p><ul><li><p>先是只使用了<strong>TGCF-DA</strong>，效果并没有好到哪里去，甚至比CycleGAN，MCD，DCAN等方法都要低几个点。这说明，抛弃Cycle Consistency，理想化的将图像分为content和style，并通过AdaIN方法进行结合等一系列操作，并没有达到作者预期的效果。<strong>我觉得作者可能一开始只提出了这个方法，做了实验之后发现效果居然没有好太多，再考虑将Self-ensembling的方法加入来提升网络的性能</strong>。不过，这个方法还是很有创意，巧妙地将任意风格迁移和GAN结合在了一起，值得我思考。</p></li><li><p>然后就是<strong>TGCF-DA + Self-ensembling</strong>，效果简直超神，顺利毕业。原理就是，通过TGCF-DA预训练模型生成增强数据，然后配合学生、老师网络进行训练，最后得到一个更好的分割模型。如下图</p><p><img src="/2019/12/10/self-ensembing/Figure3.jpg" alt="Figure3"></p></li></ul></li></ul></li></ul><h2 id="4-2-Ablation-studies"><a href="#4-2-Ablation-studies" class="headerlink" title="4.2 Ablation studies"></a>4.2 Ablation studies</h2><ul><li><p>首先，做了Self-Ensembling的消融实验，如下图</p><p><img src="/2019/12/10/self-ensembing/Figure4.jpg" alt="Figure4"></p><ul><li>在上一实验的表格中，就发现纯Self-Ensembling效果很差</li><li>这就说明了，主要的功劳并不在Self-Ensembling的应用上，而是TGCF-DA + Self-Ensembling上</li></ul></li><li><p>其次，做了TGCF-DA的消融实验</p><ul><li>在上一实验表格中，发现TGCF-DA能有效提升分割模型的效果。</li><li>其次，通过图示风格模型的mIOU，发现纯Self-Ensembling在第8个epoch就达到了极大值，继续训练效果变差，而TGCF-DA + Self-Ensembling效果持续上升，说明二者结合才是王道。也从侧面说明了TGCF-DA的重要性</li></ul></li><li><p><strong>在消融实验中，作者想表明TGCF-DA + Self-Ensembling结合的重要性，并将主要功劳放在TGCF-DA上。但正如我前文所说，TGCF-DA的效果和其他方法相差不多，那是否将其他方法和Self-Ensembling结合，也会得到更好的效果呢？</strong>不过，这也是我鸡蛋里挑骨头了，本文主要的贡献已经很多了，就比如TGCF-DA + Self-Ensembling结合训分割模型，提升性能，也是本文的卖点。</p></li></ul><h1 id="5-Analysis"><a href="#5-Analysis" class="headerlink" title="5. Analysis"></a>5. Analysis</h1><p>​    在这块内容，作者可视化了几个模块的结果，并进行进一步的分析。</p><h2 id="5-1-Visualizaton"><a href="#5-1-Visualizaton" class="headerlink" title="5.1 Visualizaton"></a>5.1 Visualizaton</h2><ul><li><p>首先是对于Self-Ensembling中间结果的可视化，如下图</p><p><img src="/2019/12/10/self-ensembing/Figure5.jpg" alt="Figure5"></p><ul><li>图中表明，教师网络能够很好的指导学生网络进行训练。</li><li>此外，根据热力图可以发现，consistency loss在训练中会逐渐关注到物体的轮廓，从而微调轮廓提升预测的效果。</li></ul></li><li><p>其次是，可视化自己的增强数据</p><p><img src="/2019/12/10/self-ensembing/Figure6.jpg" alt="Figure6"></p><ul><li>根据作者的描述，大多数方法都扰乱了物体的轮廓</li><li>甚至，有的方法发生了“负迁移”，就是将天空迁移到了马路上，但作者在这里没提“负迁移”，提了一个叫“spills over”的概念，我思索一下就是“负迁移”。</li><li>从而，得出自己的方法计算得快，在视觉程度上效果还好。</li></ul></li></ul><h2 id="5-2-Analysis-of-self-ensembling-with-per-class-IoUs"><a href="#5-2-Analysis-of-self-ensembling-with-per-class-IoUs" class="headerlink" title="5.2 Analysis of self-ensembling with per-class IoUs"></a>5.2 Analysis of self-ensembling with per-class IoUs</h2><ul><li><p>为了更好的理解self-ensembling，作者比较了在使用self-ensembling下，不同类别精度的提升情况，如下图</p><p><img src="/2019/12/10/self-ensembing/Figure7.jpg" alt="Figure7"></p></li><li><p>可以看到，每个类别提升的程度不同。作者人为原因是各个类别数据的不平衡。数据越多的类，提升的效果也越明显。在Self-ensembling中，这个效果会越明显。因为学生网络持续学习到教师网络的预测，那么将会不断的在稀少类别上做出错误的预测。这也印证了作者说的：教师网络的预测是学生网络的伪标签。</p></li><li>这一块实验个人感觉精华就是：类别越多，提升性能越大</li></ul><h1 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h1><ul><li>作者在文章的结尾，还解释了各种超参数的设计和原因，若是对文章感兴趣的朋友可以自行阅读。</li><li>那么，我在看完这篇文章后，总结如下<ul><li>Domain Adaption可以通过迁移真实数据的知识到虚拟数据中，获得增强后的数据，以缓解基于深度学习的语义分割任务中，对于大量标注数据的需求。</li><li>但是，现有的Domain Adaption方法有以下问题<ul><li>生成的增强图像内容信息损失，如结构紊乱，和源数据的标签不匹配了</li><li>生成的增强图像发生了“负迁移”，导致知识迁移的位置不正确，如“天空”迁移到“马路”上</li><li>现有的Domain Shift方法大多基于Cycle-consistency，参数多耗时耗力</li></ul></li><li>故本文提出了基于GAN的数据增强方法TGCF-DA<ul><li>两个Encoder，一个抽取源数据的内容，另一个抽取目标数据的风格</li><li>抽取到的内容和风格通过AdaIN结合在一起构成Generator，生成fake图像</li><li>fake图像，和目标数据集中的图像作为Discriminator的输入，更新Generator</li></ul></li><li>但仅用TGCF-DA生成的增强数据训练分割网络和其他的方法效果差不多，故本文又引入了Self-ensembling来训练分割网络。稍微不同的是，原始的Self-ensembling改变图像的形状，但本文是给图像注入高斯噪音。因为改变几何形状会破坏图像和mask标签的匹配性</li><li>最后，TGCF-DA + Self-ensembling的结合，在实验上取得了令人瞩目的效果</li></ul></li><li>我认为本文的亮点如下<ul><li>TGCF-DA网络的构造，结合了GAN+任意风格迁移，很新颖</li><li>TGCF-DA + Self-ensembling，说白了就是将Domain Shit方法和Self-ensembling方法结合在一起。</li></ul></li><li>我认为本文未解释清楚的就是，没有做实验证明， 其他的Domain Shift方法和Self-ensembling结合在一起，是否也会得到很好的效果</li></ul><p>最后，感谢本文作者的贡献，respect!</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第二章 K近邻法</title>
      <link href="/2019/12/05/knn/"/>
      <url>/2019/12/05/knn/</url>
      
        <content type="html"><![CDATA[<h1 id="1-题目分析"><a href="#1-题目分析" class="headerlink" title="1. 题目分析"></a>1. 题目分析</h1><p>给定一个二维空间数据集T={正实例：(5, 4), (9, 6), (4, 7)；负实例：(2, 3), (8, 1), (7, 2)}，试基于欧氏距离，找到数据点S(5, 3)的最近邻(k=1)，并对S点进行分类预测</p><ul><li>输入：训练集数据$T = \left\{(5, 4, 1), (9, 6, 1), (4, 7, 1), (2, 3, 2), (8, 1, 2), (7, 2, 2)\right\}$；测试集$S = \left\{(5, 3, _)\right\}$；</li><li>输出：测试集中样本所属类别y；<ul><li>根据欧氏距离，在训练集T中找出与测试集中S最邻近的k个点</li><li>统计这k个点中，类别个数最多的点，作为测试样本的类别</li></ul></li></ul><p>​    k近邻法没有显式的学习过程</p><h1 id="2-算法分析"><a href="#2-算法分析" class="headerlink" title="2. 算法分析"></a>2. 算法分析</h1><p>K近邻完整算法请走<a href="https://github.com/GodWriter/Statistical-Learning-Method/blob/master/Chapter%203/KNN.ipynb" target="_blank" rel="noopener">传送门</a>，下面将拆解分析</p><h2 id="2-1-初始化"><a href="#2-1-初始化" class="headerlink" title="2.1 初始化"></a>2.1 初始化</h2><pre class=" language-lang-python"><code class="language-lang-python"># 定义训练集和测试数据training_set = np.array([[5, 4, 1],                         [9, 6, 1],                         [4, 7, 1],                         [2, 3, 2],                         [8, 1, 2],                         [7, 2, 2]])rows = training_set.shape[0]cols = training_set.shape[1]S = np.array([[5, 3]])</code></pre><ul><li>将训练数据存储为2行3列的array，最后一列是每条数据所属类别</li><li>我们以1作为正例，以2作为负例</li></ul><h2 id="2-2-图示样本"><a href="#2-2-图示样本" class="headerlink" title="2.2 图示样本"></a>2.2 图示样本</h2><pre class=" language-lang-python"><code class="language-lang-python"># 方法：画散点图def plot(sample):    plt.xlabel('x')    plt.ylabel('y')    plt.axis([0, 10, 0, 10])    x = sample[0]    y = sample[1]    cat = sample[2]    if cat == 1:        plt.scatter(x, y, c='r', marker='o')    else:        plt.scatter(x, y, c='b', marker='x')</code></pre><ul><li>首先，定义好坐标轴刻度，都控制在(0, 10)之间</li><li>其次，画点<ul><li>若为正例，画红色圈</li><li>若为负例，画蓝色叉</li></ul></li></ul><h2 id="2-3-欧氏距离"><a href="#2-3-欧氏距离" class="headerlink" title="2.3 欧氏距离"></a>2.3 欧氏距离</h2><pre class=" language-lang-python"><code class="language-lang-python"># 方法：计算样本之间的欧式距离def compute_distance(x_train, x_test):    distance = np.sqrt(np.square((x_train[0] - x_test[0])) +                        np.square((x_train[1] - x_test[1])))    return np.array([distance, x_train[2]])</code></pre><ul><li>即计算训练集中样本和测试集样本的距离</li><li>需要保留训练集样本所属的类别，便于后面比较距离时调用，故返回的是一个array([两点之间距离，训练样本所属类别])</li></ul><h2 id="2-4-遍历样本"><a href="#2-4-遍历样本" class="headerlink" title="2.4 遍历样本"></a>2.4 遍历样本</h2><pre class=" language-lang-python"><code class="language-lang-python"># 遍历训练集并计算距离distance_array = np.zeros([1, 2])for row in range(rows):    x_train = training_set[row, :]    x_test = S[0]    distance = compute_distance(x_train, x_test)    distance_array = np.row_stack((distance_array, distance))    plot(x_train)</code></pre><ul><li>首先，定义一个distance_array，用于保存测试样本与训练集中所有样本的距离</li><li>遍历训练集中的所有样本<ul><li>计算两点之间的欧式距离</li><li>动态扩展distance_array的大小<ul><li>np.row_stack()可在distance_array中加入子项</li><li>即插入行数据，每行数据为compute_distance()返回的结果</li></ul></li><li>同时，将此刻用到的训练集样本在坐标系中画出</li></ul></li></ul><h2 id="2-5-判断类别"><a href="#2-5-判断类别" class="headerlink" title="2.5 判断类别"></a>2.5 判断类别</h2><pre class=" language-lang-python"><code class="language-lang-python"># 选取k值，并得到类别k = 5distance_array = distance_array[np.argsort(distance_array[:, 0])]cat_array = distance_array[1: (k+1), 1].astype(np.int32)max_cat = np.argmax(np.bincount(cat_array))test_value = np.column_stack((S,                               np.array([max_cat],                               dtype=np.int32)))[0]print("The test data belongs to ", max_cat)</code></pre><ul><li>首先，选取K值，这里我们设置为5，即比较所有数据与测试样本的距离</li><li>其次，我们调用np.argsort()将测试样本与训练集中样本的根据距离从小到大进行排序，得到排序后的distance_array并返回</li><li>接着，由于我们初始化distance_array时，默认存入[0, 0]，经排序后放在第一行；故删除第一行，取剩下所有行；且只保留类别信息存入cat_array</li><li>最后，我们调用np.bincount()对cat_array中出现的类别进行统计，并调用np.argmax()取出出现次数最大值，作为测试样本的类别；并将类别加入到测试样本中，便于图示</li></ul><h2 id="2-6-图示样本集"><a href="#2-6-图示样本集" class="headerlink" title="2.6 图示样本集"></a>2.6 图示样本集</h2><pre class=" language-lang-python"><code class="language-lang-python"># 图示所有点的位置plot(test_value)plt.text(test_value[0], test_value[1]+0.2, 'S')plt.show()</code></pre><ul><li><p>先画出测试样本在图中的位置</p></li><li><p>并根据y坐标，标记其符号S</p></li><li><p>整体展示所有样本</p><p><img src="/2019/12/05/knn/plot.png" alt="超平面"></p></li></ul><h1 id="3-Sklearn实现"><a href="#3-Sklearn实现" class="headerlink" title="3. Sklearn实现"></a>3. Sklearn实现</h1><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.neighbors import KNeighborsClassifier# 1. 定义K值k = 5# 2. 根据参数要求得到训练集和测试集x_train = training_set[:, :2]y_train = training_set[:, 2]x_test = S# 3. 进行KNN分类clf = KNeighborsClassifier(n_neighbors=k, n_jobs=1)clf.fit(x_train, y_train)y_predict = clf.predict(x_test)print("The test data belongs to ", y_predict[0])</code></pre>]]></content>
      
      
      <categories>
          
          <category> 统计学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一章 感知机</title>
      <link href="/2019/12/03/perceptron/"/>
      <url>/2019/12/03/perceptron/</url>
      
        <content type="html"><![CDATA[<h1 id="1-题目分析"><a href="#1-题目分析" class="headerlink" title="1. 题目分析"></a>1. 题目分析</h1><p>已知训练数据集D，其正实例点是$x_1 = (3, 3)$，$x_2 = (4, 3)$, 负实例点是$x_3 = (1, 1)^T$，求感知机模型</p><ul><li>输入：训练数据集$T = \left\{(x_1, 1), (x_2, 1), (x_3, 1)\right\}$；学习率$\eta(0 &lt; \eta \leq1)$;</li><li>输出：w, b；感知机模型$f(x) = sign(w\cdot{x} + b)$.<ul><li>选取初值$w_0, b_0$</li><li>在训练集中选取数据$(x_i, y_i)$</li><li>如果$y_i(w\cdot{x} + b) \leq 0$<ul><li>$w \leftarrow w + \eta{y_ix_i}$</li><li>$b \leftarrow b + \eta{y_i}$</li></ul></li><li>转至(2)，直至训练集中没有误分类点</li></ul></li></ul><p>直观上的解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w和b，使得分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。</p><h1 id="2-算法分析"><a href="#2-算法分析" class="headerlink" title="2. 算法分析"></a>2. 算法分析</h1><p>感知机完整算法请走<a href="https://github.com/GodWriter/Statistical-Learning-Method/blob/master/Chapter%202/Perception.ipynb" target="_blank" rel="noopener">传送门</a>，下面将拆解分析</p><h2 id="2-1-初始化"><a href="#2-1-初始化" class="headerlink" title="2.1 初始化"></a>2.1 初始化</h2><pre class=" language-lang-python"><code class="language-lang-python"># 定义数据集data = np.array([[3, 3, 1],                 [4, 3, 1],                 [1, 1, -1]])rows = data.shape[0]cols = data.shape[1]</code></pre><ul><li>将训练数据存储为2行3列的array，最后一列是每条数据的标签</li><li>分别得到矩阵的行数rows和列数cols，为后续遍历做准备</li></ul><pre class=" language-lang-python"><code class="language-lang-python"># 选取初值w0, b0w = np.ones(cols - 1)b = 0thea = 0.001</code></pre><ul><li>初始化$w_0 = (1, 1)$，$b = 0$</li><li>初始化$theta$，用于图示超平面，会在下面讲解</li></ul><h2 id="2-2-判断条件"><a href="#2-2-判断条件" class="headerlink" title="2.2 判断条件"></a>2.2 判断条件</h2><pre class=" language-lang-python"><code class="language-lang-python"># 方法：判别是否存在分类错误的点def sign(data, w, b):    flag = False    rows = data.shape[0]    cols = data.shape[1]    for row in np.arange(rows):        x_i = data[row, :-1]        y_i = data[row, -1]        if y_i * (np.matmul(w, x_i.T) + b) <= 0:            flag = True            break    return flag</code></pre><ul><li>对应算法中的第(3)步，筛选数据集中是否存在样本，使得$y_i(w\cdot{x} + b) \leq 0$成立</li><li>设置$flag$变量，若为True则存在，若为False则不存在</li><li>一旦发现不存在，马上跳出循环</li></ul><h2 id="2-3-图示超平面"><a href="#2-3-图示超平面" class="headerlink" title="2.3 图示超平面"></a>2.3 图示超平面</h2><pre class=" language-lang-python"><code class="language-lang-python"># 方法：画散点图def plot(data, w, b):    plt.xlabel('x1')    plt.ylabel('x2')    plt.axis([0, 6, 0, 6])    x = data[:, 0]    y = data[:, 1]    plt.scatter(x, y)    x_line = [p for p in np.arange(10)]    if w[1] == 0:        y_line = [(-w[0]*x - b)/(w[1] + thea) for x in x_line]    else:        y_line = [(-w[0]*x - b)/w[1] for x in x_line]    plt.plot(x_line, y_line)</code></pre><ul><li><p>首先，定义好坐标轴的刻度，都控制在(0, 6)之间</p></li><li><p>其次，先将数据集中的点画出来</p></li><li><p>接着，根据当前的$w, b$值，选取$x_1, x_2$</p><ul><li><p>$x_1$我们选定为(1, 10)中的整数</p></li><li><p>根据$w_1x_1 + w_2x_2 + b = 0$，已知$x_1$，求得$x_2$如下</p><p>$x_2 = \frac{-W_1x_1 - b}{w_2}$</p></li><li><p><strong>由于$w_2$可能为0，导致$x_2$求值出现问题，故我们加上一个$\theta$变量，一旦$w_2$为0， 则使得$x_2$为</strong></p><p>$x_2 = \frac{-W_1x_1 - b}{w_2 + \theta}$</p></li></ul></li><li><p>最后，画出超平面即可</p></li></ul><h2 id="2-4-遍历求解"><a href="#2-4-遍历求解" class="headerlink" title="2.4 遍历求解"></a>2.4 遍历求解</h2><pre class=" language-lang-python"><code class="language-lang-python"># 遍历数据集while sign(data, w, b):    for row in np.arange(rows):        x_i = data[row, :-1]        y_i = data[row, -1]        if y_i * (np.matmul(w, x_i.T) + b) <= 0:            w = w + y_i * x_i            b = b + y_i    plot(data, w, b)</code></pre><ul><li><p>以$sign()$函数作为条件，判断有无误分类点，一旦有就遍历循环</p><ul><li><p>遍历每一条数据</p><ul><li>若是满足条件$y_i(w\cdot{x} + b) \leq 0$</li><li>更新$w, b$</li></ul></li><li><p>将当前的超平面画出</p><p><img src="/2019/12/03/perceptron/plot.png" alt="超平面"></p></li></ul></li></ul><h1 id="3-Sklearn实现"><a href="#3-Sklearn实现" class="headerlink" title="3. Sklearn实现"></a>3. Sklearn实现</h1><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.linear_model import Perceptron# 1. 定义数据集data = np.array([[3, 3, 1],                 [4, 3, 1],                 [1, 1, -1]])# 2. 定义模型，求解perceptron = Perceptron()perceptron.fit(data[:, :2], data[:, 2])# 3. 打印w,b，并图示print("w: ", perceptron.coef_, "b: ", perceptron.intercept_)plot(data, perceptron.coef_[0, :], perceptron.intercept_[0])# 4. 测试模型准确率res = perceptron.score(data[:, :2], data[:, 2])print("correct rate:{:.0%}".format(res))</code></pre>]]></content>
      
      
      <categories>
          
          <category> 统计学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>N way to write</title>
      <link href="/2019/09/25/pape-word/"/>
      <url>/2019/09/25/pape-word/</url>
      
        <content type="html"><![CDATA[<h1 id="1-常用词的N种说法"><a href="#1-常用词的N种说法" class="headerlink" title="1. 常用词的N种说法"></a>1. 常用词的N种说法</h1><h2 id="1-1-Besides"><a href="#1-1-Besides" class="headerlink" title="1.1 Besides"></a>1.1 Besides</h2><ul><li>Moreover</li><li>In addition</li></ul><h2 id="1-2-Get"><a href="#1-2-Get" class="headerlink" title="1.2 Get"></a>1.2 Get</h2><ul><li>obtain</li></ul><h2 id="1-3-Show"><a href="#1-3-Show" class="headerlink" title="1.3 Show"></a>1.3 Show</h2><ul><li>exhibit</li><li>demonstrate</li><li>present</li></ul><h2 id="1-4-Improve"><a href="#1-4-Improve" class="headerlink" title="1.4 Improve"></a>1.4 Improve</h2><ul><li>be boosted to</li></ul><h2 id="1-5-Compare"><a href="#1-5-Compare" class="headerlink" title="1.5 Compare"></a>1.5 Compare</h2><ul><li>In contrast</li></ul><h2 id="1-6-Can"><a href="#1-6-Can" class="headerlink" title="1.6 Can"></a>1.6 Can</h2><ul><li>allow to</li><li>attempt to </li><li>be able to</li><li>be exploited for</li><li>play the role of</li><li>be capable of </li></ul><h2 id="1-7-Many"><a href="#1-7-Many" class="headerlink" title="1.7 Many"></a>1.7 Many</h2><ul><li>diverse    多种多样的</li><li>multiple</li></ul><h2 id="1-8-Motivated-by"><a href="#1-8-Motivated-by" class="headerlink" title="1.8 Motivated by"></a>1.8 Motivated by</h2><ul><li>Inspired by</li></ul><h2 id="1-9-Suppose"><a href="#1-9-Suppose" class="headerlink" title="1.9 Suppose"></a>1.9 Suppose</h2><ul><li>assume</li></ul><h2 id="1-10-Use"><a href="#1-10-Use" class="headerlink" title="1.10 Use"></a>1.10 Use</h2><ul><li>utilize</li><li>employ</li><li>apply … to …</li><li>enforce</li><li>be easily integrated with</li><li>deploy … as …</li><li>be taken as </li><li>be employed as</li></ul><h2 id="1-11-solve"><a href="#1-11-solve" class="headerlink" title="1.11 solve"></a>1.11 solve</h2><ul><li>address</li></ul><h2 id="1-12-define"><a href="#1-12-define" class="headerlink" title="1.12 define"></a>1.12 define</h2><ul><li>refer to sth as sth</li><li>sth be identical to sth</li><li>be represented as </li></ul><h2 id="1-13-for-example"><a href="#1-13-for-example" class="headerlink" title="1.13 for example"></a>1.13 for example</h2><ul><li>for instance</li><li>take … as an example</li></ul><h2 id="1-14-come-from"><a href="#1-14-come-from" class="headerlink" title="1.14 come from"></a>1.14 come from</h2><ul><li>drift from</li></ul><h2 id="1-15-aim-to"><a href="#1-15-aim-to" class="headerlink" title="1.15 aim to"></a>1.15 aim to</h2><ul><li>for the purpose of</li></ul><h1 id="2-专有名词"><a href="#2-专有名词" class="headerlink" title="2. 专有名词"></a>2. 专有名词</h1><h2 id="2-1-Image-to-Image-Translation"><a href="#2-1-Image-to-Image-Translation" class="headerlink" title="2.1 Image-to-Image Translation"></a>2.1 Image-to-Image Translation</h2><h3 id="2-1-1-Word"><a href="#2-1-1-Word" class="headerlink" title="2.1.1 Word"></a>2.1.1 Word</h3><ul><li>stylised image    转换后的风格图</li><li>paired image    成对的图片</li><li>be transfered to</li><li>a fixed target style</li><li>multimodal translation    多迁移</li><li>deconvolution layer  反卷积层</li><li>stable training</li><li>uniform sampling</li><li>randomly sampled from</li></ul><h3 id="2-1-2-Advantage"><a href="#2-1-2-Advantage" class="headerlink" title="2.1.2 Advantage"></a>2.1.2 Advantage</h3><ul><li>deterministic one-to-one mapping transfer a soure image into the target style</li><li>these works can be divided into two categories according to the controllabiliy of the target styles</li><li>transfer the images in S into the style of T</li><li>transfer source images into the target style</li><li>By jointly optimizing all modules, CycleGAN model is able to transfer source images into the target sytle and v.v.</li><li>We take the S-&gt;T direction as an example, and the other direction can be similarly applied.</li><li>After the model is learnt, source images can only be translated to a fixed style</li></ul><h3 id="2-1-3-Disadvantage"><a href="#2-1-3-Disadvantage" class="headerlink" title="2.1.3 Disadvantage"></a>2.1.3 Disadvantage</h3><ul><li>Most previous studies for GAN-baed Image-to-Image translation methods rely on various forms of cycle-consistency$^{[1]}$.</li></ul><h2 id="2-2-Domain-Adaption"><a href="#2-2-Domain-Adaption" class="headerlink" title="2.2 Domain Adaption"></a>2.2 Domain Adaption</h2><h3 id="2-2-1-Word"><a href="#2-2-1-Word" class="headerlink" title="2.2.1 Word"></a>2.2.1 Word</h3><ul><li>source domain    源域</li><li>target domain     目标域</li><li>intermediate domain    中间域</li><li>generalization ability    生成能力</li><li>domainness variable    邻域变量</li><li>pixel level</li><li>synthetic data    合成数据</li><li>real scenario  真实场景</li><li>be proportional to  和…成比例</li><li>recover … from …</li><li>be implemented with  被实施</li><li>spread along … from … to …</li><li>cross-domain semantic segmentation problem</li><li>a mixture of different target styles</li><li>shift from … to …</li><li>domain gap</li><li>domain alignment</li><li>distribution difference</li><li>semantic constraint  语义约束</li></ul><h3 id="2-2-2-Advantage"><a href="#2-2-2-Advantage" class="headerlink" title="2.2.2 Advantage"></a>2.2.2 Advantage</h3><ul><li>Domain adaptation aims to utilize a labeled source domain to learn a model that performs well on an unlabeled target domain</li><li>Domain generalization aims to learn a model that could be generalized to an unseen target domain by using multiple labeled source domains</li><li>align the image distributions for two domains</li><li>Object function of GAN can be seen as a lower bound of the Hessen-Shannon divergence</li><li>translate each source image into an arbitrary intermediate domain</li><li>Unsupersived domain adaption seeks to adapt the model trained on the source domain to the target domain$^{[1]}$.</li><li>Self-ensembling is composed of a teacher and a student network, where the student is compelled to produce consistent predictions provided by the teacher on target data$^{[1]}$.</li></ul><h3 id="2-2-3-Disadvantage"><a href="#2-2-3-Disadvantage" class="headerlink" title="2.2.3 Disadvantage"></a>2.2.3 Disadvantage</h3><ul><li>require a large amount of data with pixel-level annotations</li><li>The adversarial loss may trigger a negative transfer, which aligns the target feature with the source feature in an incorrect semantic category$^{[1]}$.</li><li></li></ul><h2 id="2-3-Style-Transfer"><a href="#2-3-Style-Transfer" class="headerlink" title="2.3 Style Transfer"></a>2.3 Style Transfer</h2><h3 id="2-3-1-Word"><a href="#2-3-1-Word" class="headerlink" title="2.3.1 Word"></a>2.3.1 Word</h3><ul><li>style patterns</li><li>the iterative optimization process</li><li>feedforward methods</li><li><strong>insufficient visual quality</strong></li><li>global feature statistics</li><li>in the feature space</li><li>encoder-decoder module</li><li>global color distribution</li><li>texture</li><li>local style patterns</li><li>bush strokes</li><li>receptive field</li></ul><h3 id="2-3-2-Sentence"><a href="#2-3-2-Sentence" class="headerlink" title="2.3.2 Sentence"></a>2.3.2 Sentence</h3><ul><li>Arbitrary style transfer aims to synthesize a content image with the style of an image to create a third image that has never been seen before.</li><li>The ultimate goal of arbitrary style transfer is to simultaneously achieve and preserve generation, quality, and efficiency.</li><li>We use the encoder(a pre-trained VGG-19) to compute the loss function for training …</li></ul><h1 id="3-常用短语"><a href="#3-常用短语" class="headerlink" title="3. 常用短语"></a>3. 常用短语</h1><ul><li>draw increasing attention</li><li>most existing works</li><li>the benefit of</li><li>training phase    训练阶段</li><li>inference phase    测试阶段</li><li>be injected into    被注入</li><li>perform… by …</li><li>network structure</li><li>tent to </li><li>at the beginning</li><li>at the end</li><li>each of sth</li><li>computer vision tasks</li><li>achieve good performance</li><li>fill the gap between .. and …</li><li>conduct experiments on </li><li>training policy</li><li>verify the ability of </li><li>as expectedoop</li><li><strong>give comparable result</strong></li><li>by further using</li><li>from the first column, …</li><li>find it challenging to …</li><li>the seminal work of   …的开创性工作</li><li>computational cost of …</li><li>computationally expensive operations</li><li>the proposed model</li><li>network structure</li><li>experimental settings</li><li>less appealing</li><li>run time performance</li><li>in real time</li><li>be compelled to 被迫</li></ul><h2 id="3-1-Solve-the-problem"><a href="#3-1-Solve-the-problem" class="headerlink" title="3.1 Solve the problem"></a>3.1 Solve the problem</h2><ul><li>To adress this challenging issue</li><li>To overcome this limitation</li></ul><h1 id="4-常用衔接语"><a href="#4-常用衔接语" class="headerlink" title="4. 常用衔接语"></a>4. 常用衔接语</h1><ul><li>on one hand…, on the other hand</li><li>Unlike the above work</li><li>Our work is partialy inspired by</li><li>Formally, …</li><li>In particular, …</li><li>In this way, …</li><li>When …., …; and when, …</li><li>no longer aim to …, but to …</li><li>Specifically</li><li>Due to the usage of …</li><li>With regard to …</li><li>The motivation is as follows, …</li><li>Accordingly, …</li><li>In other words, …</li><li>In this section, …</li><li>It can be observed that</li><li>More interestingly, …</li><li>As discussed in </li><li>Note, …</li><li>In this paper, …</li><li>Experimental results demonstrate that …</li><li>Significant efforts have been made to …</li><li>Despite valuable efforts</li><li>Despite recent advances, …</li><li>In many cases, …</li><li>Similar to …</li></ul><h1 id="5-长句型"><a href="#5-长句型" class="headerlink" title="5. 长句型"></a>5. 长句型</h1><ul><li><strong>Specifically</strong>, we <strong>introduce two</strong> discriminators, $D_S(x)$ <strong>to</strong> distinguish $M^{(z)}$ and S, and $D_T(x)$ <strong>to </strong>distinguish $M^{(z)}$ and T, <strong>respectively</strong>.</li><li>… in two settings. In the first setting, … . In the second setting, … .</li><li>Two cases are considered, … . For the first case, … . For the second case, … .</li><li>… in two scenarios. Firstly, … . Secondly, … .</li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li>Choi, Jaehoon &amp; Kim, Taekyung &amp; Kim, Changick. (2019). Self-Ensembling with GAN-based Data Augmentation for Domain Adaptation in Semantic Segmentation. </li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Faster-RCNN</title>
      <link href="/2019/09/07/faster-rcnn/"/>
      <url>/2019/09/07/faster-rcnn/</url>
      
        <content type="html"><![CDATA[<h1 id="1-几个问题"><a href="#1-几个问题" class="headerlink" title="1. 几个问题"></a>1. 几个问题</h1><ul><li>Anchor和全卷积输出值之间的关系</li><li>“回归系数”是什么，有什么用？</li><li>在选择RPN Boxes时，既要考虑RPN Boxes与Ground Truth的IOU来筛选，又要考虑每个RPN boxes为物体前景的概率。那么在筛选RPN Boxes的时候，哪个先考虑，哪个后考虑，还是说同时考虑？如果说是同时考虑，又怎么考虑？</li></ul><h1 id="2-Anchor-Generation-Layer"><a href="#2-Anchor-Generation-Layer" class="headerlink" title="2. Anchor Generation Layer"></a>2. Anchor Generation Layer</h1><ul><li>Anchor主要为了在输入的图像上产生多个可能存在物体的bounding box，所以Anchor肯定是要分布在整个图像上，才能包括所有的物体。那么，在Faster-RCNN中Anchor是如何产生的呢？Anchor的产生下图所示：</li></ul><p><img src="/2019/09/07/faster-rcnn/Anchor-Generation-Layer.png" alt></p><ul><li>初学者很容易犯的错误就是：Anchor其实是神经网络产生的，最后要通过训练才能得到Anchor及图中那么多bounding box。其实不然，Anchor就是通过机械化的操作产生覆盖整张图的bounding box，数量多且杂。<strong><font color="red">和Anchor有关的神经网络做的并不是产生bounding box，而是产生相对应的bounding box的回归系数，回归系数正好也是4个值</font></strong>。我们先来理解什么是所谓的“机械化”，再来理解“回归系数”。</li><li>对“机械化”的理解可以从上图解释。Faster-RCNN设置了aspect ratio(0.5, 1, 2)，以及Anchor Scales(8, 16, 32)。大部分博客说到这里就结束了，他们会说3x3=9，最后一共产生9个anchors。那么到底是怎么计算的呢？详细可以看我通过源码阅读得到的计算过程，传送门在<a href="https://godwriter.github.io/2019/09/03/fast-rcnn-code/#more">这里</a>。如下面公式所示：</li><li>综上，我们知道了<ul><li>Anchors不是神经网络产生的，而是人为设计的，通过指定<strong>aspect ratio</strong>和<strong>anchor scales</strong>两个参数来生成。</li><li>Anchors的原始比例是定义在特征图上的，若是返回到原图的话，需要乘以相应的倍数</li><li><font color="red">仍有一个问题没有解决，Anchors和回归系数之间的关系？我们继续往下看</font></li></ul></li></ul><h1 id="3-Region-Proposal-Layer"><a href="#3-Region-Proposal-Layer" class="headerlink" title="3. Region Proposal Layer"></a>3. Region Proposal Layer</h1><ul><li><p>在Faster-RCNN之前，R-CNN通过selective search method来生成region proposal，但是速度慢、效率低，故Faster-RCNN通过Anchors来生成候选框，如上述。然而，Anchors生成的框就可以直接用了吗？我们看到那么多密密麻麻的生成框，而且还是最最简单的通过比例放缩得到的，肯定是需要进行筛选和回归的。那么，<strong>Region Proposal Layer就是完成了筛选和回归的工作</strong>，具体如下：</p><ul><li>从一系列生成的anchors中，确定哪些是前景，哪些是背景</li><li>通过一系列的<strong>“regression coefficients”即回归系数</strong>修正anchors的位置、宽度及高度，来提升anchors的质量。</li></ul><p>Region Proposal Layer一共包含三层：Proposal Layer， Anchor Target Layer以及Proposal Target Layer。整体结构如下图所示：</p><p><img src="/2019/09/07/faster-rcnn/Region-Proposall-Layer.png" alt></p><p>如上图所示，rpn_net通过两个1x1的卷积核，输出两个支路：rpn_cls_score_net以及rpn_bbx_pred_net，它们的高度、宽度相同，唯一不同的是深度、即通道个数。rpn_cls_score_net的通道个数为2x9=18，因为它指的是feature map上每个单元格产生的9个anchor是前景和背景的概率。rpn_bbx_pred_net的通道个数为4x9=36，因为它生成的是feature map上每个单元格产生的9个anchor的<strong>回归系数</strong>。</p></li></ul><h1 id="4-Bounding-Box-Regression-Coefficients-回归系数"><a href="#4-Bounding-Box-Regression-Coefficients-回归系数" class="headerlink" title="4. Bounding Box Regression Coefficients(回归系数)"></a>4. Bounding Box Regression Coefficients(回归系数)</h1><p>​    在详细讲解Region Proposal Layer三层结构之前，我们把困扰到现在的一个问题解决，那就是：这个回归系数究竟是什么？和Anchors有什么联系？</p><ul><li><p>“回归系数”即<font color="red"><strong>对anchors进行微调，使anchors更加接近ground truth的一组参数($t_x, t_y, t_w, t_h$)</strong></font>。再用大白话解释一下，我们通过anchors“机械”方式生成的RPN bboxes就算能够和ground truth比较接近，但是仍存在偏差，那么就需要一组参数对这些RPN bboxes进行回归，这也就是我们的“回归系数”bounding box regression coefficients。</p></li><li><p>那么，这些回归系数该怎么计算呢？我们先做如下定义</p><ul><li><p>$(T_x, T_y, T_w, T_h)$为目标边界框，即目标边界框的(左上角坐标x，左上角坐标y，宽，高)</p></li><li><p>$(O_x, O_y, O_w, O_h)$为原始边界框，即原始边界框的(左上角坐标x，左上角坐标y，宽，高)</p></li><li><p>那么$(t_x, t_y, t_w, t_h)$的计算过程如下：</p><p>​    $t_x=\frac{T_x-O_x}{O_w}$           $t_y=\frac{T_y - O_y}{O_h}$           $t_w=log(\frac{T_w}{O_w})$          $t_h=log(\frac{T_h}{O_h})$</p></li><li><p>若是不了解原始框和目标框的定义，不妨将原始框看作RPN Bbox，将目标框看作Ground Truth Bbox。这样就可以通过Anchors生成的RPN Bbox及它们各自对应的Ground Truth Bbox来计算回归系数。</p></li></ul></li><li><p>在了解了什么是bounding box regression coefficients后，我们需要注意一点：当一个图片未做过剪切，只是做了仿射变换如：放大、缩小，那么该图中的bbox所对应的回归系数是不变的，因为做了等比例的转换。为什么要强调这一点，因为在后面计算分类损失的时候，目标的回归系数将按照原始纵横比计算；而分类网络输出的回归系数，是基于方块特征图经ROI池化后计算得到的。下图解释的很明白</p><p><img src="/2019/09/07/faster-rcnn/regression-coefficients.png" alt></p></li></ul><p>​    通过对回归系数的理解后，我们可坚定的知道：<font color="red">rpn_bbx_pred_net输出的并不是一个框的坐标，而是每个RPN box所对应的回归系数，来调整自身的位置更好的接近Ground Truth</font>。</p><h1 id="5-Region-Proposal-Layer"><a href="#5-Region-Proposal-Layer" class="headerlink" title="5. Region Proposal Layer"></a>5. Region Proposal Layer</h1><p>Region Proposal Layer一共包含三层：Proposal Layer， Anchor Target Layer以及Proposal Target Layer。下面，我们将一层层来理解。</p><h2 id="5-1-Proposal-Layer"><a href="#5-1-Proposal-Layer" class="headerlink" title="5.1 Proposal Layer"></a>5.1 Proposal Layer</h2><ul><li>Proposal Layer使用基于前景分数的non-maximum suppression来筛选anchors生成的RPN box的数量。</li><li>此外，还要<strong>将“回归系数”应用到RPN boxes上来生成变换后的bounding boxes</strong>。可以通过逆推公式获得bounding boxes。这里推出的bounding box是所谓的Target boudning box，是网络的预测值，并不是Groud truth box。公式里的原始边界框指的是RPN boxes。</li><li>Proposal Layer的工作如下图所示，具体细节待补充：</li></ul><p><img src="/2019/09/07/faster-rcnn/Proposal-Layer.png" alt></p><ul><li>从图中的输出来看，此时的ROIs已经是变换坐标后的bounding box。在做非极大值抑制之前，就已经完成了坐标的转换。</li></ul><h2 id="5-2-Anchor-Target-Layer"><a href="#5-2-Anchor-Target-Layer" class="headerlink" title="5.2 Anchor Target Layer"></a>5.2 Anchor Target Layer</h2><ul><li><p>Anchor Target Layer的目的就是选择有前途的RPN boxes，用来训练RPN网络，使得RPN网络达到以下两个功能：</p><ul><li>能够区分前景和背景</li><li>为作为前景的RPN boxes生成好的“回归系数”</li></ul></li><li><p><strong>RPN的损失</strong>。为了更好的理解Anchor Target Layer，我们首先要看一下RPN的损失是怎么计算的。RPN层的主要目的是：<strong>生成更好的bounding boxes</strong>。故从一群RPN Boxes中，RPN层必须学会区分前景和背景，且要能计算“回归系数”来调整前景boxes的位置，宽度和高度，以得到一个“更好”的前景box。故RPN Loss的设计也是为了这个目的。</p><ul><li><p>RPN总损失由两项损失构成：分类损失，bbox回归损失。</p><ul><li><p>分类损失使用<strong>交叉熵</strong>来惩罚错误分类的bbox</p><p>$CrossEntropy(Predicted<em>{class}, Actual</em>{class})$</p></li><li><p>bbox回归损失使用距离函数度量<strong>真实回归系数（与作为前景的RPN box最为接近的ground truth box，通过上文提到的公式计算得到）</strong>和<strong>网络预测的回归系数</strong>。$L_{loc}$对所有前景RPN boxes的回归损失求和。作为背景的RPN box不需要进行求和，因为没有真实标签，故没意义。</p><p>$L<em>{loc} = \sum</em>{u\in{all \ foreground \ anchors}}l_u$</p></li></ul></li><li><p>$l_u$的计算公式如下所示，计算的是RPN预测的“回归系数”和“真实回归系数”（使用离RPN bbox最近的ground truth得到）之间的差。</p><p>$l<em>u = \sum</em>{i\in{x, y,w,h}}smooth_{L1}(u_i(predicted) - u_i(target))$</p><p>这里的$u_i$展开来就是$(u_x, u_y, u_w, u_h)$，smooth L1函数如下所示：</p><p><img src="/2019/09/07/faster-rcnn/Users/18917/Documents/备份/formula.png" alt></p><p>这里的$\sigma$是任意选择的，为了避免for-if循环，将会使用一个掩码矩阵来计算损失</p></li><li><p>根据上述损失的定义，我们需要计算下面的量</p><ul><li>RPN boxes的类别标签（前景或者背景）及得分，计算类别损失</li><li>前景RPN boxes的目标回归系数，计算位置损失</li></ul></li><li><p>为了得到上面的量，我们将进行以下操作</p><ul><li>首先，选择位于图像范围内的RPN boxes</li><li>其次，通过计算所有RPN boxes和Ground Truth之间的<strong>IOU</strong>重叠来选好的前景框；通过这个<strong>IOU</strong>，两种类型的RPN bbox将会被标记为前景框<ul><li>类型A：对于每个ground truth box，所有与他们IOU值最大的RPN Boxes</li><li>类型B：对于每个ground truth box，所有与他们IOU值超阈值的RPN Boxes</li></ul></li><li>最后，图示这些盒子</li></ul><p><img src="/2019/09/07/faster-rcnn/foreground.png" alt></p></li><li><p>需要注意，只有与Ground Truth的IOU值超过阈值的RPN Boxes才是前景框。这样做是为了避免给RPN带来<strong>“无望的学习任务”</strong>，即由于ground truth距离太远，而导致学习到的“回归系数”过大。同理，IOU值小于负阈值的RPN Boxes被标记为背景框。</p></li><li><p>对于每一个RPN Boxes，并不是只分：前景、背景，还有一个类别：不关心(don’t care)，即又不是前景又不是背景，这些盒子不包括在RPN损失的计算中。当然，<strong>前景框和背景框也有数量限制</strong>，比如说我各取128个。那么，将会从通过测试的前景框或背景框中，随即将多余出来的框标定为“don’t care”。</p></li><li><p>得到用于计算<strong>RPN Loss</strong>的候选框之后，就可以通过公式来计算出<strong>Target regression coefficients</strong>。<font color="red">这里用于计算位置损失的是“回归系数”，其实在理解Faster-RCNN的过程中，这个损失函数一直在困扰我。到底应该是“回归系数”，还是通过上面的公式变换得到的bbox坐标  (左上角x，左上角y，宽，高) </font>但不管是哪一个，通过代码可以详细了解，而且只是值的选择不同，并不影响了解Faster-RCNN的整体流程。</p></li></ul></li><li><p>通过对RPN Loss的理解，我们知道了Anchor Target Layer的流程，我们最后再总结一下</p><ul><li>Anchor Target Layer的输入<ul><li>RPN网络的两路输出：预测的前景背景分数，回归系数</li><li>通过人工机械生成的RPN Boxes</li><li>Ground Truth Boxes</li></ul></li><li>Anchor Target Layer的输出<ul><li>用于计算RPN Loss的前景和背景框，以及与之对应的类别标签</li><li>目标回归系数</li></ul></li></ul></li></ul><h2 id="5-3-Calculating-Classification-Layer-Loss"><a href="#5-3-Calculating-Classification-Layer-Loss" class="headerlink" title="5.3 Calculating Classification Layer Loss"></a>5.3 Calculating Classification Layer Loss</h2><p>​    我们首先来了解如何计算分类层的损失以及哪些信息用来计算分类损失，以方便后续更容易了解proposal target layer, ROI Pooling layer。</p><ul><li><p>和RPN Loss相似，Classification Layer Loss由两项损失组成</p><ul><li>Classification Loss</li><li>Bounding Box Regression Loss</li></ul><p>但，RPN Layer和Classificaiton Layer最大的区别就是：<strong>RPN层只需要处理两个类别：前景和背景；而分类层需要被训练处理所有的目标类别(+背景)</strong></p></li><li><p><strong>Classification Loss</strong>。分类的损失也是交叉熵，如下图所示。Class Scores是预测类别的得分，行数代表样本数量，列数代表总类别个数。$C_i$是每一行代表的样本真正的类别标签。0是背景类别。最后，通过Cross entropy loss，将上述参数输入，得到最后的损失值。</p></li></ul><p><img src="/2019/09/07/faster-rcnn/classification-layer-loss.png" alt></p><p><img src="/2019/09/07/faster-rcnn/classification-layer-loss-formula.png" alt></p><ul><li><strong>Bounding Box Regression Loss</strong>。边框回归损失和RPN Loss计算方法类似，但是“回归系数”是特定于类别的。神经网络将会<strong>为每个对象类都计算回归系数</strong>，但显然，最后的目标回归系数只适用于正确的类，即与该RPN Box重叠最大的Ground Truth框的类别。在计算损失时，使用一个掩码矩阵，它为每个使用到的RPN Box标记正确的对象类。那么，不正确对象类所对应的回归系数将被忽略，这个损失可用矩阵乘法来做，而不需要for-each循环。</li><li>在了解了Classification Layer如何计算之后，我们需要提供以下变量以计算损失<ul><li>分类网络的输出：预测的类别标签，回归系数</li><li>每个RPN Boxes的类别标签</li><li>目标框的回归系数</li></ul></li></ul><h2 id="5-4-Proposal-Target-Layer"><a href="#5-4-Proposal-Target-Layer" class="headerlink" title="5.4 Proposal Target Layer"></a>5.4 Proposal Target Layer</h2><p>​    在了解了Classificaiton Layer的任务和需求之后，我们再先讨论一下Proposal Target Layer。</p><ul><li><p>Proposal Target Layer要做的就是：从Propsoal Layer输出的一系列ROIs中选择有前途的ROIs。这些ROIs将会被用来在头网络提供的特征图中进行裁剪，然后被传递到网络的剩余部分，用于计算预测类别得分和框回归系数。</p></li><li><p>和Anchor Target Layer类似，如何选择传递到分类层的ROIs很重要，如那些与Ground Truth有明显重叠的ROIs，否则我们将要求分类层学习一个“无望的学习任务”。选择合适的ROIs的原则如下：</p><ul><li>计算所有ROIs与Ground Truth的重叠值(IOU)，将ROIs分为背景和前景。前景ROIs是IOU超过阈值的ROIs，<strong>背景ROIs是重叠值落在最低阈值和最高阈值之间的ROIs</strong>。这是一个“hard negative ”的典型例子，为的是给分类器提供困难的背景例子。</li><li>额外的逻辑保证前景和背景区域总数恒定。如发现背景区域太少，则尝试随机重复一些背景指数来填补数量的不足。</li></ul></li><li><p>当选择到合适的ROIs后，需要计算每个ROI和离之最近的Ground Truth之间的边界框回归系数（<strong>包括背景ROIs，因为这些ROI也存在与之重叠的Ground Truth</strong>,与前面选择背景ROI的原则互相呼应）。这些回归系数将被扩展到所有类，如下图所示。</p><ul><li>一共有N个ROIs的“回归系数”，根据它们与各自Ground Truth重叠大小，自高向下排列，放在列表最末尾的是标记为“背景”的ROIs的“回归系数”。目标回归系数也是N行，但是每行是(类别数 x 4)，根据每个Ground Truth对应的类别标签，制作bbox_targets。真实类别处为目标回归系数(通过ROIs和Ground Truth计算得到)，其余都为0。背景ROIs的的回归系数都是0，所以也不会参与损失的计算。</li><li>bbox_inside_weights作为掩码，大小和bbox_targets一致，但是只在每个ROI正确的类别对应的位置值为1，其余值为0。对于背景ROIs，它的值也全为0。因此，<strong>当计算classification layer损失中的Bounding Box Regression Loss时，只考虑前景区域的回归系数</strong>。而<strong>计算Classification Loss的时候，前景和背景同时都要考虑</strong>。</li></ul><p><img src="/2019/09/07/faster-rcnn/ROIs.png" alt></p></li></ul><p><strong>讲到这里，很多童鞋都会有疑惑，怎么又要选择合适的框ROIs了？RPN那里不是选择了256个合适的RPN Boxes了吗，还分了前景和背景，这里为何又要重复采样，多此一举？其实，初学者都会有这样的疑惑，导致他们对Faster-RCNN的理解止步于此，因为太混乱了，就假装自己看懂了，不多想，我也曾是其中的一员。就算想通过Faster-RCNN的代码来了解详细的流程，结果又被复杂琐碎的代码劝退。</strong><font color="red"><strong>如果有以上疑问，我想下面的解释会让你们茅塞顿开。要始终记住，Faster-RCNN虽然是End-2-End，但它是2-stage，不像SSD、YOLO等是1-stage。2-stage的意思就是，训练分两个阶段：首先训练RPN，达到检测目的；其次训练网络剩余部分，达到识别目的。那么，我们虽然在训练RPN的时候就完成了前景、背景框的筛选，如最后挑选出256个既包括前景又包括背景的RPN Boxes，但它们是用来训练RPN的，都是阶段1做的事情，也就说和阶段2：Classification Layer层的训练是没有关系的。当你的RPN训练的很好的时候，就能够得到比较准确的前景、背景分数及回归系数，这时候我们再开始训练Classificaiton Layer，也就是说开始阶段2了。由于阶段1和阶段2分开了，阶段2也拿不到那256个RPN Boxes用于训练RPN的样本，所以阶段2需要重新采样，经过筛选得到比较好的用于训练Classification Layer的ROIs样本框。</strong></font></p><ul><li>最后我们再总结一下Proposal Target Layer做的事情<ul><li>Proposal Target Layer的输入<ul><li>proposal layer提供的ROIs</li><li>Ground Truth信息</li></ul></li><li>Proposal Target Layer的输出<ul><li>符合重叠标准的前景和背景ROIs</li><li>确定好类别的ROI目标回归系数</li></ul></li></ul></li></ul><h1 id="6-Crop-Pooling"><a href="#6-Crop-Pooling" class="headerlink" title="6. Crop Pooling"></a>6. Crop Pooling</h1><ul><li><p>Proposal target layer为我们提供了多个有效的，用于训练的ROIs来计算相关的类别标签和回归系数。下一步就是从头网络生成的特征图中抽取与这些ROIs相关的特征，并用剩余网络的来得到每个ROI所代表的：物体类别的概率分布，回归系数。<strong>Crop Pooling layer</strong>的工作就是从卷积特征图中进行特征抽取。</p></li><li><p>在crop pooling后的关键想法是<strong>Spatial Transformation Networks</strong>来抽取特征。CNN提取特征时，通常需要考虑输入样本的局部性、平移不变性、缩小不变性及旋转不变性，即图像的裁剪、平移、缩放、旋转。而实现这些方法就是对图像进行空间坐标变换，如<strong>仿射变化</strong>。<strong>SNT</strong>以一种统一的结构，自适应实现这些变化。SNT不需要关键点的标定，能根据分类或者其他任务自适应地将数据进行空间变换和对齐。幸运的是，Pytorch提供了两个API来实现：torch.nn.functional.affine_grid，torch.nn.functional.grid_sample。</p></li><li><p>Crop Pooling地步骤如下</p><ul><li>ROIs的坐标表示是相对于原图尺寸地(800 x 600)，为了将这些坐标带入输出特征图的空间上，我们必须要将它们除以步长（一般为16）。</li><li>为了使用上述的API，我们需要仿射变换矩阵。我们需要目标特征图上的x,y维度点的数量，这个由配置参数cfg.POOLING_SIZE提供。因此，在crop pooling中，非正方形地ROIs会被用来在特征图上裁剪区域，并转变成固定大小的正方形窗口。这种转换必须要做，因为Crop Pooling地输出将会被进一步传递到卷积和全连接层，而它们需要固定维度特征。</li></ul><p><img src="/2019/09/07/faster-rcnn/crop-pooling.png" alt></p></li></ul><h1 id="7-Classification-Layer"><a href="#7-Classification-Layer" class="headerlink" title="7. Classification Layer"></a>7. Classification Layer</h1><ul><li><p>crop pooling层接收proposal target layer的ROIs及头网络输出的特征图作为输入，输出平方特征图。这个特征图将被传入到4层ResNet在空间维度上做平均池化，对于每个ROI，结果将会是一维特征向量，过程如下：</p><p><img src="/2019/09/07/faster-rcnn/Classification-Layer.png" alt></p></li><li><p>特征图被传递到两个全连接层：bbox_pred_net，cls_score_net。cls_score_net为每个bounding box产生类别分数（可通过softmax转变成概率矩阵）。bbox_pred_net生成特定类别下的边框回归系数，这些回归系数将与proposal target layer的原始边框相结合，生成最终的边界框。</p><p><img src="/2019/09/07/faster-rcnn/Classification-Layer2.png" alt="zui&#39;ha"></p></li><li><p>最好回忆一下两组回归系数之间的差异：RPN网络产生的，classification network产生。</p><ul><li>第一组RPN的回归系数用来训练RPN网络来得到更好的前景盒子（更加紧密的围绕目标边界）。此时目标回归系数，即由anchor得到的RPN Boxes与其最匹配的Ground Trouth计算得到。</li><li>第二组回归系数由Classification Layer生成。这些系数是特定于类别的，即每个对象类都会产生一组回归系数，最后选择正确类别的系数即可。目标回归系数将在Proposal Target Layer得到，也是通过Ground Truth匹配得到，但是需要制作成特定的形式。<strong>值得注意的是，由于仿射变化SNT使得classification network是在正方形的特征图上操作的，故可能会造成回归系数也会收到影响的误解。然而，由于回归系数对无剪切的仿射变化是不变的，因此Proposal target layer和Classification Layer之间的回归系数依然可以进行比较，作为有效的学习信号。</strong></li></ul></li><li><p>值得注意的是，训练Classification Layer时，错误的梯度也会反向传播到RPN网络。这是因为在Crop Pooling时，使用的ROIs本身就是网络的输出，是RPN网络生成回归系数应用到RPN Box的结果。故在反向传播期间，误差梯度将通过crop pooling传播到RPN层。计算和应用这些梯度很难实现，但Pytorch提供了很好的crop pooling API，方便了细节的处理。</p></li></ul><h1 id="8-实现细节：推理"><a href="#8-实现细节：推理" class="headerlink" title="8. 实现细节：推理"></a>8. 实现细节：推理</h1><ul><li><img src="/2019/09/07/faster-rcnn/Inference.png" alt></li><li>Anchor target和Proposal Target Layer不参与此过程。RPN网络将RPN Boxes分类成：前景、背景，并生成良好的框回归系数。Proposal Layer仅仅将回归系数应用到RPN Boxes并进行非极大值抑制来消除大量重叠的框，最后的剩余的框将被送到Classification Layer生成类得分及基于类别的边框回归系数。</li><li><img src="/2019/09/07/faster-rcnn/Inference1.png" alt></li></ul><p>​        红框显示的是排名前6的RPN Boxes。绿框表示应用RPN Boxes的回归系数后的框，显然绿框更加适合目标。在应用回归系数后，矩形依然是矩形，即存在显著的重叠，这种冗余需要通过非极大值抑制来解决。</p><ul><li><img src="/2019/09/07/faster-rcnn/Inference2.png" alt></li></ul><p>​        红色框表示非极大值抑制之前的5个框，绿色框表示非极大值抑制后的5个框。通过抑制重叠框，其他框（得分较低的框）就有机会向上移动了。</p><ul><li><img src="/2019/09/07/faster-rcnn/Inference3.png" alt></li></ul><p>​        通过最后的分类分数数组(dim:n, 21)，我们选择分数最大的作为类别。并选择该类别所对应的框回归因子来调整框，它比其他因子更加适合调整这个特定的类别。最后的检测结果如下图所示：</p><ul><li><img src="/2019/09/07/faster-rcnn/final-result.png" alt></li></ul><h1 id="9-参考资料"><a href="#9-参考资料" class="headerlink" title="9. 参考资料"></a>9. 参考资料</h1><ul><li><a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/</a></li><li><a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31426458</a></li><li><a href="https://blog.csdn.net/u011961856/article/details/77920970" target="_blank" rel="noopener">https://blog.csdn.net/u011961856/article/details/77920970</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Faster RCNN 源码理解</title>
      <link href="/2019/09/03/fast-rcnn-code/"/>
      <url>/2019/09/03/fast-rcnn-code/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Anchor-Generation-Layer"><a href="#1-Anchor-Generation-Layer" class="headerlink" title="1. Anchor Generation Layer"></a>1. Anchor Generation Layer</h1><p>对于生成anchors的源码理解主要来源于两个代码</p><ul><li>RBG大神的caffe源码：<a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/py-faster-rcnn</a></li><li>Github上复现的pytorch源码：<a href="https://github.com/chenyuntc/simple-faster-rcnn-pytorch" target="_blank" rel="noopener">https://github.com/chenyuntc/simple-faster-rcnn-pytorch</a></li></ul><p>由于两种方法生成anchors的技巧不同，故分开讨论，并主要以RBG大神的代码为主，讲解anchors的生成原理与生成技巧。</p><h2 id="1-1-Caffe源码"><a href="#1-1-Caffe源码" class="headerlink" title="1.1 Caffe源码"></a>1.1 Caffe源码</h2><ul><li><p>首先，解释一下，重要的参数</p><ul><li>base_size=16，由于原图经过卷积池化后得到的特征图是原图的$\frac{1}{16}$，故用于采样anchor的特征图上的一个cell就相当于原图的$16 \times 16$区域。</li><li>ratios=[0.5, 1, 2]，固定anchor面积下的长宽比，即$[1:2 \quad 1:1 \quad 2:1]$</li><li>scales=[8, 16, 32]，即将anchors放大的倍数，具体在哪里用到会在后面详细解释</li></ul></li><li><p>其次，我们根据RBG大神的源码走一遍anchors生成的流程</p><ul><li><pre class=" language-lang-python"><code class="language-lang-python">def generate_anchors(base_size=16, ratios=[0.5, 1, 2],                     scales=2**np.arange(3, 6)):    """    Generate anchor (reference) windows by enumerating aspect ratios X    scales wrt a reference (0, 0, 15, 15) window.    """    base_anchor = np.array([1, 1, base_size, base_size]) - 1    ratio_anchors = _ratio_enum(base_anchor, ratios)    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)                         for i in xrange(ratio_anchors.shape[0])])    return anchors</code></pre><ul><li><strong>generate_anchors()</strong>函数是一切的开端，首先定义了base_anchor，由于图像的坐标以左上角为原点且值为(0, 0)，故base_anchor的坐标(xmin, ymin, xmax, ymax)为(0, 0, 15, 15)。</li><li>其次，调用<strong>_ratio_enum()</strong>函数如下</li></ul></li><li><pre class=" language-lang-python"><code class="language-lang-python">def _ratio_enum(anchor, ratios):    """    Enumerate a set of anchors for each aspect ratio wrt an anchor.    """    w, h, x_ctr, y_ctr = _whctrs(anchor)    size = w * h    size_ratios = size / ratios    ws = np.round(np.sqrt(size_ratios))    hs = np.round(ws * ratios)    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)    return anchors</code></pre><ul><li>为了计算w, h, x_ctr, y_ctr，又调用了<strong>_whctrs()</strong>函数，如下所示</li></ul></li><li><pre class=" language-lang-python"><code class="language-lang-python">def _whctrs(anchor):    """    Return width, height, x center, and y center for an anchor (window).    """    w = anchor[2] - anchor[0] + 1    h = anchor[3] - anchor[1] + 1    x_ctr = anchor[0] + 0.5 * (w - 1)    y_ctr = anchor[1] + 0.5 * (h - 1)    return w, h, x_ctr, y_ctr</code></pre><ul><li><strong>_whctrs()</strong>函数的功能就是传入参数为（左上角x，左上角y，右上角x，右上角y），将其转换为（宽， 高， 中心坐标x，中心坐标y）</li></ul></li><li><p>让我们回到<strong>_ratio_enum()</strong>函数</p><ul><li>得到base_anchor的（宽， 高， 中心坐标x，中心坐标y），经过计算值为（16, 16, 7.5, 7.5）</li><li>size = w x h = 16 x 16 = 256</li><li>size_ratios = $\frac{256}{[0.5 \quad 1 \quad 2]}$ = $[512, 256, 128]$</li><li>对size_ratios开根号，再四舍五入，得到 ws = [23, 16, 11]</li><li>ws和ratios相乘就得到了 hs = [12, 16, 22]</li><li><strong>ws和hs其实是相同面积下，anchor不同长宽比条件下，得到的长和宽。但由于四舍五入的缘故，ws x hs的面积值不一定相等</strong></li><li>得到上面的变量值后，又调用了<strong>_mkanchors()</strong>函数返回计算后的anchors，函数如下</li></ul></li><li><pre class=" language-lang-python"><code class="language-lang-python">def _mkanchors(ws, hs, x_ctr, y_ctr):    """    Given a vector of widths (ws) and heights (hs) around a center    (x_ctr, y_ctr), output a set of anchors (windows).    """    ws = ws[:, np.newaxis]    hs = hs[:, np.newaxis]    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),                         y_ctr - 0.5 * (hs - 1),                         x_ctr + 0.5 * (ws - 1),                         y_ctr + 0.5 * (hs - 1)))    return anchors</code></pre><ul><li><p>根据上面的代码，会得到如下的计算公式</p><script type="math/tex; mode=display">7.5 - \frac{1}{2}\left[\begin{matrix} 22 \\ 15 \\ 10 \end{matrix}\right] = \left[\begin{matrix} -3.5\\ 0\\ 2.5\end{matrix}\right]</script><script type="math/tex; mode=display">7.5 - \frac{1}{2}\left[\begin{matrix} 12\\ 16\\ 22\end{matrix}\right] = \left[\begin{matrix} 1.5\\ 0\\ -3\end{matrix}\right]</script><script type="math/tex; mode=display">7.5 + \frac{1}{2}\left[\begin{matrix} 22 \\ 15 \\ 10 \end{matrix}\right] = \left[\begin{matrix} 18.5\\ 15\\ 12.5\end{matrix}\right]</script><script type="math/tex; mode=display">7.5 + \frac{1}{2}\left[\begin{matrix} 12\\ 16\\ 22\end{matrix}\right] = \left[\begin{matrix} 13\\ 15\\ 18\end{matrix}\right]</script></li><li><p>最后anchors的值为$\left[\begin{matrix} -3.5 &amp; 1.5 &amp; 18.5 &amp; 13.5\\ 0 &amp; 0 &amp; 15 &amp; 15\\ 2.5 &amp; -3 &amp; 12.5 &amp; 18\end{matrix}\right]$</p></li><li><p><strong>这里得到的是，面积都为256下，以（7.5， 7.5）为中心坐标的，不同长宽比例下的anchor坐标。根据坐标的计算公式，可以发现，都是以7.5为中心坐标减去一半的长或宽，那么得到的是新的（左上角x，左上角y，右上角x，右上角y）形式的坐标值。为什么坐标会是负数，因为左上角坐标超出了图片范围，故为负数。</strong></p></li></ul></li><li><p>得到以上anchors后，我们直接返回到<strong>generate_anchors()</strong>函数</p><ul><li>通过一系列函数的调用，我们得到了ratio_anchors的值，即$\left[\begin{matrix} -3.5 &amp; 1.5 &amp; 18.5 &amp; 13.5\\ 0 &amp; 0 &amp; 15 &amp; 15\\ 2.5 &amp; -3 &amp; 12.5 &amp; 18\end{matrix}\right]$</li><li>最后一步，就是调用<strong>_scale_enum()</strong>函数，得到不同scale下，不同长宽比例的anchors。目前的scale为[8, 16, 32]，对于每一个scale都要调用<strong>_scale_enum()</strong>函数；传入不同长宽比、以(7.5, 7.5)为中心坐标的anchors（<strong>即ratio_anchors的每一行</strong>），每次返回3组变换尺度后的anchors，故最后会有9组anchors。<strong>_scale_enum()</strong>函数如下</li></ul></li><li><pre class=" language-lang-python"><code class="language-lang-python">def _scale_enum(anchor, scales):    """    Enumerate a set of anchors for each scale wrt an anchor.    """    w, h, x_ctr, y_ctr = _whctrs(anchor)    ws = w * scales    hs = h * scales    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)    return anchors</code></pre><ul><li>我们以$[-3.5 \quad 1.5 \quad 18.5 \quad 13.5]$为例</li><li>调用<strong>_whctrs()</strong>函数，得到中心坐标表示，w, h, x_ctr, y_ctr = $[23 \quad 12 \quad 7.5\quad 7.5]$</li><li><script type="math/tex">ws = 23 \times \left[\begin{matrix} 8\\ 16\\ 32\end{matrix}\right] = \left[\begin{matrix} 184\\ 368\\ 736\end{matrix}\right]</script>，其实是宽为23的情况下，放大宽的值</li><li><script type="math/tex">hs = 12 \times \left[\begin{matrix} 8\\ 16\\ 32\end{matrix}\right] = \left[\begin{matrix} 96\\ 192\\ 384\end{matrix}\right]</script>，其实是长为12的情况下，放大长的值</li><li>由于中心坐标都是(7.5, 7.5)不变，但宽和高的值变了，所以新得到的anchors坐标需要再次调用<strong>_mkanchors()</strong>对坐标进行调整。在新的长和宽下，仍然以(7.5, 7.5)为中心坐标。</li><li>最后计算得到的anchors坐标为$\left[\begin{matrix} -83 &amp; -39 &amp; 100 &amp; 56\\ -175 &amp; -87 &amp; 192 &amp; 104\\ -359 &amp; -183 &amp; 376 &amp; 200\end{matrix}\right]$</li></ul></li></ul></li><li><p>至此，RBG大神生成Anchors的方法就介绍完毕</p></li></ul><h2 id="1-2-Pytorch源码"><a href="#1-2-Pytorch源码" class="headerlink" title="1.2 Pytorch源码"></a>1.2 Pytorch源码</h2><ul><li><p>Pytorch版本就不详细解释了，直接上代码，简单易懂</p><pre class=" language-lang-python"><code class="language-lang-python">def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2],                         anchor_scales=[8, 16, 32]):    """    Returns:        ~numpy.ndarray:        An array of shape :math:`(R, 4)`.        Each element is a set of coordinates of a bounding box.        The second axis corresponds to        :math:`(y_{min}, x_{min}, y_{max}, x_{max})` of a bounding box.    """    py = base_size / 2.    px = base_size / 2.    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4),                           dtype=np.float32)    for i in six.moves.range(len(ratios)):        for j in six.moves.range(len(anchor_scales)):            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])            index = i * len(anchor_scales) + j            anchor_base[index, 0] = py - h / 2.            anchor_base[index, 1] = px - w / 2.            anchor_base[index, 2] = py + h / 2.            anchor_base[index, 3] = px + w / 2.    return anchor_base</code></pre><ul><li>参数和caffee一致，不同点在于，计算anchor_base的方式</li><li>这里的anchor_base没有-1</li><li>调用了两个循环，即遍历9次，每次得到一个anchors的坐标</li><li>计算的公式很奇怪，为何对ratios开根号，应该是有奇怪的转换公式的</li><li>最后，是直接求anchor_base的每一个坐标，以中心坐标为基准，计算(ymin, xmin, ymax, xmax)</li></ul></li></ul><p><strong>未完待续~~~</strong></p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ROI Align理解</title>
      <link href="/2019/08/30/roi-align/"/>
      <url>/2019/08/30/roi-align/</url>
      
        <content type="html"><![CDATA[<h1 id="1-几个问题"><a href="#1-几个问题" class="headerlink" title="1. 几个问题"></a>1. 几个问题</h1><ul><li>Mask R-CNN中为何要使用ROI Align取代Faster R-CNN中的ROI Pooling</li><li>何为线性插值，何为双线性插值？插值的意义？</li><li>ROI Align的具体步骤</li></ul><h1 id="2-ROI-Pooling"><a href="#2-ROI-Pooling" class="headerlink" title="2. ROI Pooling"></a>2. ROI Pooling</h1><ul><li><p>在Faster RCNN中（不懂Faster RCNN的点击<a href="https://godwriter.github.io/2019/08/27/Faster-RCNN/#more">这里</a>），通过Achor生成的RPN Boxes经过一系列筛选后，会得到一系列ROIs用于后面的识别等工作。但是用于识别的网络需要输入固定维度的图像特征，而ROIs的大小不一致，故抽取的图像特征也不一致。为此，在Faster RCNN中，使用ROI Pooling来解决这个问题。</p><p><img src="/2019/08/30/roi-align/ROI-pooling.png" alt></p></li><li><p>首先，我们理解一下ROI Pooling的基本思路，如上图所示</p><ul><li>假设我们有一个8*9的feature map，2个ROI，且要求输出的固定维度大小为2x2</li><li>我们经过<strong>坐标的变化</strong>，得到在特征图上的投影坐标（左上角x，左上角y，宽度，高度）为(0, 3, 3, 4)，(4, 1, 4, 4)，如图中的红框</li><li>既然需要得到2x2的固定特征维度，那么我们需要对红框内的特征图做Pooling，Pooling方式也如图中所示，以及最后的结果，都一目了然。</li><li><strong>有的特征图在划分的时候不能整除，那就只能采取图中做法，类似于加了个zero-pooling</strong></li></ul></li><li><p>其次，我们来说一下ROI Pooling存在的问题。我们在上面提到了”坐标变换“这四个字，我也特地加粗了，其实问题就出在这里。</p><ul><li>假设我们使用VGG16, feat_stride=32来提取图片特征。若原图大小为800x800，那么最后输出的特征图xxxxxxxxx……</li></ul></li></ul><p><strong>未完待续~~~</strong></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li><a href="https://blog.csdn.net/u011436429/article/details/80279536" target="_blank" rel="noopener">https://blog.csdn.net/u011436429/article/details/80279536</a></li><li><a href="https://www.cnblogs.com/wangyong/p/8523814.html" target="_blank" rel="noopener">https://www.cnblogs.com/wangyong/p/8523814.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo下Next主题的优化1</title>
      <link href="/2019/08/28/hexo-next-1/"/>
      <url>/2019/08/28/hexo-next-1/</url>
      
        <content type="html"><![CDATA[<h1 id="1-版本概览"><a href="#1-版本概览" class="headerlink" title="1. 版本概览"></a>1. 版本概览</h1><ul><li>Node.js版本：v10.16.3</li><li>Hexo版本：v3.9.0</li><li>Next版本：v5.1.4</li></ul><h1 id="2-首页不显示全文"><a href="#2-首页不显示全文" class="headerlink" title="2. 首页不显示全文"></a>2. 首页不显示全文</h1><ul><li><p>希望达到效果：首页显示文章列表，列表里的每一篇文章只显示预览，不显示全文。效果如下图</p><p><img src="/2019/08/28/hexo-next-1/notlong.jpg" alt></p></li><li><p>解决方法如下</p><ul><li><p>进入themes/next目录</p></li><li><p>打开_config.yml文件</p></li><li><p>搜索“auto_excerpt”，找到如下部分</p><pre class=" language-lang-yaml"><code class="language-lang-yaml">auto_excerpt:  enable: true  length: 250</code></pre></li><li><p>把enable改为true，length设置你想显示的长度</p></li></ul></li><li><p>问题即可迎刃而解</p></li></ul><h1 id="3-首页添加头像"><a href="#3-首页添加头像" class="headerlink" title="3. 首页添加头像"></a>3. 首页添加头像</h1><ul><li><p>希望达到效果：首页能够有自己的头像显示，效果如下图</p><p><img src="/2019/08/28/hexo-next-1/touxiang.jpg" alt></p></li><li><p>解决步骤如下</p><ul><li><p>进入themes/next目录</p></li><li><p>打开_config.yml文件</p></li><li><p>搜索”avatar”，修改成如下</p><pre class=" language-lang-yaml"><code class="language-lang-yaml">avatar: /images/avatar.gif</code></pre></li><li><p>将你的图像放到\themes\next\source\images文件夹下，命名为：avatar.gif，若已经有这个文件直接替换即可。</p></li></ul></li><li><p>问题即可迎刃而解</p></li></ul><h1 id="4-开启版权声明"><a href="#4-开启版权声明" class="headerlink" title="4. 开启版权声明"></a>4. 开启版权声明</h1><ul><li><p>希望达到效果：每篇文章末尾会有版权声明，效果如下图</p><p><img src="/2019/08/28/hexo-next-1/banquan.jpg" alt></p></li><li><p>解决步骤如下</p><ul><li><p>进入themes/next目录</p></li><li><p>打开_config.yml文件</p></li><li><p>搜索“post_copyright”，修改成如下</p><pre class=" language-lang-yaml"><code class="language-lang-yaml">post_copyright:  enable: true  license: CC BY-NC-SA 3.0  license_url: https://creativecommons.org/licenses/by-nc-sa/3.0/</code></pre></li></ul></li><li><p>问题即可迎刃而解</p></li></ul><h1 id="5-添加github标志"><a href="#5-添加github标志" class="headerlink" title="5. 添加github标志"></a>5. 添加github标志</h1><ul><li><p>希望达到效果：自己博客的右上角有一个github的小标志，点击可以直达主页，效果如下图</p><p><img src="/2019/08/28/hexo-next-1/icon.jpg" alt></p></li><li><p>实现步骤如下</p><ul><li><p>点击<a href="http://tholman.com/github-corners/" target="_blank" rel="noopener">这里</a>挑选自己喜欢的样式，并复制代码。 例如，我是复制的这一个：</p><p><img src="/2019/08/28/hexo-next-1/iconwebsite.jpg" alt></p></li><li><p>进入themes/next/layout/_layout.swig文件中，并搜索如下代码</p><pre class=" language-lang-html"><code class="language-lang-html"><div class="headband"></div></code></pre></li><li><p>将复制的代码放到该代码行下，并把href改为自己的github地址</p></li></ul></li><li><p>问题即可迎刃而解</p></li></ul><h1 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a>6. 参考资料</h1><ul><li><p><a href="https://www.jianshu.com/p/393d067dba8d" target="_blank" rel="noopener">https://www.jianshu.com/p/393d067dba8d</a></p></li><li><p><a href="https://jingyan.baidu.com/article/d5a880ebeb42f113f147cce5.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/d5a880ebeb42f113f147cce5.html</a></p></li><li><p><a href="https://blog.csdn.net/weixin_43971764/article/details/96478950" target="_blank" rel="noopener">https://blog.csdn.net/weixin_43971764/article/details/96478950</a></p></li><li><p><a href="https://blog.csdn.net/fly_wt/article/details/86674138" target="_blank" rel="noopener">https://blog.csdn.net/fly_wt/article/details/86674138</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> next </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
