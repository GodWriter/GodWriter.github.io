<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深入BBN，如何解决长尾数据分布的同时兼顾表示学习</title>
      <link href="/2020/06/13/bbn/"/>
      <url>/2020/06/13/bbn/</url>
      
        <content type="html"><![CDATA[<h1 id="1-问题引入"><a href="#1-问题引入" class="headerlink" title="1. 问题引入"></a>1. 问题引入</h1><ul><li>本次要记录的论文是，CVPR2020 的 <strong>“ BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition “</strong>。该文旨在解决长尾数据分布的同时兼顾表示学习。</li><li>长尾数据是视觉认知任务如：图像分类、目标检测中影响实验结果的主要问题之一。长尾数据分布的意思是：数据集中某几个类别占据了大部分的数据，而剩余的类别各自的数据很少。</li><li>举个例子，想用一个1000张图像的数据集训练一个分类模型，数据集包含三个类别：人、狗、猫。其中，人900张，狗80张，猫20张。那么狗、猫就属于长尾数据了。对于深度学习这种见多识广的技术来说，见得少了的东西识别起来当然困难，最后训练出来的分类模型几乎看到什么都判别为人，错误率特别高。</li></ul><h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h1><p>当然，长尾数据分布的问题并不是第一次提出来，也有很多方法对其进行了缓解，这些方法统称为Re-balancing Strategies，具体如下：</p><ul><li>Class Re-balancing Strategies。这里面又分为两种方法，Over-Sampling和Under-sampling。<ul><li>Over-Sampling，在训练的过程中多次采样数据集中数据量占比量较小的数据，使得这些数据在训练时被多次用到，从而缓解长尾数据分布的问题。</li><li>Under-Sampling，在训练中抛弃数据量占比较高的数据，从而达到各个类别数据量的平衡，以缓解长尾数据分布的问题。</li></ul></li><li>Re-weighting Strategies。在训练模型的过程中，增加损失中长尾数据的权重，有点类似于Boost的方法。但该方法无法处理实际生活中的数据，一旦长尾数据分布很严重，该方法还容易引起优化的问题。</li><li>Two-stage Fine-tuning Strategies。这个方法将训练分为两个阶段。第一阶段像往常一样正常训练，第二个阶段使用较小的学习率以Re-balancing的方式微调网络。</li><li>Mix-up。这是数据增强领域的一种方法，通过在融合多张图像到同一个图像中增广数据集用于训练；还有的方法在图像的流形特征上进行增广。</li></ul><h1 id="3-进一步探索"><a href="#3-进一步探索" class="headerlink" title="3. 进一步探索"></a>3. 进一步探索</h1><p>那么，为什么Re-balancing Strategies的方法能够处理长尾数据分布的问题呢？作者做了实验，作者假设：</p><ul><li>基于深度学习的分类模型分为2部分，特征抽取器(feature extractor) 和 分类层(the classifier)。</li><li>那么，分类模型学的过程也可以分为2部分，表示学习(Representation Learning) 和 分类学习(Classifier Learning)。</li></ul><p>作者认为：Class re-balancing通过改变原始训练集的分布，使之接近测试集的分布，从而使得分类模型将更多的关注放在长尾数据身上，提升了分类的准确性。但作者又认为由于Class re-balancing改变了原始训练集的分布，导致Representation Learning的过程受到破坏，故对图像中原始特征的提取也会收到破坏。为此，作者做了如下实验，如下图所示：</p><p><img src="/2020/06/13/bbn/rb-exp.jpg" alt></p><ul><li>作者将训练分为2个阶段，former stage和latter stage，两个阶段分别采取3种训练方式，一共训练出9个模型。上图中左右两个矩阵是在两个数据集上训练出来模型的精度，我们以左边的矩阵为例解释。</li><li>横坐标代表Representation Learning采用的学习策略，纵坐标代表Classifier Learning采用的学习策略。CE代表传统的交叉熵方法训练，RW即上述的Re-weighting Strategies，而RS代表上述的Re-sampling方法。</li><li>当垂直的看矩阵的每一列时，表明该列所有分类模型先用(CE, RW, RS)三个方法之一正常训练出一个模型，模型训练完毕之后，固定feature extractor参数不变，重头开始训练the Classifier。通过这个方法固定Representation Learning，可以判别出哪种方法训练出来的分类器具有较好的效果。根据结果，可以看出RS，RW的方法明显好于CE。</li><li>当水平看矩阵的每一行时，表明该行所有分类模型先用(CE, RW, RS)三个方法之一正常训练出一个模型，模型训练完毕之后，固定the Classifier参数不变，重头开始训练feature extractor。通过这个方法固定the Classifier，可以判别出哪种方法训练出来的特征抽取器具有较好的效果。根据结果，可以看出CE的方法具有最低的错误率。</li></ul><p>最后，作者就可以得到结论：传统的训练方式有助于Feature Extractor的学习，而RW、RS有助于分类器的学习。很自然，是否可以结合两者的优点呢？上述的Two-stage方法就是干这个事儿的，但是需要2个阶段，是否可以设计一个One-stage的端到端网络实现这一点呢？</p><h1 id="4-BBN"><a href="#4-BBN" class="headerlink" title="4. BBN"></a>4. BBN</h1><p>作者提出的模型称之为BBN，模型由三个部分组成：Conventional Learning Branch，Re-Balancing Branch和Cumulative Learning，具体结构如下图：</p><p><img src="/2020/06/13/bbn/model-exp.jpg" alt></p><ul><li><p>Conventional Learning Branch，在这个分支中，每个训练epoch的数据都是等比例的从原始训练集中采样，从而保持原始数据的分布，有利于模型的Representation Learning。</p></li><li><p>Re-balancing Branch，这个分支旨在缓解长尾数据分布并提升分类的准确性，这个分支的数据通过Reversed Sampler获取，后面会详细解释。通过这个Reversed Sampler，数据集中类别样本数量越多的，被采样的几率越小。</p></li><li><p>上述两个分支的模型结构一摸一样，且共享了除最后一个残差块的所有参数。为什么要共享参数了？作者给出了两个原因：</p><ul><li>有利于Representation Learning的学习，但并没有相关的消融实验。而且一旦这个模型没有参数共享，那么它其实就是散装了两个网络而已，不知道这里的共享是为了有效而共享，还是为了共享而共享。当然这并不能说明该模型就不好了，因为模型的两点应该在于Cumulative Learning模块。</li><li>降低运算资源，这个很直接，作者在文中表示所有的模型都在一块1080-ti上完成，我表示很理解。毕竟，作为同行，我也缺运算资源。</li></ul></li><li><p>在讲Cumulative Learning模块之前，我们先具体解释一下Reversed Sampler的构建。</p><ul><li><p>定义$N<em>i$为类别$i$所包含的样本个数，$N</em>{max}$为所有类别中包含最多样本类别的样本个数</p></li><li><p>构建Reversed Sampler有3个子过程</p><ul><li><p>首先，根据样本个数计算类别$i$的采样概率$P_i$</p><script type="math/tex; mode=display">P_i = \frac{w_i}{\sum_{j=1}^cw_j}$$，其中$$w_i = \frac{N_{max}}{N_i}</script><p>通过计算$w_i$可以发现，类别包含样本个数越小，$w_i$值越大。为什么要计算$P_i$呢？那肯定是为了归一化啦，这样所有样本采样的概率和为1。</p></li><li><p>其次，根据$P_i$随机选择一个类别</p></li><li><p>最后，均匀的从被选择到的类别采集图像数据获得mini-batch</p></li></ul></li></ul></li><li><p>最后，我们来深入理解一下Cumulative Learning模块。其主要目的是为了：通过权衡两个分支提取到的特征的权重来控制分类损失，从而控制模型在学习中的关注点从Representation Learning逐渐转移到长尾数据分布问题上。</p><ul><li><p>在模型的前面两个分支中，分别可得到Conventional Learning Branch的特征向量$f_c$，以及Re-balancing Branch的特征向量$f_r$。那么如何去合理的融合这两个特征呢？</p></li><li><p>定义$\alpha$为平衡因子，$W_c$为Conventional Learning Branch的分类器，$W_r$为Re-balancing Branch的分类器，那么两个分支的特征可融合为：</p><script type="math/tex; mode=display">z = \alpha W_c^Tf_c + (1 - \alpha)W_r^Tf_r</script></li><li><p>那么，这个$\alpha$如何计算呢？定义整个总训练epoch个数为$T_{max}$，当前训练的epoch为$T$：</p><script type="math/tex; mode=display">\alpha = 1 - (\frac{T}{T_{max}})^2</script><p>通过式子可以看出，随着$T$逐渐增大，$\alpha$逐渐减少。再结合上面$z$的计算，可以发现总的融合特征$z$首先将注意力放在$f_c$特征，强调了Representation Learning的重要性；随着epoch的提升，注意力逐渐转移到$f_r$上，模型开始处理长尾数据分布的问题，从而做到Representation和长尾数据分布两者兼顾。</p></li><li><p>在得到了$z$之后，和平常的方法类似，需要先做一个$softmax$多分类的平滑：</p><script type="math/tex; mode=display">\hat{p_i} = \frac{e^{z_i}}{\sum_{j=1}^Ce^{z_j}}</script><p>再通过交叉熵结合两个特征向量各自的损失，即两个特征向量有各自的Ground Truth标签，所以损失也分别做并相加</p><script type="math/tex; mode=display">L = \alpha E(\hat{p}, y_c) + (1 - \alpha) E(\hat{p}, y_r)</script></li></ul></li></ul><p>至此，本文所提出的模型讲解结束，总结一下模型的用意：</p><ul><li>Conventional Learning Branch正常采样，保证Representation Learning</li><li>Re-balancing Branch配备创新点Reverse Sampler，针对长尾数据分布问题</li><li>此外，还设计了$\alpha$参数，来自动平衡两个分支的输出特征，旨在先保证Representation Learning，再逐渐将注意力放到长尾数据分布的身上</li></ul><h1 id="5-相关实验"><a href="#5-相关实验" class="headerlink" title="5. 相关实验"></a>5. 相关实验</h1><blockquote><p>实验配置</p></blockquote><ul><li>作者上来先定义了长尾数据分布的严重程度指标$\beta = \frac{N<em>{max}}{N</em>{min}}$，其中$N<em>{max}$指的是所有类别中最大的数据量，而$N</em>{min}$指的是所有类别中最小的数据量，可想而知$\beta &gt;= 1$。文中采用$\beta = 10, 50, 100$，增广CIFAR数据集为长尾数据分布的数据集。</li><li>本文主要对比了以下几类方法<ul><li>Baseline，focal loss</li><li>Two-stage的方法，CE-DRW, CE-DRS</li><li>state-of-art的在不平衡数据集上取得高分的方法，如LDAM, CB-Focal</li></ul></li><li>在测试的时候，$\alpha = 0.5$，因为2种特征都很重要。</li></ul><blockquote><p>实验一</p></blockquote><p><img src="/2020/06/13/bbn/exp1.jpg" alt></p><ul><li>上述主要三行方法分别为：MixUp、Two-Stage、State-of-art</li><li>可以看出，本文提出的方法均取得了最佳；但还有一个现象，Two-stage也去了较好的方法。这也很容易理解，因为本文的方法是Two-stage的进一步提升。</li></ul><blockquote><p>实验二</p></blockquote><p><img src="/2020/06/13/bbn/exp3.jpg" alt></p><ul><li>该实验为了证明Reversed Sample的有效性。其中Uniform sample就是传统的采样方式，根据原始数据分布的比例采样；Balanced Sampler是等概率采样，类似于Re-balanced方法。</li><li>可以看出，本文提出的方法最好，Balanced Sampler的方法比Uniform sampler好也能体现一个隐藏信息：深度学习网络在大量的数据下，对于频率出现较高的图像识别能力早已饱和。所以即使是等概率采样，减少了高频图像出现的概率，又一定程度破坏了Representing Learning，效果都比传统训练方法的好。</li></ul><blockquote><p>实验三</p></blockquote><p><img src="/2020/06/13/bbn/exp4.jpg" alt></p><ul><li>该实验为了证明当前$\alpha$策略的有效性。为此，采用了其他两类$\alpha$选择策略<ul><li>Progress-relevant strategies，即和在训练过程中修改$\alpha$的方法，如：Linear Decay, Cosine Decay</li><li>Progress-irrelevant strategies，即和训练过程无关的方法，如：Equal weight, $\beta$-distribution中采样</li></ul></li><li>首先，可以发现在训练过程相关的动态修改方法明显优于不相关的。</li><li>其次，Parabolic increment方法是效果最差的，这也证明了作者的观点：应该先将关注点放在Representation Learning，再逐渐迁移到原始数据的分布。而Parabolic increment是反着来，所以效果是最差的。</li></ul><blockquote><p>实验四</p></blockquote><p><img src="/2020/06/13/bbn/exp5.jpg" alt></p><ul><li>该实验主要为了证明BBN有着接近原始方法训练的模型的Representation Learning的能力。实验方法和之前9宫格矩阵的方法类似，首先正常训练网络，再固定backbone不变，重头训练分类器。可以看见BBN的两个分支都有着中肯的错误率，至少比RW、RS要好。</li><li>此外，这也说明了共享策略的有效性。</li></ul><blockquote><p>实验五</p></blockquote><p><img src="/2020/06/13/bbn/exp6.jpg" alt></p><ul><li>该实验是为了证明BBN训练出来的分类器不存在偏爱性，对所有的类别一视同仁从而解决了长尾数据分布的问题。横坐标是不同的类别，纵坐标是$l2-norm$可以反映分类器对某个类别的偏爱程度。</li><li>可以看到，在所有的折线中，BBN是最为平坦的。虽然RW、RS也都较为平坦，但是作者还计算了不同方法不同类别$l2-norm$的标准差，标准差越小的差别越小，而BBN是最小的。</li></ul><p>写在后面：一直以来都认为自己读论文的姿势不对，故我开始在博客中记录中我对论文的理解。但又一直感觉记录论文博客的写法也不对，思考良久，我觉得如何去理解论文、如何去记录对论文的理解，也应该符合一篇论文创作的顺序，故我一直在调整撰写的方式。祝愿有一天，我能做到更加轻松的阅读论文，并把自己对论文的理解做到如数家珍的侃侃而谈。如果大家有很好的论文阅读姿势，欢迎在评论区留言分享~~~</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning 2</title>
      <link href="/2020/05/27/reinforcement-learning2/"/>
      <url>/2020/05/27/reinforcement-learning2/</url>
      
        <content type="html"><![CDATA[<h1 id="1-两种Policy"><a href="#1-两种Policy" class="headerlink" title="1. 两种Policy"></a>1. 两种Policy</h1><p>强化学习有两种训练方法：</p><ul><li>On-policy，要学的Agent边学边玩</li><li>Off-policy，要学的Agent看别人玩</li></ul><p>前文讲解的Policy Gradient其实是On-policy的做法，这理解起来很直观：</p><script type="math/tex; mode=display">\nabla\bar{R}_\theta = E_{\tau\thicksim P_\theta(\tau)}[R(\tau)\nabla log(P_\theta(\tau))]</script><ul><li>使用$\pi_\theta$来收集数据。当$\theta$被更新之后，必须重新采样数据。故Policy Gradient需要大量的时间去训练数据</li></ul><p>而Off-Policy得目标是：使用$\pi<em>{\theta^{‘}}$收集得样本来训练$\theta$，由于$\pi</em>{\theta^{‘}}$是固定的，故可以反复使用它采集数据。</p><h1 id="2-Importance-Sampling"><a href="#2-Importance-Sampling" class="headerlink" title="2. Importance Sampling"></a>2. Importance Sampling</h1><ul><li><p>在Policy Gradient中，玩一轮游戏可能得全局期望为：</p><script type="math/tex; mode=display">E_{x\thicksim p}[f(x)] \approx \frac{1}{N}\sum_{i=1}^Nf(x^i)</script><ul><li>$x^i$从$p(x)$中采样，取平均值近似期望。</li></ul></li><li><p>这里的$p(x)$即为某一个$\tau$出现的概率，当充分采样时，其概率近似$\frac{1}{N}$。</p></li><li><p>那么假设不从$p(x)$中采集数据，而是从$q(x)$中采集数据且$q(x)$可以是任意得形式，那么可否用从$q(x)$中采集的数据训练$p(x)$下的模型，从而避免多次和环境交互的过程呢？理论上是可行的，涉及到的方法就是重要性采样：</p><script type="math/tex; mode=display">E_{x \thicksim p}[f(x)] \\ = \int f(x)p(x)dx \\ = \int f(x)\frac{p(x)}{q(x)}q(x)dx \\ = E_{x \thicksim q}[f(x)\frac{p(x)}{q(x)}]</script><ul><li>通过巧妙地转换，使得从从$q(x)$中采集的样本获得总期望和$p(x)$下的相等，只是$f(x)$多了一个权重$\frac{p(x)}{q(x)}$。</li><li>所以即使不能从$p(x)$中采样数据，也可以通过从$q(x)$中采样来获取期望</li></ul></li><li><p>理论上，$q(x)$可以采用任何形式的分布，但实际上$q(x)$不能和$p(x)$相差太多，否则还是会出现问题，我们可以通过比较两个期望的方差来得到这一结论</p><ul><li><p>我们通过重要性得到如下等式：</p><script type="math/tex; mode=display">E_{x \thicksim p}[f(x)] = E_{x \thicksim q}[f(x)\frac{p(x)}{q(x)}]</script></li><li><p>那么，我们现在计算它们的方差，方差的计算公式为：$Var(x) = E(x^2) - [E(x)]^2$</p><ul><li><script type="math/tex; mode=display">Var_{x \thicksim p}[f(x)] = E_{x \thicksim p}[f(x)^2] - (E_{x \thicksim p}[f(x)])^2</script></li><li><script type="math/tex; mode=display">Var_{x \thicksim q}[f(x)\frac{p(x)}{q(x)}] = E_{x \thicksim q}[(f(x)\frac{p(x)}{q(x)})^2] - E_{x \thicksim q}[f(x)\frac{p(x)}{q(x)}]^2 = E_{x \thicksim p}[f(x)^2\frac{p(x)}{q(x)}] - (E_{x \thicksim p}[f(x)])^2</script></li></ul></li><li><p>通过对比可以发现，上面两个公式的第二项是相同的，所以最影响方差的出现在第一项的。当$p(x)$和$q(x)$的差距很大时，会导致第一项中的权重系数$\frac{p(x)}{q(x)}$变化剧烈，这就导致方差严重不同。</p></li><li><p>所以，当从$p(x)$和$q(x)$采样次数足够多时，那么$E<em>{x \thicksim p}[f(x)] = E</em>{x \thicksim q}[f(x)\frac{p(x)}{q(x)}]$还是成立的；但当采样次数不够多的时候，由于从$p(x)$采样和从$q(x)$采样方差相差过大，那么等式相等难以成立。</p></li></ul></li><li><p>让我们举一个通俗易懂的例子来解释这件事，如下图</p><p><img src="/2020/05/27/reinforcement-learning2/issue.jpg" alt></p><ul><li>图中，黑线横轴上方$f(x)$值为正，下方为负。$p(x)$和$q(x)$都是长尾分布，所以直观上它们本身的方差就很大</li><li>当从$p(x)$中采样时，高概率采集的样本$f(x)$都是在黑线横轴下方的，故期望值应该为负数；而从$q(x)$中采样时，高概率采集的样本$f(x)$值为正，故期望值应该为正数。</li><li>这就说明如果$p(x)$和$q(x)$相差过大，那么重要性采样在采样不充分的情况下会导致$E<em>{x \thicksim p}[f(x)] = E</em>{x \thicksim q}[f(x)\frac{p(x)}{q(x)}]$不成立。</li></ul></li></ul><h1 id="3-On-Policy-to-Off-Policy"><a href="#3-On-Policy-to-Off-Policy" class="headerlink" title="3. On Policy to Off-Policy"></a>3. On Policy to Off-Policy</h1><p>在On policy的形式下，原始的策略梯度形式为：</p><script type="math/tex; mode=display">\nabla\bar{R}_\theta = E_{\tau\thicksim P_\theta(\tau)}[R(\tau)\nabla log(P_\theta(\tau))]</script><p>那么在加入了重要性采样的方法后，该策略梯度就变成了Off policy的形式，如下：</p><script type="math/tex; mode=display">\nabla\bar{R}_\theta = E_{\tau\thicksim P_{\theta^{'}}(\tau)}[\frac{P_\theta(\tau)}{P_{\theta^{'}}(\tau)}R(\tau)\nabla log(P_\theta(\tau))]</script><p>这里让$\theta^{‘}$作为Actor与环境做互动，给$\theta$做榜样，从而帮助$\theta$采集大量的训练数据。在Off Policy下，当前训练的Actor就无需时刻与环境做互动从而获得训练数据，可通过$\theta^{‘}$所代表的Actor与环境做互动拿到的数据更新$\theta$即可。</p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Statistic Learning 6</title>
      <link href="/2020/05/15/statistic-learning6/"/>
      <url>/2020/05/15/statistic-learning6/</url>
      
        <content type="html"><![CDATA[<h1 id="Linear-Model-Selection-and-Regularization"><a href="#Linear-Model-Selection-and-Regularization" class="headerlink" title="Linear Model Selection and Regularization"></a>Linear Model Selection and Regularization</h1><p>在回归方法中，标准的线性模型如下：</p><script type="math/tex; mode=display">Y = \beta_0 + \beta_1x_1 + ... + \beta_px_p + \epsilon</script><p>用来描述Response Y和一系列变量$x_1, x_2, … ,x_p$之间的关系。在前文中，我们使用最小二乘来拟合模型，的确线性模型在推理方面有自己的明确方式，在实际应用中也和非线性方法有得一拼。所以，在进入非线性世界之前，我们先讨论一些线性模型改进的方法，使用可选的方法来替代普通最小二乘。为什么要用其他的拟合方式取代最小二乘？正如下文大家将看到的，替代的方案可以产生更好的预测精度和模型的可解释性。</p><blockquote><p>Prediction Accuracy</p></blockquote><p>假设Response和Predictors之间的关系的确是线性的，最小二乘估计的bias（偏差）将会较低。</p><ul><li>当$n &gt;&gt; p$，即样本数量远大于变量个数的时候，最小二乘估计趋向于有较低的方法。因此，在测试样本上会表现较好。</li><li>当$n$仅小幅度大于$p$时，那在最小二乘拟合的过程中将会出现很多变化（相对于大样本来说，小样本变化更多），这会导致过拟合从而使得在测试集上预测效果很差。</li><li>当$n &lt; p$时，就不存在唯一的最小二乘系数估计（方程个数比未知数还要少，那么未知数的结果应该有多个解，类似于线代中的奇异矩阵？）因此，最小二乘不适合。</li></ul><p>针对$n$仅小幅度大于$p$时，可以通过约束方法；针对$n &lt; p$，而可通过缩小估计稀疏的方法；以上两个方法通常可以在偏差几乎不增加的情况下大幅度减少方差。这可以让模型在测试集上的准确性得到显著的提高。</p><blockquote><p>Model interpretability</p></blockquote><ul><li><p>通常来说，大部分回归模型中的变量实际上和Response并无太大关联。若包含这些变量，还会增加模型不必要的复杂度。那么，通过消除这些变量，或使得相应的系数估计值为0，我们就可以得到一个更容易解释的模型。但最小二乘很难产生值为0的系数估计，所以在下文中，我们会见到一些Feature Selection（特征选择）和Variable Selection（变量选择）的方法，从而在多元回归的模型中排除不相关的变量。</p></li><li><p>除最小二乘外，还有很多其他的选择，包括经典的和先进的方法，后文将主要讨论三种重要方法</p><ul><li><p>Subset Selection</p><p>该方法首先确定了$p$个Predictors中，与Response最相关的Predictor所构成的子集。我们再用该子集去应用最小二乘法</p></li><li><p>Shrinkage（特征缩减技术）</p><p>该方法使用所有$p$个Predictors来拟合一个模型。然而相对于最小二乘估计，估计系数可缩小到接近于零。该Shrinkage（Regularization）可以达到减少方差的效果，根据不同的Shrinkage方法，有些系数可能恰好为零。因此，Shrinkage方法可以进行变量选择（Variable Selection）。</p></li><li><p>Dimension Reduction</p><p>该方法先将$p$个Predictors投影到M维的子空间上（M &lt; P），再通过计算M个不同的线性组合投影来实现。然后使用该M作为Predictors，用最小二乘法拟合线性回归模型。</p></li></ul></li></ul><p>在下文中，我们将更详细的描述这些方法，以及它们的优缺点。虽然所举的例子仅用于回归，但同样的概念也会适用于分类。</p><h1 id="Subset-Selection"><a href="#Subset-Selection" class="headerlink" title="Subset Selection"></a>Subset Selection</h1><blockquote><p>Best Subset Selection</p></blockquote><ul><li><p>为了使用Best Subset Selection，我们需要对$p$个Predictors所有可能的组合，拟合最小二乘回归。比如说：</p><ul><li>p个只包含一个Predictor的模型</li><li>所有$\left(\begin{matrix}p \\ 2\end{matrix}\right) = p(p-1)/2$个包含2个Predictors的模型</li><li>依此类推…</li></ul><p>最后从上述训练好的模型中挑选出效果最好的那一个</p></li><li><p>但从通过Best Subset Selection得到的$2^p$个模型中，挑选出最佳模型并不简单，这通常分为3个阶段</p><ul><li>定义$M_0$为空模型（Null Model），$M_0$包含0个Predictors。这个模型仅能简单预测每个观测值的样本均值。</li><li>对于$k = 1,2,…,p$<ul><li>拟合所有$\left(\begin{matrix}p \\ k\end{matrix}\right)$个包含k个Predictors的模型</li><li>从这$\left(\begin{matrix}p \\ k\end{matrix}\right)$个模型中选择最好的那个，称之为$M_k$。这里最好的定义是有着最小的RSS值，或者最大的$R^2$。</li></ul></li><li>使用Cross-validate Prediction Error，$C_p$，AIC，BIC，或者adjusted $R^2$从$M_0, M_1, …, M_p$中选择最好的那个模型。</li></ul></li><li><p>在上述过程中，步骤2定义了训练集上每个子集中最好的那个模型，把最优模型的可选个数从$2^p$降低为$(p+1)$个。所以问题转化为从剩下的$(p+1)$个模型中选择最优模型。然而问题在于，随着Predictors个数的增加，RSS指标逐渐下降，同时$R^2$指标逐渐上升，这会导致选择最优模型永远是Predictors最多的那一个，不符合我们的目的：排除对Response结果无关的Predictors。</p></li><li><p>故问题得根源在于评价指标：RSS越低，$R^2$越高，都表明模型有着低Train Error，但我们希望选择测试误差较低的模型。（根据前系列文章，Train Error往往低于Test Error，低Train Error并不能保证低Test Error）。所以在步骤3中，需要使用Cross-validate Prediction Error，$C_p$，AIC，BIC，或者adjusted $R^2$指标选择最优模型。</p></li><li><p>同样的方法也适用于其他的统计方法，如Logistic Regression。在Logistic Regression的情况下，上述算法的步骤2将不会使用RSS对模型排序，而是使用偏差，一种类似于RSS但作用范围更广的度量方法。偏差是最大对数似然的负两倍；偏差越小，模型越好。</p></li><li><p>虽然Best Subset Selection很简单且有效果，但对算力要求过高。一般来说，p个Predictors有$2^p$种可选模型；当p=10，$2^p = 1024$；当p=20，这简直是一场灾难；当p&gt;40时，再好的计算机也很难保证方法的可行性。为了排除某些选择，有一些称之为“分支界限技术”的方法可排除某些选项。但随着p的增大，这些技术也会出现局限性。故需要有更高效的子集选择方法。</p></li></ul><blockquote><p>Forward Stepwise Selection</p></blockquote><p>相比于Best Subset Selection, Forward Stepwise Selection的计算效率很高，其考虑的模型个数远小于$2^p$个。</p><ul><li>Forward Stepwise Selection从不包括Predictors的空模型开始，然后逐渐的往模型中加入Predictors，一次加一个，直到所有的Predictors都在模型中。具体来说，每一步中，提升模型拟合能力最大的Predictors被置入模型中，具体算法如下：<ul><li>定义$M_0$为NULL Model，不包含任意一个Predictor</li><li>对于$k=0, 1, …, p-1$<ul><li>考虑所有$(p-k)$个往$M_k$中增加一个额外的Predictor的模型</li><li>从$(p-k)$各模型中选择最好的那个，称之为$M_{k+1}$。同样，最好模型的定义为有着最小的RSS或者最高的$R^2$</li></ul></li><li>从$M_0,M_1,…,M_p$中采用Cross-validate Prediction Error，$C_p$，AIC，BIC，或者adjusted $R^2$，挑选出最优的那个模型。</li></ul></li><li>相比于Best Subset Selection需要拟合$2^p$个模型，Forward Stepwise Selection仅需要拟合一个Null Model，以及第$k$轮中的$(p-k)$个模型。所以Forward Stepwise Selection需要拟合的模型个数为$1 + \sum_{k=0}^{p-1}(p-k) = 1 + p(p+1)/2$。这是个实质性的差别，当$p=20$的时候，Best Subset Selection需要拟合1048576个模型，而Forward Stepwise Selection仅需要拟合211个模型。</li><li>Forward Stepwise Selection相较于Best Subset Selection的计算优势很明显，但其选择的模型并非是全局最优。举个例子，一个有3个Predictors的数据集$p=3$，当$p=1$时，best model为$X_1$；当p=2时，best model为$X_2, X_3$。对于Best Subset Selection，在$p=1,p=2$时都能挑选出最优的模型。但对于Forward Stepwise Selection，当$p=1$时，best model为$X_1$；当$p=2$时，它只能从$X_2,X_3$中挑选出一个最优的加进去，从而错过了Best Model $X_2, X_3$。</li></ul><blockquote><p>Backｗard Stepwise Selection</p></blockquote><ul><li>和Forward Stepwise Selection类似，Backward Stepwise Selection相较于Best Subset Selection提供了更高效的计算。与Forward Stepwise Selection不同的是，它从包含了所有p个Predictors的模型开始，并逐渐对模型帮助最小的Predictor。</li><li>Backward Stepwise Selection的具体算法如下：<ul><li>定义$M_p$为Full Model，即包含所有p个predictors</li><li>For $k=p, p-1, …, 1$：<ul><li>考虑所有的从$M_k$中去除一个Predictor从而剩下$(k-1)$个predictors的模型</li><li>从这$k$个模型中，选择最优的模型称之为$M_{k-1}$。同样，最好模型的定义为有着最小的RSS或者最高的$R^2$</li></ul></li><li>从$M_0,M_1,…,M_p$中采用Cross-validate Prediction Error，$C_p$，AIC，BIC，或者adjusted $R^2$，挑选出最优的那个模型。</li></ul></li><li>和Forward Stepwise Selection类似，Backward Stepwise Selection只需要搜索$1 + p(p+1)/2$个模型，所以可以应用于$p$过大从而难以采用Best Subset Selection的方法。同理，Backward Stepwise Selection也无法保证获取最优模型。</li></ul><blockquote><p>Hybrid Approaches（混合方法）</p></blockquote><p>Best Subset Selection，Forward Stepwise Selection，Backward Stepwise Selection这三个方法往往得到相似但不完全相同的模型。作为可选项，Forward和Backward是可以混合在一起使用的。</p><ul><li>首先，Predictor根据顺序加入模型中，类似于Forward</li><li>再去除对模型帮助不大的变量</li></ul><p>这种方法近似于做Best Subset Selection，但又保证了Forward和Backward的计算优势。</p><h1 id="Choosing-the-Optimal-Model"><a href="#Choosing-the-Optimal-Model" class="headerlink" title="Choosing the Optimal Model"></a>Choosing the Optimal Model</h1><p>Best Subset Selection，Forward Stepwise Selection，Backward Stepwise Selection都会得到一系列包含p个Predictors的模型，我们需要判断哪个模型是最好的。</p><ul><li>前文提到了，一个包含所有p个Predictors的模型将会有最小的RSS和最大的$R^2$，这就导致包含了所有Predictors的模型会被选中，不符合筛选无用Predictors的目的。且RSS，$R^2$都和Training Error相关，往往Training Error难以估计Test Error，所以$R^2$，RSS并不适合作为挑选最佳模型的指标</li><li>为了选择和测试误差相关的最优模型，我们得估计Test Error，总体来说有两种常见的方式：<ul><li>通过调整Training Error解释过拟合带来的偏差，从而间接地估计Test Error。</li><li>使用Validation Set Approach，Cross-Validation Approach来直接估计Test Error</li></ul></li></ul><blockquote><p>$C_p$, AIC, BIC, 以及 Adjusted $R^2$的引入</p></blockquote><ul><li>在前文中，我们提到Training Set得MSE往往低估了Test MSE（$MSE = \frac{RSS}{n}$）。因为当拟合一个模型得时候，我们使用的是训练集，让模型拟合训练集时Training RSS（Not Test RSS）越来越小。尤其当变量越来越多得时候，Training Error将会降低，但Test Error不会。</li><li>所以，Training Set RSS，Training Set $R^2$并不能用从包含不同个数变量的模型中挑选出一个最优模型。但可以使用一些技术来调整模型的训练误差，从而从包含不同变量个数的模型中寻找最优模型。一共有4种方法，它们分别为：$C_p$, AIC, BIC, 以及 Adjusted $R^2$。</li></ul><blockquote><p>$C_p$</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Statistic Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning 1</title>
      <link href="/2020/05/06/reinforcement-learning1/"/>
      <url>/2020/05/06/reinforcement-learning1/</url>
      
        <content type="html"><![CDATA[<h1 id="1-RL几个应用"><a href="#1-RL几个应用" class="headerlink" title="1. RL几个应用"></a>1. RL几个应用</h1><blockquote><p>Learning to play AlphaGo</p></blockquote><ul><li>Supervised（监督学习的方式）<ul><li>从老师那里学习，即通过打标签的方式学习</li><li>比如看棋谱学，但棋谱是人下的，人下的那一步就是最优的吗？</li></ul></li><li>Reinforement Learning<ul><li>从经验中学习，其实人可能都不知道哪一步是最优的</li></ul></li><li>AlphaGo的学习方式<ul><li>先做Supervised Learning，从高手棋谱中学习</li><li>通过Supervised Learning训练出两个模型，他们互相博弈</li></ul></li></ul><blockquote><p>Learning a Chat-Bot</p></blockquote><ul><li>如何评判生成句子的好坏是关键问题，总不能一直“See you”循环下去。</li><li>预测将来可能出现的做法<ul><li>在RL中引入生成对抗网络</li><li>判别生成的句子是否符合真实数据的分布</li></ul></li></ul><blockquote><p>Playing Video Game</p></blockquote><ul><li>两个常用的游戏平台<ul><li>Gym</li><li>Universe</li></ul></li><li>episode，一局游戏，从开始到结束</li></ul><h1 id="2-RL的概念"><a href="#2-RL的概念" class="headerlink" title="2. RL的概念"></a>2. RL的概念</h1><blockquote><p>RL的难点</p></blockquote><ul><li><p>Reward Delay 奖励延迟</p><p>比如，打飞机游戏中，只有开火才会拿到Reward，但左右操作不会。但左右操作又会影响最后得分的高低，毕竟就算一直开火而不躲避也会导致被子弹击中从而导致游戏结束。<strong>所以，这就需要机器能够合理规划以获得长期的利益。</strong></p></li><li><p>Agent采取的行为会影响之后的行为</p><p>Agent需要充分的探索它所处的环境。还是以打飞机游戏为例，若是Agent从来都没有尝试过开火，那么它永远无法得到高分。</p></li></ul><blockquote><p>RL常用的做法</p></blockquote><ul><li>Policy-Based，主要目的是学习一个Actor</li><li>Value-Based，主要目的是学习一个Critic</li><li>那现在比较流行的做法是：Actor + Critic</li></ul><h1 id="3-RL三个步骤"><a href="#3-RL三个步骤" class="headerlink" title="3. RL三个步骤"></a>3. RL三个步骤</h1><blockquote><p>Neural Network as Actor</p></blockquote><ul><li>Input：机器的观察，Vector或者是Matrix</li><li>OutPut：输出层每一个神经元代表一个动作</li></ul><blockquote><p>Goodness of Actor 1</p></blockquote><ul><li><p>给定一个Actor $\pi<em>\theta(s)$，神经网络就是Actor，参数为$\theta$ 。使用$\pi</em>\theta(t)$来玩游戏，那么总体的Reward如下：</p><script type="math/tex; mode=display">Total~Reward : R_\theta = \sum_{t=1}^Tr_t</script></li><li><p>由于环境是一直在变化的（输入会变），比如打游戏玩家不同的位置、画面等，所以就算是同一个Actor玩千百次游戏最后得到的$R<em>\theta$也会不一样。而强化学习的目的是最大化Reward从而选择出最佳的Actor，那么直接最大化$R</em>\theta$是没有意义的，而应该最大化所有$R_\theta$的期望。</p></li><li><p>定义$\bar{R}<em>\theta$为$R</em>\theta$的期望值，所以最终的目的应该是最大化$\bar{R}_\theta$。当玩的所有局获得的Reward期望最大时，就说明这个Actor整体上达到了较好的能力。（可用随机放回采样钞票来类比，若是整体期望很大，那么盒子里钞票值普遍都比较大，即Reward普遍比较大）。</p></li></ul><blockquote><p>Goodness of Actor 2</p></blockquote><ul><li><p>将一个episode考虑为一个$\tau$，即一场游戏的开始到结束这个过程。这个过程有千百种，有些经常出现，有些不太出现。$\tau$的定义如下：</p><script type="math/tex; mode=display">\tau = \{s_1, a_1, r_1, s_2, a_2, r_2, ..., s_T, a_T, r_T\}</script><p>其中$s_i$代表当前的状态，$a_i$代表采取的动作，$r_i$代表收获的Reward。</p></li><li><p>当一个Actor开始玩游戏的时候，每一个$\tau$都有一定的概率出现，我们用$P(\tau|\theta)$来代表不同的$\tau$出现的概率。那么所有$R_\theta$的期望为：</p><script type="math/tex; mode=display">\bar{R}_\theta = \sum_{\tau}R(\tau)P(\tau|\theta)</script><p>这个公式的理解方式类比于离散随机变量求期望：假设可以穷举所有的$\tau$，那么$\bar{R}_\theta$应该为每种$\tau$出现的机率乘上该$\tau$能够获取的总体Reward的总值。</p></li><li><p>对$\bar{R}<em>\theta$的进一步思考：由于总体的目标是希望选择的Actor在不同的$\tau$下都能够获得尽可能高的$Reward$，那么使得$\bar{R}</em>\theta$最大的那个Actor就是我们所需要的。</p></li><li><p>由于前穷举所有的$\tau$可能性很低，那么只能用一个Actor玩N场游戏得到$\{\tau^1, \tau^2, …, \tau^3\}$来类比所有的$\tau$。也就相当于从$P(\tau|\theta)$中采样N次，那么每个$\tau$出现的概率应该为$\frac{1}{N}$，所以$\bar{R}_\theta$近似为：</p><script type="math/tex; mode=display">\bar{R}_\theta = \sum_tR(\tau)P(\tau|\theta) \approx \frac{1}{N}\sum_1^NR(\tau^n)</script><p>相当于把N个$\tau$的$R(\tau)$求和并取平均。</p></li></ul><blockquote><p>Pick the best Actor 1</p></blockquote><ul><li><p>根据前文，已将问题转化为：</p><script type="math/tex; mode=display">\theta^* = arg~\underset{\theta}{max}\bar{R}_\theta = arg~\underset{\theta}{max}~\sum_{\tau}R(\tau)P(\tau|\theta)</script></li><li><p>那么就可以采用梯度上升来更新模型的参数</p><ul><li>Start with $\theta^0$</li><li>$\theta^1 = \theta^0 + \eta\nabla\bar{R}_{\theta^0}$</li><li>$\theta^2 = \theta^1 + \eta\nabla\bar{R}_{\theta^1}$</li><li>……</li></ul><p>其中$\nabla\bar{R}<em>\theta = \left[\begin{matrix} \part\bar{R}</em>\theta/\part W<em>1 \\ \part\bar{R}</em>\theta/\part W<em>2 \\ … \\ \part\bar{R}</em>\theta/\part b_1\end{matrix}\right]$</p></li></ul><blockquote><p>Pick the best Actor 2</p></blockquote><p>那么$\nabla\bar{R}_\theta$到底该怎么求呢？</p><ul><li><p>根据$\bar{R}_\theta$的公式，可以得到：</p><script type="math/tex; mode=display">\nabla\bar{R}_\theta = \sum_\tau R(\tau)\nabla P(\tau|\theta)</script><ul><li>其中，$R(\tau)$和$\theta$并没有关系，所以不需要求梯度。因为Reward是环境给的，比如游戏中的计分板，我们不需要计分的具体过程和规则，只要拿到值就可以了。</li><li>所以即使$R(\tau)$不可微，不知道其具体的$function$也没有关系，只要给了输入能够给输出就行。</li></ul></li><li><p>再进一步看上面的式子，可以推导出：</p><script type="math/tex; mode=display">\nabla\bar{R}_\theta = \sum_\tau R(\tau)\nabla P(\tau|\theta) \\ = \sum_\tau R(\tau)P(\tau|\theta)\frac{\nabla P(\tau|\theta)}{P(\tau|\theta)} \\ \approx \frac{1}{N}\sum_{n=1}^N R(\tau^n) \nabla\log P(\tau^n|\theta)</script><p>约等于已在上文解释，因为无法穷取所有的$\tau$，故只能采样类比。那么每个$\tau$出现的概率应该为$\frac{1}{N}$，可以提到求和前面。</p></li></ul><blockquote><p>Pick the best Actor 3</p></blockquote><p>根据上文的推导，问题就转化为了$\nabla\log P(\tau^n|\theta) = ?$，那么该怎么求呢？</p><ul><li><p>前文定义$\tau = \{s_1, a_1, r_1, s_2, a_2, r_2, …, s_T, a_T, r_T\}$，故可以得到：</p><script type="math/tex; mode=display">P(\tau|\theta) = P(s_1)P(a_1|s_1, \theta)P(r_1,s_2|s_1, a_1)P(a_2|s_2, \theta)...</script><ul><li>$P(s_1)$代表游戏开始画面出现的几率</li><li>$P(a_1|s_1, \theta)$代表$\theta_1$下的Actor，在$s_1$状态下，取动作$a_1$的机率</li><li>$P(r_1,s_2|s_1, a_1)$代表，在$s_1$状态下采取动作$a_1$，获取$r_1$并指向状态$s_2$的概率</li></ul></li><li><p>那么，上面的式子可以进一步写为：</p><script type="math/tex; mode=display">P(\tau|\theta) = P(s_1)\prod_{t=1}^TP(a_t|s_t, \theta)P(r_t, s_{t+1}|s_t, a_t) \\ = \log P(s_1) + \sum_{t=1}^T \log P(a_t|s_t,\theta) + \log P(r_t, s_{t+1}|s_t, a_t)</script><p>其中$\log P(s<em>1)$和$\log P(r_t, s</em>{t+1}|s_t, a_t)$都是与$\theta$无关的项，与游戏环境相关。</p></li><li><p>于是，可以得到：</p><script type="math/tex; mode=display">\nabla \log P(\tau|\theta) = \sum_{t=1}^T \nabla \log P(a_t|s_t, \theta)</script></li></ul><blockquote><p>Pick the best Actor 4</p></blockquote><p>根据上文的推导，最终的问题可以转化为如下式子：</p><script type="math/tex; mode=display">\nabla \bar{R}_\theta \approx \frac{1}{N}\sum_{n=1}^N R(\tau^n) \nabla\log P(\tau^n|\theta) \\ = \frac{1}{N}\sum_{n=1}^N R(\tau^n) \sum_{t=1}^{T_n} \nabla \log P(a_t^n|s_t^n, \theta) \\ = \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n)\nabla \log P(a_t^n|s_t^n, \theta)</script><p>最后得到的一项式子是非常直觉的，可以体现学习过程中很多重要的信息，将在下面一一列出。</p><ul><li><p>机器在$\tau^n$过程中，在$s_t^n$的状态下采取行动$a_t^n$导致</p><ul><li>$R(\tau^n)$为正，则希望这个$a_t^n$出现的机率越大越好，需调整$\theta$来增加$P(a_t^n|s_t^n)$</li><li>$R(\tau^n)$为负，则希望这个$a_t^n$出现的机率越小越好，需调整$\theta$来降低$P(a_t^n|s_t^n)$</li></ul></li><li><p>$P(a_t^n|s_t^n, \theta)$的意思是：在某一$\tau$过程中，在$t$时刻的所观测的状态$s_t^n$，采取动作$a_t^n$的概率。</p><ul><li>但$P(a_t^n|s_t^n, \theta)$乘以的是该$\tau$整体的Reward $R(\tau^n)$，而不是当前时刻$t$采取行动后的Reward $R(\tau^n_t)$</li><li>所以，训练完毕后的Actor每次采取的动作往往是为了最大化整体的利益，而不是局部的利益。这就可以避免前文提到的问题：打飞机只有开火才能获得Reward，而左移、右移不会有任何Reward，但会对最大化Reward有帮助，所以机器每次做的决策要考虑全局而不是一直开火。</li></ul></li><li><p>$\nabla \log P(a_t^n|s_t^n, \theta)$为什么要取$\log$?分母$P(a_t^n|s_t^n, \theta)$的作用是什么呢？</p><script type="math/tex; mode=display">\nabla \log P(a_t^n|s_t^n, \theta) = \frac{\nabla P(a_t^n|s_t^n, \theta)}{P(a_t^n|s_t^n, \theta)}</script><ul><li>举个例子，如有以下的$\tau$过程<ul><li>$\tau^{13}$，采取行动a，$R(\tau^{13}) = 2$</li><li>$\tau^{15}$，采取行动b，$R(\tau^{15}) = 1$</li><li>$\tau^{17}$，采取行动b，$R(\tau^{17}) = 1$</li><li>$\tau^{33}$，采取行动b，$R(\tau^{33}) = 1$</li></ul></li><li>虽然动作a会有更高的Reward，但由于动作b出现次数很多，在求和所有R之后，b动作带来的总Reward会更高；这就导致机器将b出现的机率调高，以获得总体的Reward更高。故虽然a可以获取更高的Reward，机器也会限制它的发生机率。</li></ul></li><li><p>在看了上述例子之后，我们再回头解释分母$P(a_t^n|s_t^n, \theta)$存在的意义，其实$\frac{\nabla P(a_t^n|s_t^n, \theta)}{P(a_t^n|s_t^n, \theta)}$相当于对各种action出现的次数做了normalize，从而使得机器不会偏好出现次数过高的action，而是综合评价出现次数和单次行动获得的Reward。</p></li></ul><p>通过上述对目标函数直观的解释和理解，我们可以发现目标函数不仅保证了全局利益，也能保证得到做出合理决策的Actor。</p><blockquote><p>Add a Baseline 1</p></blockquote><p>若是$R(\tau)^n$一直为正，会发生什么事情？</p><ul><li><p>理想状态下，不会有什么问题。若有三个动作a、b、c，通过充分的采样，三个动作都发生了，那么三个动作在进行梯度下降的时候都会有相应的提升。</p></li><li><p>但若采样不充分，导致某个动作没有发生如动作a没有发生，就会使得动作b、c的机率增加，而a的机率减小。所以，我们希望$R(\tau^n)$有正有负，那么可以添加一项$b \approx E(R(\tau^n))$。</p><script type="math/tex; mode=display">\nabla \bar{R}_\theta = \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} (R(\tau^n) - b)\nabla \log P(a_t^n|s_t^n, \theta)</script></li></ul><blockquote><p>Add a BaseLine 2</p></blockquote><ul><li><p>假设有两个$\tau$的片段如下：</p><ul><li>$(s_a, a_1, r=5),~(s_b, a_2, r=0),~(s_c, a_3, r=-2), ~ R=3$</li><li>$(s_a, a_1, r=-5),~(s_b, a_2, r=0),~(s_c, a_3, r=-2), R=-7$</li></ul></li><li><p>在上面两个例子中，执行$a_2$并不会获得太差的Reward。但在原始的Policy Gradient中，乘上的权重为$R(\tau^n)$，即一场游戏的总Reward，这虽然能考虑全局收益，但同时拉低了某些好的action出现的概率，从而使得当前Actor出现偏好。</p></li><li><p>如在上述的例子中，$a_2$之前采取的动作其实和$a_2$将产生的结果并没有任何关系，$a_2$之后产生的结果只和$a_2$之后的动作相关。故可以修改Policy Gradient为：</p><script type="math/tex; mode=display">\nabla\bar{R_\theta} \approx \frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{T_n}[\sum_{t^{'}=t}^{T_n}r_{t^{'}}^n - b]\nabla logP_\theta(a_t^n|s_t^n)</script><p>即将权重从全局的Reward修改为，采取动作$a_t$之后所有Reward的总和</p></li><li><p>再进一步，由于当前的action对后续动作的影响会随着时间的推移而逐渐减弱，故需要乘上一个衰减因子$\gamma &lt; 1$，于是式子更新为：</p><script type="math/tex; mode=display">\nabla\bar{R_\theta} \approx \frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{T_n}[\sum_{t^{'}=t}^{T_n}\gamma^{t^{'}-t} r_{t^{'}}^n - b]\nabla logP_\theta(a_t^n|s_t^n)</script></li><li><p>中间的那一项称之为Advantage Function，即</p><script type="math/tex; mode=display">A^\theta(s_t, a_t) = \sum_{t^{'}=t}^{T_n}\gamma^{t^{'}-t} r_{t^{'}}^n - b</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Statistic Learning 5</title>
      <link href="/2020/05/02/statistic-learning5/"/>
      <url>/2020/05/02/statistic-learning5/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Cross-Validation-on-Classification-Problems"><a href="#1-Cross-Validation-on-Classification-Problems" class="headerlink" title="1. Cross-Validation on Classification Problems"></a>1. Cross-Validation on Classification Problems</h1><blockquote><p>Cross-Validation用于分类任务概念</p></blockquote><p>在系列前文中，描述的都是$Cross-Validation$在回归任务中的应用，并使用$MSE$量化Test Error，但$Cross-Validation$同样可以应用于分类任务中。</p><ul><li><p>在分类任务中，$Cross-Validation$的用法和前文一致，不一样的地方在于没有使用$MSE$来量化误差，而是使用错误分类的个数。</p></li><li><p>举个例子，在分类任务中，$LOOCV$的Error Rate计算如下：</p><script type="math/tex; mode=display">CV_{(n)} = \frac{1}{n}\sum_{i=1}^{n}Err_i</script><p>这里的$Err_i = I(y_i \neq \hat{y_i})$，$I$是指示函数，但括号内条件满足时值为$1$，否则值为$0$。</p></li></ul><blockquote><p>Cross-Validation用于分类任务实例</p></blockquote><ul><li><p>首先生成模拟数据，并选定Logistic Regression作为分类模型。由于数据为生成数据，可以得到此时的True Test Error=0.201，而此时的Bayes Error Rate=0.133，这说明了当前的Logistic Regression的灵活度难以完成对该生成数据的Bayes Decision Boundary建模。</p></li><li><p>但通过采用$predictors$的多项式形式，可以将Logistic Regression扩展为非线性决策边界。举个例子，Logistic Regression的二次型如下：</p><script type="math/tex; mode=display">log(\frac{p}{1-p}) = \beta_0 + \beta_1x_1 + \beta_2x_1^2 + \beta_3x_2 + \beta_4x_2^2</script><ul><li>若采用上述形式，Test Error Rate将会从0.201降低为0.197。</li><li>类似上面，拟合一个包含三次项形式$predictor$的Logistic Regression，Test Error Rate降低为0.160。</li><li>而拟合一个包含四次项形式的，Test Error Rate稍微上升，为0.162。</li></ul></li><li><p>但在实际应用中，Bayes Decision Boundary和Test Error Rate是未知的，那么我们如何决定使用上述4中Logistic Regression的哪一种形式呢？<strong>答案是：Cross-Validation。</strong></p><ul><li>可以使用$10-fold~CV$的方式分别训练不同自由度的Logistic Regression方法（多项式的次数从1到10）</li><li>从图中可以发现，Training Rate随着模型自由度的提升而降低（虽然单调下降，但是随着模型自由度的增加，Training Error Rate总体上呈下降的趋势）。</li><li>作为对比，Test Error展现出一个$U$型，即先下降再上升。说明自由度太高，模型可能过拟合了，所以在$U$型底部的模型应该是一个维持了$bais-variance~trade-off$的模型。</li><li>虽然一定程度上，$10-fold~CV$ Error Rate低估了错误率，但四阶多项式时其达到了最小值，这与三阶多项式测试曲线的最小值相当接近。</li></ul></li><li><p>同样，可以使用$KNN$方法用于对比</p><ul><li>可以发现，随着$K$值的减小，Training Rate随着模型灵活度的提高而再次下降（对于$KNN$，$K$越大，模型的自由度越低）。</li><li>同样，Cross-Validation Error Curve低估了Test Error Rate，但对于寻找最优$K$，它还是很有帮助的。</li></ul></li></ul><h1 id="2-BootStrap（自举法）"><a href="#2-BootStrap（自举法）" class="headerlink" title="2. BootStrap（自举法）"></a>2. BootStrap（自举法）</h1><p>$BootStrap$是一个广泛适用、功能强大的统计工具，可以用来量化和估计统计学习方法的不确定性。</p><ul><li>一个简单的例子是，$BootStrap$可以用来估计线性回归拟合系数的标准误差。</li><li>$BootStrap$的强大之处在于它可以很容易的应用于各种各样的统计学习方法。</li></ul><p>在下面的例子中，我们将讨论如何使用$BootStrap$评估线性模型参数的变化性。</p><blockquote><p>BootStrap实例</p></blockquote><ul><li><p>假设我们想将一笔固定的金额投资于两种分别产生$X$和$Y$回报的金额资产，其中$X$和$Y$时随机变量。</p><ul><li><p>通过参数$\alpha$来分配投资的比例，将$\alpha$比例的钱投资$X$，$(1-\alpha)$比例的钱投资$Y$。</p></li><li><p>由于$X,Y$的回报会不断变化，我们想要求出令投资风险最小的$\alpha$。换句话说，我们想要最小化：</p><script type="math/tex; mode=display">Var(\alpha X + (1-\alpha)Y)</script><p>即总回报的方差很小，偏离均值的幅度很少，那么风险就比较小。</p></li><li><p>为了最小化风险，可以通过下式来计算$\alpha$：</p><script type="math/tex; mode=display">\alpha = \frac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{XY}}</script><p>其中<script type="math/tex">\sigma_X^2 = Var(X)</script>，<script type="math/tex">\sigma_Y^2 = Var(Y)</script>，<script type="math/tex">\sigma_{XY} = Cov(X, Y)</script></p></li></ul></li><li><p>但在实际应用中，<script type="math/tex">\sigma_X^2,\sigma_Y^2, \sigma_{XY}</script>是未知的，但我们可以通过一个已有的测量$X,Y$的数据集来估计<script type="math/tex">\hat{\sigma}_X^2,\hat{\sigma}_Y^2, \hat{\sigma}_{XY}</script>，从而带入前面的式子中求得$\alpha$。</p><ul><li><p>为此，我们从真是分布中生成100批数据，并估计分布的$\hat{\alpha}$值。</p></li><li><p>重复上述操作1000次，那么就可以得到<script type="math/tex">\hat{\alpha}_1, \hat{\alpha}_2, \hat{\alpha}_3,...,\hat{\alpha}_{1000}</script>。</p></li><li><p>我们设置生成分布数据的<script type="math/tex">\sigma_X^2=1,\sigma_Y^2 = 1.25, \sigma_{XY} = 0.5</script>，故我们可以算出真实的$\alpha=0.6$。而此时1000个$\hat{\alpha}$的估计可以得到：</p><script type="math/tex; mode=display">\bar{\alpha} = \frac{1}{1000}\sum_{r=1}^{1000}\hat{\alpha_r} = 0.5996</script></li><li><p>这时${\bar{\alpha}}$和0.6非常接近了，这个值的标准差为：</p><script type="math/tex; mode=display">\sqrt{\frac{1}{1000-1}\sum_{r=1}^{1000}(\hat{\alpha_r - \bar{\alpha}})^2} = 0.083</script></li></ul></li><li><p>但在实际中，上述估计$SE(\hat{\alpha})$的方式难以应用，因为真实数据的分布是未知的，故我们无法从分布中多次采样新的数据。但$BootStrap$允许计算机来模拟这个采样过程，这就确保无需使用额外的样本来估计$\hat{\alpha}$了。$BootStrap$不再从分布中生成随机数据，而是在原始数据集中重复采样样本。</p><ul><li><p>假设有一个简单的数据集$Z$，从数据集$Z$中随机放回采样$n$个样本得到$bootstrap$数据集<script type="math/tex">z^{*i}</script>。由于是放回采样，这意味着<script type="math/tex">Z^{*i}</script>中的样本是可以重复的。</p></li><li><p>重复上述步骤$B$次，可以获得<script type="math/tex">Z^{*1},Z^{*2},Z^{*3},...,Z^{*B}</script>，同样我们可以计算出相对应的$\alpha$估计：<script type="math/tex">\hat{\alpha}^{*1},\hat{\alpha}^{*2},\hat{\alpha}^{*3},...,\hat{\alpha}^{*B}</script>。</p></li><li><p>于是，可以计算这些$bootstrap$的标准误差如下式：</p><script type="math/tex; mode=display">SE_B(\hat{\alpha}) = \sqrt{\frac{1}{B-1}\sum_{r=1}^B(\hat{\alpha}^{*r} - \frac{1}{B}\sum_{r^{'}=1}^B\hat{\alpha}^{*r})}</script><p>这就是对原始数据的$\hat{\alpha}$的估计</p></li></ul></li><li><p>最后，简单总结一下$BootStrap$：从原始数据集中随机放回采样$n$个样本$B$次，构建$B$个$bootstrap$数据集。通过对这些数据集的计算，获取统计量的分布。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Statistic Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Statistic Learning 4</title>
      <link href="/2020/04/26/statistic-learning4/"/>
      <url>/2020/04/26/statistic-learning4/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Resampling-Methods"><a href="#1-Resampling-Methods" class="headerlink" title="1. Resampling Methods"></a>1. Resampling Methods</h1><p>重采样方法是现代统计学习中不可缺失的一环。它们包括：</p><ul><li>重复的从训练集中采样</li><li>重复的拟合模型以获得额外的信息</li></ul><p>重采样方法会耗费计算机资源，因为需要从训练集中采集不同的子集来多次拟合同一个模型。下面，我们将要讨论两个较为常用的重采样方法：</p><ul><li>$Cross-Validation$（交叉验证）</li><li>$BootStrap$</li></ul><p>其中交叉验证估计测试误差以灵活的选择模型；$BootStrap$衡量了给定方法参数估计的准确性。</p><h1 id="2-Cross-Validation"><a href="#2-Cross-Validation" class="headerlink" title="2. Cross Validation"></a>2. Cross Validation</h1><blockquote><p>Test Error Rate, Train Error Rate</p></blockquote><ul><li>Test Error Rate得到了一个方法预测新样本的平均误差；而Train Error Rate得到了一个方法在训练过程中Response和Label的误差。</li><li>然而，Test Error Rate往往高于Train Error Rate</li></ul><p>当缺少一个较大的测试集来估计Test Error Rate的时候，还有很多其他的替代方案如通过训练集得到Test Error Rate。在交叉验证中，训练样本中一个假冒的子集会扮演测试集。</p><blockquote><p>The validation set approach</p></blockquote><ul><li>假设我们要估计一个模型的Test Error Rate，Validation set approach是个非常简单的策略。它将所有样本随机分成2份，一份作为Training Set，另一份作为Validation Set(Hold-out Set)。模型在Training Set上进行训练，并通过Validation Set来验证模型的能力。尝试用$MSE$来评价模型的验证效果，它是一种对Test Error Rate的评估。</li><li>现在使用Auto数据集来验证Validation Set Approach方法<ul><li>在Auto dataset中，mpg和horsepower有着非线性的关系，所以在线性回归模型中加入$horsepower^2$这一项，将会使得模型获得更好的效果。那么，我们很自然的想知道，三次拟合或者更高次的拟合会不会有更好的效果呢？在前文中，我们采用$p-value$来检测这一点，但现在可以使用验证方法如$MSE$来回答这个问题。</li><li>我们将392个样本随机分为2份，即Training Set和Validation Set各196个样本。我们使用Training Set拟合不同的回归模型（从1次到高次），并在Validation Set上使用$MSE$作为验证误差的度量，以评估各模型的性能。</li><li>从实验结果上，我们可以发现：二次拟合明显优于线性拟合；然而三次和之后的高次拟合MSE不减反增，这证明了它们并没有很好的提升模型的性能。当整个数据集被重复的随机分成不同的Training Set和Validation Set，并训练模型后，会发现各个MSE曲线各不相同，它们对于哪个模型能够获得最好的性能并没有统一的意见。我们仅能得到的结论是：二次项的加入大大提升了模型的性能，表现在MSE普遍降低，故线性回归不适合该数据集。</li></ul></li><li>Validation Set Approach很简单但有两个潜在的问题<ul><li>对所有数据集，不同的拆分发，训练出来的模型在验证集上$MSE$各不相同，过于依赖特定的Training Set和Validation Set。</li><li>在验证方法中，只用训练集中的样本拟合模型，故当训练样本较小时，验证集的Test Error Rate将被过高的估计。</li></ul></li></ul><h1 id="3-Leave-One-Out-Cross-Validation"><a href="#3-Leave-One-Out-Cross-Validation" class="headerlink" title="3. Leave-One-Out Cross-Validation"></a>3. Leave-One-Out Cross-Validation</h1><ul><li><p>简称$LOOCV$。假设有样本$\{(x_1, y_1), (x_2, y_2) …. (x_n, y_n)\}$，$LOOCV$每次从样本中挑选出一个样本作为Validation Set，剩下的$(n-1)$个样本当作Training Set。模型将会用$(n-1)$个样本来拟合模型，并用1个样本计算$MSE$值。</p><script type="math/tex; mode=display">MSE = (y_1 - \hat{y_1})^2</script></li><li><p>根据上述选择Test Set的方法，可将过程重复$n$次得到$n$个$MSE$的值。$LOOCV$估计Test MSE的方法为，求$n$个Test Error估计的均值：</p><script type="math/tex; mode=display">CV(n) = \frac{1}{n}\sum_{i=1}^nMSE_i</script></li><li><p>$LOOCV$相对于Validation Set Approach方法有以下两个优点</p><ul><li>偏差低（far less bias）。bias高会导致过高估计Test Error Rate，因为浮动性太大，而造成这个问题的主要原因是训练样本不充足。而$LOOCV$每次都选取$n-1$个样本，相对于Validation Set Approach的$\frac{n}{2}$，Training Set大得多。故偏差高的问题得到缓解。</li><li>Validation Set Approach中的Training Set中的样本随机性太强，导致各个MSE曲线各不相同，无法得出有用的结论。而$LOOCV$总会得出相似的MSE曲线，故在拆分训练集/测试集时没有随机性。</li></ul></li><li><p>然而，使用$LOOCV$对算力要求较高，因为要测试$n$次模型。当$n$过大时，会耗费大量的时间拟合最小二乘线性或者多项式回归中，但是可通过一个捷径减小$LOOCV$的开销至训练一个模型相当。公式如下：</p><script type="math/tex; mode=display">CV(n) = \frac{1}{n}\sum_{i=1}^n(\frac{y_i-\hat{y_i}}{1-h_i})^2</script><ul><li>其中，$\hat{y_i}$未原始最小二乘的第$i$个拟合值；$h_i$是高杠杆点。这个值和原始的$MSE$相似，但第$i$个残差除以了$(1-h_i)$。$leverage$值位于$(\frac{1}{n}, 1)$之间，反应了一个样本对于拟合自身的影响。因此，高杠杆点的残差被正确的计算，从而使得等式成立。</li><li>也就是说，影响Test Error Rate往往是异常点：离群点、高杠杆点。其中，高杠杆点的影响尤为明显，拟合$n$次模型就是为了消除这些异常点的影响。而通过上述捷径公式可直接消除异常点的影响，无需进行多次的拟合，但该方法并不一直使用。</li></ul></li></ul><h1 id="4-K-Fold-Cross-Validation"><a href="#4-K-Fold-Cross-Validation" class="headerlink" title="4. K-Fold Cross-Validation"></a>4. K-Fold Cross-Validation</h1><ul><li><p>$LOOCV$的一个替代品就是K-Fold CV，其步骤如下</p><ul><li><p>首先将训练集分成等大小的$k$组</p></li><li><p>每次选择其中一组作为验证集，模型拟合剩余的$(k-1)$组，MSE计算均方差。</p></li><li><p>重复$k$次后，K-Fold CV可用于计算下式的估计</p><script type="math/tex; mode=display">CV(k) = \frac{1}{k}\sum_i^kMSE_i</script></li></ul><p>不难发现，当$k=n$时，$LOOCV$是K-Fold CV的一个特殊形式。</p></li><li><p>在实践中，往往设置$k=5$或者$k=10$，那么K-Fold相对于$LOOCV$的优势在哪里呢？</p><ul><li>首先肯定是降低了计算量。$LOOCV$需要拟合$n$次模型，当$n$很大的时候，这几乎是场灾难。</li><li>$Bias-Variance-Trade-Off$。通过将训练集随机分成10个，并使用9个进行训练得到的Error Curve（误差曲线）的变化明显没有Validation Set Approach强烈。</li></ul></li><li><p>我们再详细的看一下$Bias-Variance-Trade-Off$。</p><ul><li>当$k&lt;n$时，K-Fold CV相对于$LOOCV$有着计算上的优势。我们将该优势放在一边，可以发现K-Fold CV一个不太明显但却更加重要的优点是：它相较于$LOOCV$对于Test Error Rate的估计会更加的精确，这和$Bias-Variance-Trade-Off$相关。</li><li>在上文中，我们提到Validation Set Approach会对Test Error Rate估计过高，因为这个方法中的训练集仅为数据集的一半。在这个逻辑下，不难看出$LOOCV$给出的测试误差近似无偏估计（因为每个训练集包含$(n-1)$个样本，接近整体了，求的就是每个整体数据集训练出来的模型的MSE的均值）。</li><li>而K-Fold CV，当$k=5$或$k=10$的时候，将会找到偏差较为中间的部分，因为每个训练集包含$\frac{n(k-1)}{k}$个样本，比$LOOCV$的方法少得多，但比Validation Approach多得多。所以，从减少偏差的角度，$LOOCV$优于K-Fold CV。</li><li>但在估计的过程中，偏差（Bias）并不是唯一需要关心的，我们必须要同时考虑$Variance$。事实证明，当$k&lt;n$时，$LOOCV$的$Variance$明显高于K-Fold CV。为什么？当我们进行$LOOCV$的时候，我们实际上室对$n$个拟合模型的输出进行平均，每个模型几乎都在相同的训练集上进行训练，因为这些输出高度相关。相比之下，K-Fold CV的输出相关性较小，因为每个模型的训练集的重叠度较小。</li><li>因为许多高度相关量的均值比低相关量的均值，有更高的方差；因此，$LOOCV$产生的测试误差估计往往比K-Fold CV有更高的方差。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Statistic Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Statistic Learning 3</title>
      <link href="/2020/04/14/statistic-learning3/"/>
      <url>/2020/04/14/statistic-learning3/</url>
      
        <content type="html"><![CDATA[<h1 id="1-LDA-for-p-gt-1"><a href="#1-LDA-for-p-gt-1" class="headerlink" title="1. LDA for p &gt; 1"></a>1. LDA for p &gt; 1</h1><p>假设$X=(x_1, x_2, …, x_p)$来自多元高斯分布，关于K类有着不同的均值，但协方差矩阵相同。</p><blockquote><p>多元高斯分布的一些直观概念</p></blockquote><ul><li>假设每个$predictor$符合一维高斯分布，每对$predictor$中存在相关性。<ul><li>当$p=2$时，表面积的高度代表了$(x_1, x_2)$那边附近的概率；若言这$x_1$轴或者$x_2$轴切开，那得到的截面都是一维高斯分布的形状</li><li>当$p=2$时，若$Var(x_1) = Var(x_2)$，$Cor(x_1, x_2)=0$，那么表面积将会是钟型。若$x_1,x_2$存在相关性，或者协方差矩阵不相等时，那么表面积的形状就是椭圆了。</li></ul></li></ul><blockquote><p>多元高斯分布的相关定义</p></blockquote><ul><li><p>一个p维的多元高斯分布的随机变量表示为$X\sim N(\mu, \Sigma)$</p><ul><li><p>其中$E(x)=\mu$是$X$的均值，一个$1\times p$维的向量</p></li><li><p>$Cov(X) = \Sigma$是$X$的$p\times p$维的协方差矩阵</p></li><li><p>多元高斯分布的密度函数定义为：</p><script type="math/tex; mode=display">f(x) = \frac{1}{(2\pi)^{\frac{p}{2}}\mid \Sigma\mid^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}</script></li></ul></li><li><p>当$p&gt;1$时，$LDA$的分类器第$k$个类别中的观测对象服从多元高斯分布$N(\mu_k, \Sigma)$，有着不同的均值$\mu_k$，相同的协方差矩阵$\Sigma$。将第$k$个类别的$f_k(X=x)$带入贝叶斯公式中，经代数变换后，对于$X=x$得到如下式子：</p><script type="math/tex; mode=display">\delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + log\pi_k)</script><p>最大的$\delta_k(x)$所对应的类别$k$即为$X=x$所属的类别。</p></li><li><p>假设现在有3个类别，$\mu_k$不同，但$\Sigma$相同。那么将会有3条线充当贝叶斯的决策边界，<strong>因为3个类对应了3对类别：1和2，1和3，2和3。</strong>为了得到$\delta_k$，就需要去估计未知参数$\mu_1, \mu_2, …, \mu_k$；$\pi_1, \pi_2, …, \pi_k$；以及$\Sigma$。其公式和$p=1$时的十分类似，同样$\delta_k(x)$是$x$的线性函数，这说明$LDA$依赖于其元素的线性组合。</p></li></ul><blockquote><p>LDA应用于Default数据集</p></blockquote><ul><li><p>当$LDA$应用于$Default$数据集的时候，可以发现$LDA$拟合10000条训练数据的训练误差为$2.75\%$。这听起来很低，但需要注意两点：</p><ul><li>训练误差往往低于测试误差。话句话说，当其用于预测一批新的个体是否会$default$的时候，将会表现的很糟糕。这是因为我们调整了模型的参数使得其在训练集上表现更好，但测试集和训练集的分布还是有差距的。</li><li>由于训练集中，仅有$3.33\%$的个体$default$了，在不考虑$balance$和$student ~ status$的情况下，一个简单的分类器总会倾向于预测一个个体将不会$default$。换句话说，一个没有任何参数的分类器预测所有样本为正样本，它的错误率也仅比$LDA$高一点而已。</li></ul></li><li><p>混肴矩阵</p><p>​      在使用$LDA$的过程中，$LDA$可能误把正样本当成负样本；把负样本当成正样本。使用<strong>混肴矩阵</strong>可以明确的表示这些指标。</p><ul><li>对于$default$数据来说，9667人中仅有23人被标记错误会$default$，错误率很低。但是，333个人会$default$的人，仅仅检测出了81个人，错误率高达$(333 - 81)/333 = 75.7\%$。</li><li>一般用$sensitivity$和$specificity$来描述医护和生物方面的分类指标。在$default$数据集中，$sensitivity$是被检测到真正会$default$的人，只有$(1 - 75.7\%) = 24.3\%$；而$specificity$是被检测到不会$default$的人，高达$(1-\frac{23}{9667}) \times 100\% = 99.8\%$。</li></ul></li><li><p>为什么$LDA$在辨别$default$人员时表现的如此差劲？如何去避免这个问题呢？</p><ul><li>$LDA$尝试去近似贝叶斯分类器，在所有类别中取得较低的整体错误率。为此，贝叶斯分类器为产生尽可能少的错误，再者正负样本不平衡，那么分类器倾向于预测测试样本为正样本。</li><li>降低置信度，如$Pr(default=Yes\mid X=x) &gt; 0.2$，那么更多的人将会被预测为$default=Yes$。对于银行来说，预测错几个正样本没问题，但遗漏了负样本将会是致命的问题。</li></ul></li></ul><blockquote><p>ROC曲线</p></blockquote><ul><li>ROC曲线的横坐标是$False ~ Positive ~ Rate$，纵坐标是$True ~ Positive ~ Rate$。</li><li>ROC曲线下的面积$AUC$反映了当前分类器的好坏。若$AUC$越大，说明该分类器越好。<strong>ROC曲线各对用的$FP,TP$</strong>值都是调节置信度得到的，但置信度并没有反应在曲线图中。</li><li>若是$predictors$和预测的$Response$无关，那么$ROC$曲线应该在$y=x$下面。</li></ul><h1 id="2-Quadratic-Discriminant-Analysis"><a href="#2-Quadratic-Discriminant-Analysis" class="headerlink" title="2. Quadratic Discriminant Analysis"></a>2. Quadratic Discriminant Analysis</h1><blockquote><p>Quadratic Discriminant Analysis（二次型判别分析）</p></blockquote><p>$LDA$假设样本来自多元高斯分布，均值向量不同，但$k$个类别的协方差矩阵相同。$QDA$和$LDA$类似，但$QDA$假设每个类别都有不同的协方差矩阵，即来自第$k$类的样本服从：</p><script type="math/tex; mode=display">X \sim N(\mu_k, \Sigma_k)</script><p>在这个假设下，贝叶斯分类器可简化为：</p><script type="math/tex; mode=display">\delta_k(x) = -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) - \frac{1}{2}log\mid\Sigma_k\mid + log\pi_k \\ = -\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k -\frac{1}{2}log\mid\Sigma_k\mid + log\pi_k</script><p>所以，$QDA$需要估计$\Sigma_k,\mu_k,\pi_k$，然后将$X=x$代入$\delta_k(x)$，最大的那一项所对应的$k$即为$X=x$所属的类别。由于$x$在上式中是二次函数，故$QDA$由此得名。</p><blockquote><p>QDA和LDA，用哪个更好？</p></blockquote><p>为什么要假设$X$是否共享同一协方差矩阵呢？或者$LDA$，$QDA$，用哪个更好？回答在于，<strong>bias-variance trade-off</strong>。</p><ul><li>若现在有$p$个$predictors$，为每一类别估计一个协方差矩阵总共需要$\frac{p(p+1)}{2}$个参数，$QDA$为每个类别都估计了$\Sigma$，那么就需要$k\frac{p(p+1)}{2}$个参数。若有50个$predictors$，那么就接近$1275$个参数，参数较$LDA$多了很多。所以$LDA$的灵活度没有$QDA$高。但反过来讲，$LDA$的$variance$更低，可能提高模型的能力。但需要注意的是，一旦$LDA$关于$k$个类共享同一个协方差矩阵的假设是错误的，那$LDA$就会受到高偏差的影响。</li><li>一般来说，若训练样本较少，$LDA$比$QDA$更加适合，因为减少$variance$很重要。相反，若是训练集很大，分类器的$variance$不是主要考虑的问题，或者说$k$类共享同一个协方差矩阵的假设站不住脚的，这时候就推荐使用$QDA$了。<strong>当贝叶斯的决策边界接近于非线性的时候，$QDA$是明显优于$LDA$的。</strong></li></ul><h1 id="3-A-comparison-of-Classification-Methods"><a href="#3-A-comparison-of-Classification-Methods" class="headerlink" title="3. A comparison of Classification Methods"></a>3. A comparison of Classification Methods</h1><p>主要比较：$Logistic ~ Regression$，$LDA$，$QDA$，$KNN$。</p><ul><li><p>首先比较$Logistic ~ Regression$和$LDA$</p><ul><li><p>$LDA$：$\delta_k(x) = x\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + log(\pi_k)$</p><p>对于二分类问题，且$p=1$时，$\begin{cases} P_1(x) \\ P_2(x)=1-P_1(x) \end{cases}$，可以推出：</p><script type="math/tex; mode=display">log(\frac{P_1(x)}{1-P_2(x)}) = log(\frac{P_1(x)}{P_2(x)}) = c_0 + c_1x  ~~~~~~~~ 公式(1)</script><p>这里的$c_0$，$c_1$都是$\mu_1, \mu_2, \sigma^2$的方法。</p></li><li><p>在$Logistic ~ Regression$中，</p><script type="math/tex; mode=display">log(\frac{P_1}{1-P_1}) = \beta_0 + \beta_1x ~~~~~~~~ 公式(2)</script></li><li><p>公式(1)和公式(2)都是$x$的线性方法，因此$Logistic ~ Regression$和$LDA$都产生了线性决策边界。<strong>两个方法唯一不同的是，$Logistic ~ Regression$</strong>和$LDA$只在拟合的过程中不同，大多数人可能认为它俩结果类似，虽然如此但也并不一直如此。那么它们分别在什么时候用呢？</p><ul><li>$LDA$假设样本来自高斯分布，且每个类别的方差相同。当该假设被满足时，$LDA$优于$Logistic ~ Regression$。</li><li>当高斯分布的假设未被满足时，$Logistic ~ Regression$优于$LDA$。</li></ul></li></ul></li><li><p>再谈谈$KNN$</p><ul><li>当预测$X=x$的类别时，$KNN$使用离$x$最近的$k$个样本中，出现此处最多的类别作为$x$的类别。</li><li>由于$KNN$是完全没有参数的方法，即对于决策边界的形状没有任何的约束，当真实决策边界为非线性时，使用$KNN$将会超越$LDA$和$Logistic ~ Regression$。</li></ul></li><li><p>然后谈谈$QDA$</p><ul><li>$QDA$是无参数$KNN$，线性$LDA$，$Logistic ~ Regression$三个方法折衷的一个版本。由于$QDA$假设了决策边界为二次型，比起线性模型，能解决更广的问题。虽然没有$KNN$灵活，当训练样本数量有限的时候，$QDA$能产生较好的效果，因为它确实对决策边界做了假设。</li></ul></li><li><p>最后总结一下各个方法应用的场景</p><ul><li>当真实的决策边界为线性时，$LDA$，$Logistic ~ Regression$都会产生比较好的效果。</li><li>当真实的决策边界为非线性时，$QDA$表现更加出色</li><li>当真实的决策边界更为复杂时，$KNN$能够表现的更好，但需合理的控制$K$值。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Statistic Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Statistic Learning 2</title>
      <link href="/2020/04/10/statistic-learning2/"/>
      <url>/2020/04/10/statistic-learning2/</url>
      
        <content type="html"><![CDATA[<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><blockquote><p>三种常用的分类方法</p></blockquote><ul><li>Logistic Regression</li><li>Linear Discriminant Analysis</li><li>K-NN Negihbor</li></ul><blockquote><p>为什么不用Linear Regression建模分类模型？</p></blockquote><ul><li><p>若使用Linear Regression来建模</p><script type="math/tex; mode=display">P(x) = \beta_0 + \beta_1x</script></li><li><p>上述模型存在两个问题</p><ul><li>概率可能为负</li><li>概率可能超过1</li></ul></li></ul><blockquote><p>Logistic Regression建模</p></blockquote><script type="math/tex; mode=display">P(x) = \frac{e^{\beta_0 + \beta_1x}}{1 + e^{\beta_0 + \beta_1x}}</script><ul><li><p>上述的模型是一根曲线，且范围在$(0, ~ 1)$</p></li><li><p>此外，可以得到</p><script type="math/tex; mode=display">\frac{P(x)}{1-P(x)} = e^{\beta_0 + \beta_1x}</script></li><li><p>由此可以推出</p><script type="math/tex; mode=display">log(\frac{P(x)}{1-P(x)}) = \beta_0 + \beta_1x</script></li><li><p>在上面的式子中，$logit$关于$x$是线性的；若是$x$增加$1-unit$，那么$logit$平均增加$\beta_1x$。但是$P(x)$和$x$不是线性关系的，若是$x$增加$1-unit$，$P(x)$改变的值根据当前值决定。若是忽略$x$的值，只要$\beta_1$为正数，那么增加$x$值会增加$P(x)$的概率。</p></li></ul><blockquote><p>Estimating the Regression Coefficients</p></blockquote><ul><li><p>可用最大似然的方法估计$\beta_0,\beta_1$的值</p></li><li><p>如何验证相关系数的有效性</p><script type="math/tex; mode=display">z-statistic = \frac{\beta_1}{SE(\hat{\beta_1})}</script><p>$z-statistic$的值越大，说明$SE(\hat{\beta_1})$的值越小；这将成为拒绝$H_0$的主要原因。</p></li></ul><blockquote><p>Confounding现象</p></blockquote><ul><li>在估计拖欠率的时候，只有$student$作为$predictor$。此时，若身份是学生，那么拖欠率会降低</li><li>但，增加了$balance, income$这两个$predictor$后，此时若身份是一个学生，那么拖欠率会变高。为什么呢？这是因为$balance$和$student$之间存在相关性。$balance$越大，会导致身份为$student$的拖欠率会升高。就像$Linear ~ Regression$中，一旦$predictor$间存在了相关性，$Y$的增长不仅会和一个$predictor$增长一个$1-unit$相关。</li></ul><h1 id="2-Linear-Discriminant-Analysis"><a href="#2-Linear-Discriminant-Analysis" class="headerlink" title="2. Linear Discriminant Analysis"></a>2. Linear Discriminant Analysis</h1><p>$Logistic ~ Regression$建立了$Response ~ Y$的条件分布；而在给定$predictor$的时候，$LDA$根据不同的类别，对$predictor$的分布进行建模。</p><blockquote><p>为什么有了Logistic Regression，还需要LDA?</p></blockquote><ul><li>当各类之间的边缘明确时，$LDA$比$Logistic ~ Regression$更加的稳定</li><li>当$n$较小时，$x$基本成正态分布，此时$LDA$更为稳定</li><li>当分类类别在$2$类之上时，$LDA$更加流行。</li></ul><blockquote><p>分类中的贝叶斯理论</p></blockquote><ul><li><p>定义符号</p><ul><li>$\pi_k$：随机选中一个样本属于第$k$类别的先验概率</li><li>$f_k(x) \equiv Pr(X=x|Y=k)$：类别$k$下样本分布的概率密度函数</li></ul></li><li><p>贝叶斯理论</p><script type="math/tex; mode=display">p_k(x) = Pr(Y=k|X=x) = \frac{\pi_kf_k(x)}{\sum_{l=1}^k\pi_lf_l(x)}</script><ul><li>可用缩写$p_k(x)$代表当$X=x$时，$Y=k$的概率</li><li>根据公式，若要估计$p_k(x)$，可以先估计$\pi_k,f_k(x)$再带入贝叶斯公式</li><li>$\pi_k$比较容易估计，可以通过计算训练集中各类别数据的占比；但$f_k(x)$的计算比较有挑战性。</li></ul></li><li><p>$p_k(x)$被称之为$X=x$属于第$k$个类别的后验概率；如果能找一种方法估计$p_k(x)$，就可以得到一个近似贝叶斯分类器的分类器。</p></li></ul><blockquote><p>Linear Discriminant Analysis for p=1</p></blockquote><ul><li><p>假设此时只有一个$predictor$，$p=1$；我们需要获得$f_k(x)$的估计，来计算$p_k(x)$。那么对于一个样本而言，$p_k(x)$最大的那个$k$就是它所属的类别。</p></li><li><p>为了估计$f_k(x)$，需要做一些假设</p><ul><li><p>假设$f_k(x)$服从正态分布，一维正太分布的概率密度函数如下：</p><script type="math/tex; mode=display">f_k(x) = \frac{1}{\sqrt2\sigma_k}e^{-\frac{1}{2\sigma_k^2}(x-\mu_k)^2}</script><p>其中，$\mu_k,\sigma_k$是第$k$个类别的均值和方差</p></li><li><p>此外，假设所有$k$个类别共享相同的方差，统称为$\sigma^2$</p></li></ul></li><li><p>将$f_k(x)$代入$p_k(x)$中，得到如下式子：</p><script type="math/tex; mode=display">p_k(x) = \frac{\pi_k\frac{1}{\sqrt2\sigma_k}e^{-\frac{1}{2\sigma_k^2}(x-\mu_k)^2}}{\sum_{l=1}^k\pi_l\frac{1}{\sqrt2\sigma_l}e^{-\frac{1}{2\sigma_l^2}(x-\mu_l)^2}} ~~~~~~~~ 公式(1)</script><p>其中，$\pi_k$声明了一个样本属于第$k$个类别的先验概率</p></li><li><p>若是对$公式(1)$取$log()$，可以得到以下式子：</p><script type="math/tex; mode=display">\delta_k(x) = x\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + log(\pi_k) ~~~~~~~~ 公式(2)</script></li><li><p>对于公式$2$，当$k=2, \pi_1=\pi_2$时，计算如下式子：</p><script type="math/tex; mode=display">\delta_1(x) - \delta_2(x) > 0 \\ \Rightarrow \frac{x}{\sigma^2}(\mu_1-\mu_2) - \frac{1}{2\sigma^2}(\mu_1^2 - \mu_2^2) > 0 \\ \Rightarrow 2x(\mu_1 - \mu_2) > \mu_1^2 - \mu_2^2</script><p>此时可以得到得到一个决策边界，如下：</p><script type="math/tex; mode=display">x = \frac{\mu_1^2-\mu_2^2}{2(\mu_1-\mu_2)} = \frac{\mu_1+\mu_2}{2} ~~~~~~~~ 公式(3)</script></li><li><p>在上面的例子中，我们假设$x$来自正态分布，所以我们知道$f_k(x)$，但需要估计$\mu,\sigma$；此外，我们还假设了$\pi_1=\pi_2$。那在实践中，我们依然假设每个类别中的$x$来自正态分布，但需要估计$\mu_1,\mu_2,…,\mu_k$，以及$\pi_1,\pi_2,…,\pi_k$和$\sigma^2$。</p></li><li><p>$LDA$通过估计$\pi_k,\mu_k,\sigma^2$来近似贝叶斯分类器。在实践中，当$p=1$时，可以估计出</p><script type="math/tex; mode=display">\begin{cases} \hat{\mu_k} = \frac{1}{n_k}\sum_{i:y_i=k}x_i \\ \hat{\sigma}^2 = \frac{1}{n-k}\sum_{k=1}^k\sum_{i:y_i=k}(x_i - \hat{\mu}_k)^2 \end{cases}</script><p>其中，$n$为训练样本总量；$n_k$为类别$k$的样本总量；可以看出</p><ul><li>$\hat{\mu}$仅仅是第$k$个类别中所有样本的均值</li><li>$\hat{\sigma}^2$是每$k$个类别样本方差的加权平均</li></ul></li><li><p>如何计算$\pi_k$?</p><ul><li><p>有的时候该项已知，直接用</p></li><li><p>当缺少$\pi_k$时，$LDA$使用训练集中某类样本占总样本量的比例来估计$\pi_k$</p><script type="math/tex; mode=display">\hat{\pi}_k = \frac{n_k}{n}</script></li><li><p>最终可以判别$X=x$属于不同类别的概率，式子如下</p><script type="math/tex; mode=display">\hat{\delta}_k(x) = x\frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}_k^2}{2\hat{\sigma}^2} + log(\hat{\pi}_k) ~~~~~~~~ 公式(4)</script></li><li><p><strong>LDA中的”Linear”就源于判别方法$\hat{\delta}_k$是x的线性方法。</strong></p></li></ul></li></ul><p>最后，重申以下：当$p=1$时，$LDA$假设每个类别都来自正态分布，均值不同，但共享同一个方差；估计$\pi,\mu,\sigma$后，带入到贝叶斯分类器中即可。</p>]]></content>
      
      
      <categories>
          
          <category> Statistic Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Statistic Learning 1</title>
      <link href="/2020/03/30/statistic-learning1/"/>
      <url>/2020/03/30/statistic-learning1/</url>
      
        <content type="html"><![CDATA[<h1 id="1-RSS-RSE-TSS等"><a href="#1-RSS-RSE-TSS等" class="headerlink" title="1. RSS, RSE, TSS等"></a>1. RSS, RSE, TSS等</h1><blockquote><p><strong>RSS（Residual Sum of Squares）</strong></p></blockquote><script type="math/tex; mode=display">RSS = e_1^2 + e_2^2 + e_3^2 + ... + e_n^2 \\  =(\hat{y_1} - \hat{\beta_0} - \hat{\beta_1}x_1) + ... + (\hat{y_n} - \hat{\beta_0} - \hat{\beta_1}x_n) \\  = \sum_{i=1}^n(y_i - \hat{y_i})^2</script><p>RSS定义了，在进行了回归之后，模型未能解释的变量。</p><blockquote><p><strong>RSE（Residual Standard Error）</strong></p></blockquote><script type="math/tex; mode=display">RSE = \sqrt{\frac{RSS}{n-2}} \\  =\sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y_i})^2}</script><p>RSE说明了，即使再好的回归模型也存在着RSE置信区间之内的误差，即模型对于数据的欠拟合程度。</p><blockquote><p><strong>TSS（Total Sum of Squares）</strong></p></blockquote><script type="math/tex; mode=display">TSS = \sum_{i=1}^n(y_i - \overline{y})</script><p>其中$\overline{y} = \frac{1}{n}y_i$。TSS定义了$y$自身的方差，即衡量了$Y$中$y$固有的变化程度。</p><blockquote><p><strong>$R^2$</strong></p></blockquote><script type="math/tex; mode=display">R^2 = \frac{TSS-RSS}{TSS} = 1-\frac{RSS}{TSS}</script><ul><li>$TSS$：衡量了$Y$中$y$固有的变化程度。</li><li>$RSS$：进行回归之后，模型未能解释的变量值</li><li>$TSS-RSS$：固有的变化程度 - 未能解释的变量值 = 能解释的变量值</li><li>$\frac{TSS-RSS}{TSS}$：已经解释的变量值占所有固有变化的比例</li></ul><p>$R^2$的变化区间为$(0, 1)$，与$y$的尺度无关。所以，理论上$R^2$越大应该越好，即大量的变量可以被回归所解释。但实际场景中，$R^2$的值要看应用。</p><blockquote><p><strong>$F-statistic$用于估计$H_0$</strong></p></blockquote><script type="math/tex; mode=display">F = \frac{(TSS - RSS)/p}{RSS/(n-p-1)}</script><p>其中，$n$为样本个数，$p$为多项式回归中的回归系数的个数</p><ul><li>$TSS$：$y$固有的方差，及固有的变量</li><li>$RSS$：回归后，未能解释的变量</li><li>$TSS - RSS$：回归后，能够解释的变量</li><li>$\frac{TSS-RSS}{p}$：回归后，每个$predictor$所占的解释比例<strong>（1）</strong></li><li>$\frac{RSS}{n-p+1}$：回归后，每个样本未能被解释的比例 <strong>（2）</strong></li><li>$\sigma=RSE=\sqrt{\frac{RSS}{n-2}}$：每个样本的未能被解释的占比<strong>（3）</strong></li></ul><p>若对于上述的<strong>（1），（2），（3）</strong>式</p><ul><li><p>（1）=（3），意味着每个$predictor$能解释的占比很低</p></li><li><p>（2）=（3），意味着每个样本能比解释的占比很低</p></li><li><p>可以推出$F-statistic=1$，即</p><script type="math/tex; mode=display">\beta_1 = \beta_2 = ...= \beta_n = 0</script><p>说明各个$predictor$对预测$y$都是没有帮助的。</p></li></ul><p>若对于上述的<strong>（1），（2），（3）</strong>式</p><ul><li><p>（1）&gt;（3），意味着每个$predictor$能解释的占比很高</p></li><li><p>（2）=（3），意味着每个样本能比解释的占比很低</p></li><li><p>可以推出可以推出$F-statistic&gt;1$，可以推出</p><p>$至少有一\beta_i,i\in\{1,2,…,p\}不为0$</p></li></ul><blockquote><p>$F-statistic$，用于检验部分predictors是否为0</p></blockquote><script type="math/tex; mode=display">F-statistic = \frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}</script><ul><li>$RSS_0$：省略了$q$个$predictors$的模型的$RSS$</li><li>$RSS_0 - RSS$：即这$q$个$predictor$能够解释的变量</li><li>$\frac{(RSS_0 - RSS)}{q}$：平均每个$q$能解释的变量的比例</li><li>$\frac{RSS}{n-p-1}$：平均每个样本未能被解释的比例</li></ul><p>如果使用个体的$t-statistic$和相关的$p-value$来衡量变量和响应之间的关系，很可能会得到错误的结论。</p><h1 id="2-Variable-selection"><a href="#2-Variable-selection" class="headerlink" title="2. Variable selection"></a>2. Variable selection</h1><p>在一个多元回归式中，究竟哪些变量是和$y$有关系的？将没有关系的找出来</p><ul><li><p>若是$p=2$，即有两个$predictor$，那么需要设计4个模型</p><ul><li>No variable</li><li>只包含$X_1$</li><li>只包含$X_2$</li><li>包含了$X_1,X_2$</li></ul><p>然后对每个模型，可用如下指标进行检验：$R^2, BIC, AIC, C_p$。但是当$p$特别打的时候，如$p=20$，那么就需要$2^{20}$个子集，这样做效率过低。故需要其他的手段</p></li><li><p><strong>Forward Selection</strong></p><p>先假设一个参数为空的模型，只有截距$\beta_0$。此外，有训练好了的$p$个$variable$。一个个往模型中加$variable$，并保证最低的$RSS$。满足某个条件的时候停止</p></li><li><p><strong>Backward Selection</strong></p><p>先假设所有的$variable$都要。然后，选择$p-value$最大的删除。不断地重复，直到满足某条件；如设定好$p-value$的阈值。</p></li><li><p><strong>Mixed Selection</strong></p><p>先假设一个参数为空的模型。然后，不断地加$Variable$进去，且保证加进去的使$p-value$最小，一旦超过了某个阈值，该$variable$就先放在一旁。最后，$p-value$分成两份，一份使得整个模型的$p-value$都较小，另一份使得$p-value$都较大。</p></li></ul><h1 id="3-Model-Fit"><a href="#3-Model-Fit" class="headerlink" title="3. Model Fit"></a>3. Model Fit</h1><p>两个衡量指标：$R^2,RSE$</p><ul><li><p>对于$RSE$来说，具有较多变量的模型都有更大的$RSE$，只要$RSS$增长的幅度比$p$小，如下公式：</p><script type="math/tex; mode=display">RSE = \sqrt{\frac{1}{n-p-1}RSS}</script></li></ul><h1 id="4-Prediction"><a href="#4-Prediction" class="headerlink" title="4. Prediction"></a>4. Prediction</h1><ul><li><p>两个error</p><ul><li>Random Error $\epsilon$不可控错误。即其他不明的错误未考虑进来，完美的变量是不可能被找到的，只能被估计。</li><li>Model Bias是可控变量。可以通过不断地做实验，或训练模型来减少它。</li></ul></li><li><p>两个interval</p><ul><li>Confidence interval。针对大部分城市的销售量区间，$95\%$的区间内包含了真实的值。</li><li>Prediction interval。针对某个特定城市的销售量区间，$95\%$的区间包含了真实的值。</li><li><strong>两个interval拥有相同的中心，但是prediction interval的范围比confidence interval的更加广。</strong></li></ul></li></ul><h1 id="5-两个强假设"><a href="#5-两个强假设" class="headerlink" title="5. 两个强假设"></a>5. 两个强假设</h1><p>Predictors and Responses are <strong>additive</strong> and <strong>linear</strong></p><ol><li><p>Additive</p><p>Predictor $x_i$的改变，那么$y$也相应的改变$\beta_i$的大小，和其他的predictors无关。即$x_i$造成的影响和其他的predictors相互独立。</p></li></ol><ol><li><p>Linear</p><p>Predictor$x_i$每次的改变$1-unit$对于$y$来说效果是一致的，无任何叠加的变化。</p></li></ol><blockquote><p>移除Additive假设，扩展线性回归</p></blockquote><ul><li><p>当为线性回归的时候，</p><script type="math/tex; mode=display">Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon</script><p>此时$x_1$的变化，会使得$Y$的变化只和$\beta_1x_1$相关，未考虑到$x_2$对于$x_1$的影响，可能也会对$Y$造成影响。</p></li><li><p>对线性回归进行扩展，如下：</p><script type="math/tex; mode=display">Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2 +  \epsilon \\ =\beta_0 + (\beta_1 + \beta_3x_2)x_1 + \beta_2x_2 + \epsilon \\ =\beta_0 + \tilde{\beta_1}x_1 + \beta_2x_2 + \epsilon</script><p>此时，$x_1$的变化会有$x_2$的参与，$x_1$和$x_2$的$interaction$被考虑了进来。举个例子：流水线个数和员工人数，决定了生产量。现在增加流水线，提升了生产量；但生产量的提升，不仅仅是流水线的功劳，还有员工的功劳，即员工和流水线的相互作用$interaction$。</p></li><li><p>Hierarchical Principle（层次性原则）</p><p>若是一个模型中包含了$interaction$，那么这个模型也必须包含主要的影响因子$x_1, ~x_2$，即使$x_1, ~ x_2$相关系数的$p-value$很大。也就是说，当$x_1, ~x_2$的$interaction$很重要的时候，$x_1,~x_2$造成的影响也没多少人感兴趣了。但是它们得包含在模型中，否则会违背$x_1,~x_2$相关这件事。</p></li></ul><blockquote><p>移除Linear假设，扩展到Non-linear Relationship</p></blockquote><script type="math/tex; mode=display">mpg = \beta_0 + \beta_1horsepower + \beta_2horsepower^2 + \epsilon</script><p>上述式子将$mpg$与$horsepower$的关系变为了非线性，可以看出来是一个二次的曲线。但需要注意的是，<strong>这仍是一个线性表达式</strong>，可以用线性回归的方法求解相关系数。因为改变的只是式子中的$predictor$而已，并不是相关系数。</p><h1 id="6-Potential-Problems"><a href="#6-Potential-Problems" class="headerlink" title="6. Potential Problems"></a>6. Potential Problems</h1><blockquote><p>Non-linearity of the response-predictor relationships</p></blockquote><ul><li><p>残差$e_i = y_i - \hat{y_i}$</p><p>残差图（$Residual ~ Plot$）最好是橄榄球状，否则说明response和predictors是非线性关系</p></li></ul><blockquote><p>Correlation of Error Terms</p></blockquote><p>$Linear ~ Regression Model$的$\epsilon_i, i\in\{1,2,…\}$应该是故不相关的。</p><ul><li>现有计算$regression coefficients$的方法都是基于$\epsilon_i$互不相关的假设。即当前数据的$\epsilon$不会影响到下一数据的$\epsilon$。否则当前计算出的$standard ~ error$将低估了正确的SD，因为没考虑到这种相关性，导致错的离谱。<strong>预测的区间和真实的比将会更宽，如$95\%$的置信区间其实并没有0.95这么高</strong></li><li>举个例子<ul><li>假设将已有的$n$数据复制了一份，共有$2n$份数据用于训练模型</li><li>虽然标准差是$2n$个样本的，但其实真实有效的数据只有$n$份。<strong>两份数据存在了相关性</strong>。</li><li>训练得到的$coefficient$是针对$2n$份数据的，导致真实的置信区间缩小了$\sqrt2$倍。</li></ul></li><li>在$time ~ series ~ data（时序序列数据）$中经常会出现$correlation$的问题。比如说，<strong>邻近时间点采集的数据，都会有相关的$\epsilon$。</strong>如果存在相关性，那么在残差图中就会发现追踪现象，即临近残差将会有相近的值。</li><li>$Correlation$对于$Linear ~ Regression$很重要。若是数据来自同一个家庭，一样的吃饭习惯，都会使得数据存在相关性。若是线性回归中，各个样本之间能够独立，将会有更大的意义。</li></ul><blockquote><p>Non-constant variance of error terms（误差项的不恒定方差）</p></blockquote><ul><li><p>一般来说，线性回归模型满足该假设</p><p>误差项有恒定的方差$var(\epsilon_i) = \sigma^2$</p></li><li><p>但如果$response$的值不断地增加，该方差就会越来越大。当面对这个问题的时候，一个可行的方法就是对$response$进行$\sqrt y$或者$logY$。</p></li></ul><blockquote><p>Outliers（离群点）</p></blockquote><ul><li><p>虽然离群点对于回归线的影响可能不大，但对于$RSE$，$R^2$指标都有着极大的影响，这导致对模型的分析出现严重的错误。比如说，$confidence ~ interval$，$p-value$的计算都出现问题。</p></li><li><p>可以通过</p><script type="math/tex; mode=display">Studenized ~ residuals = \frac{e_i}{RSS}</script><p>来计算，如果该值大于3，则该点为离群点</p></li></ul><blockquote><p>高杠杆点</p></blockquote><ul><li><p>高杠杆点势必离群点更危险的点，因为它容易带偏回归线。</p></li><li><p>对于高杠杆点的判断可通过如下公式</p><script type="math/tex; mode=display">h_i = \frac{1}{n}+\frac{(x_i - \overline{x})}{\sum_{i^{'}}^n(x_{i^{'}} - \overline{x})^2} ~ \in (\frac{1}{n}, 1)</script><p>若是$(x_i - \overline{x})$越大，则$h_i$越大，说明了该点更可能为高杠杆点。通常$h_i &gt; \frac{P+1}{n}$的点都是高杠杆点。</p></li></ul><blockquote><p>Collinearity（共线性）</p></blockquote><ul><li><p>两个$predictors$过于相关了，可以通过$VIF$指标来检测</p><script type="math/tex; mode=display">VIF(\hat{\beta}_j) = \frac{1}{1-R^2_{X_j|X_{-j}}}</script></li><li><p>共线性使得各个变量之间互相关。而$Linear ~ Regression$假设各个边缘之间独立，否则对预测会造成影响。但在现实生活中，数据间往往存在着相关性，但机器学习侧重于预测的准确率。若准确率很高，则不用过于关注。</p></li></ul><h1 id="7-几个问题总结"><a href="#7-几个问题总结" class="headerlink" title="7. 几个问题总结"></a>7. 几个问题总结</h1><blockquote><p>sales和budget之间是否存在关系？</p></blockquote><ul><li>通过多元回归将sale和TV，Radio，Newspaper联系起来</li><li>测试$H_0,\beta_i=0,i\in\{1,2,3,…\}$是否成立，使用$F-statistic$作为指标，$p-value$越低，说明存在关系的可能性越大。</li></ul><blockquote><p>Relationship有多强？</p></blockquote><ul><li>$RSE$估计了标准误差</li><li>$R^2$记录了$Response$中可以通过$Predictor$解释的变量占比</li></ul><blockquote><p>哪个媒体对sales有贡献？</p></blockquote><ul><li>检查每个$predictor$的$t-statistic$相关的$p-value$</li><li>$p-value$越低，说明贡献越大</li></ul><blockquote><p>每个媒体在$sales$上的影响有多大？</p></blockquote><ul><li><p>$\hat{\beta_j}$的标准差可用来构建置信区间。若置信区间内不包含$0$且远离$0$，那么说明response和该predictor占一定关系。</p></li><li><p>此外，共线性会导致标准差变大。故需要检测共线性是某predictor置信区间出现0的原因，通过$VIF$来检测。</p></li><li>若想检验单个变量对sale的影响，可以各自做线性回归。</li></ul><blockquote><p>预测能力有多强？</p></blockquote><ul><li><p>若使用预测区间</p><script type="math/tex; mode=display">Y = f(x) + \epsilon</script></li><li><p>若使用置信区间</p><script type="math/tex; mode=display">Y = f(x)</script></li></ul><p>预测区间比置信区间更加广阔，因为预测区间加入了不可控变量$\epsilon$。</p><blockquote><p>是否为线性关系？</p></blockquote><p>$residual ~ plot$可用来检测非线性</p><blockquote><p>广告数据存在协同性吗？</p></blockquote><ul><li>标准的线性回归模型假设$predictors$和$response$之间存在加性关系，即各个prediction互相独立。</li><li>每个predictor造成的影响不依赖其他的predictors</li></ul><blockquote><p>线性回归与K-NN Regression比较</p></blockquote><ul><li><p>线性回归是基于$parametric$类方法，有很好的优点</p><ul><li>仅需估计有限个$\beta$</li><li>可以用统计方法进行分析</li></ul><p>但也有缺点</p><ul><li>有$F(X)$的强假设，若数据和假设无关，造成准确率很低</li></ul></li><li><p>这时候就需要$non-parametric$的方法了，如$KNN ~ Regression$，如下</p><script type="math/tex; mode=display">\hat{f(x_0)} = \frac{1}{K}\underset{x_i\in N_0}\sum y_i</script><ul><li>当K很大时，以$MSE$为衡量指标不会比$Linear ~ Regression$差多少。但是当$k$很小的时候，$K-NN ~ Regerssion$就很差了。</li><li>在现实生活中，当predictors的个数很多的时候，对于$KNN ~ Regression$就会有维度灾难，其$MSE$很大。故大多是场合还是基于$Linear ~ Regression$。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Statistic Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读：PhotoRealistic Style Transfer系列</title>
      <link href="/2020/02/17/photorealistic-style-transfer/"/>
      <url>/2020/02/17/photorealistic-style-transfer/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Prerequisite-Knowledge"><a href="#1-Prerequisite-Knowledge" class="headerlink" title="1. Prerequisite Knowledge"></a>1. Prerequisite Knowledge</h1><p> 此部分为预备知识，主要涉及内容如下：</p><ul><li><p>Upsampling, Uppooling, Transpose Convolution（上采样，上池化，转置卷积）</p></li><li><p>Whitening and Coloring Transformations（白化与上色）</p></li><li>Wavelet Transforms（小波变换）</li></ul><p>若是熟悉这几块内容的童鞋可以直接跳过~</p><h2 id="1-1-Upsampling-UpPooling-Transpose-Convolution"><a href="#1-1-Upsampling-UpPooling-Transpose-Convolution" class="headerlink" title="1.1 Upsampling, UpPooling, Transpose Convolution"></a>1.1 Upsampling, UpPooling, Transpose Convolution</h2><ul><li><p>Upsampling（上采样）</p><p><img src="/2020/02/17/photorealistic-style-transfer/1-1-1.jpg" alt></p><p>可以看出，上采样相当于做了一个均值池化操作的反操作。但会有一个问题，恢复的特征图中只保留了低频信息，丢失了高频信息。也就是说，生成的图像会比较平滑，严重的话会有些模糊的纹理。</p></li><li><p>Uppooling（反池化）</p><p><img src="/2020/02/17/photorealistic-style-transfer/1-1-2.jpg" alt></p></li><li><p>Transpose Convolution</p><p>转置卷积，也有叫反卷积的。这是一个比较深的学问，而不是直接掉个包就觉得懂了。为什么一副较小的特征图能变大？这里就讲个转置卷积的大概流程：首先对原始特征图进行插值得到放大后的特征图，再对插值后的特征图进行卷积得到最终的结果。那怎么插值？插什么值比较好？这都是学问，大家可以查找相关资料，也建议阅读源码。</p></li></ul><h2 id="1-2-Whitening-and-Coloring-Transformations"><a href="#1-2-Whitening-and-Coloring-Transformations" class="headerlink" title="1.2 Whitening and Coloring Transformations"></a>1.2 Whitening and Coloring Transformations</h2><blockquote><p>前言</p></blockquote><p>假设<script type="math/tex">X</script>是一个均值为0的向量，那么我们可以得到其协方差矩阵:</p><script type="math/tex; mode=display">\Sigma = E(XX^T)</script><p>如果说<script type="math/tex">X</script>中的向量是互相关联的，那么其协方差矩阵肯定不会是一个对角矩阵。为什么这么说？假设<script type="math/tex">X^T = [x_1 ~ x_2]</script>那么<script type="math/tex">\Sigma = \left[ \begin{matrix} x_1x_1 & x_1x_2 \\ x_1x_2 & x_2x_2 \end{matrix} \right]</script>。由于<script type="math/tex">X</script>均值为0，所以其该协方差举证的对角线元素为方差，而其他的元素都为向量中每个元素之间的协方差。所以，一旦<script type="math/tex">X</script>中的向量相互都不关联，那么所有的协方差都为0，那么<script type="math/tex">\Sigma = \left[ \begin{matrix} x_1x_1 & 0 \\ 0 & x_2x_2 \end{matrix} \right]</script>,这就是一个对角矩阵。</p><p><strong>而白化的目的就是去除原始向量中各维度的相关性，且每个维度的方差为1。</strong></p><blockquote><p>Whitening Transformation（白化）</p></blockquote><p>白化的过程分为两步：</p><ol><li><p>去除向量<script type="math/tex">X</script>中各维度之间的相关性，具体步骤如下：</p><ul><li><p>首先通过下面的办法，找到向量<script type="math/tex">X</script>协方差矩阵<script type="math/tex">\Sigma</script>的特征值和特征向量</p><script type="math/tex; mode=display">\Sigma ~ \Phi = \Phi \Lambda</script><p>其中，<script type="math/tex">\Sigma</script>是对角矩阵，其对角线上的元素都是特征值，<script type="math/tex">\Phi</script>是协方差矩阵的特征向量。推导一下，可以得到如下等式：</p><script type="math/tex; mode=display">\Phi^T ~ \Sigma ~ \Phi = \Lambda</script></li><li><p>那么，若是想得到一个新的<script type="math/tex">X</script>，其协方差矩阵为对角矩阵的话，进行如下操作</p><script type="math/tex; mode=display">Y = \Phi^TX</script></li><li><p>最后我们检验一下，<script type="math/tex">Y</script>的协方差矩阵是否为对角阵</p><script type="math/tex; mode=display">\Sigma_Y = E(YY^T) = \Phi^TXX^T\Phi = \Phi^T ~ \Sigma ~ \Phi = \Lambda</script></li></ul></li><li><p>使得向量<script type="math/tex">X</script>中每个维度的方差为1，即使得协方差矩阵的对角线上的元素相同，这就是完整的白化过程。</p><ul><li><p>我们定义进行第二步操作之后的<script type="math/tex">Y</script>为<script type="math/tex">W</script>，那么：</p><script type="math/tex; mode=display">W = \Lambda^{-\frac{1}{2}} ~ Y = \Lambda^{-\frac{1}{2}} ~ \Phi^T ~ X</script></li><li><p>那么最后<script type="math/tex">X</script>就被转化成了<script type="math/tex">W</script>，<script type="math/tex">W</script>的协方差真的是一个对角元素相同的对角阵了吗？我们检查一下：</p><script type="math/tex; mode=display">\begin{eqnarray} E(WW^T) &=& E(\Lambda^{-\frac{1}{2}T} ~ YY^T ~ \Lambda^{-\frac{1}{2}}) \\ &=& \Lambda^{-\frac{1}{2}T} ~ E(YY^T) ~ \Lambda^{-\frac{1}{2}} \\ &=& \Lambda^{-\frac{1}{2}T} ~ \Lambda ~ \Lambda^{-\frac{1}{2}} \\ &=& \Lambda^{-\frac{1}{2}T} ~ \Lambda^{\frac{1}{2}} ~ \Lambda^{\frac{1}{2}} ~ \Lambda^{-\frac{1}{2}} \\ &=& II \\ &=& I\end{eqnarray}</script></li></ul><p>以上就是白化的所有操作，我们借用参考资料<strong>[3]</strong>中的图像再解释一下，如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/1-2-3.jpg" alt></p><ul><li>假设<script type="math/tex">S</script>是从<script type="math/tex">X</script>中多次采样获得的n个样本<script type="math/tex">X_i,i=1,2,..., n.</script>构成列向量的矩阵</li><li>那么上图(a)显示了原始采样数据<script type="math/tex">X_1, X_2</script>这两个变量的关系散点图。可以看到散点图中的点云斜向下，那么这两个变量成负相关。</li><li>图(b)显示了，经过第一步操作后获得的变量<script type="math/tex">Y_1, Y_2</script>的关系散点图。此时，散点图中的点云垂直于<script type="math/tex">Y_1</script>轴</li><li>图(c)显示了，经过第二步操作后获得的变量<script type="math/tex">W_1, W_2</script>的关系散点图。此时，散点图中的点云形成一个圆，这说明了<script type="math/tex">W_1, W_2</script>的相关性接近于0。也就达到了白化的目的，去相关性。</li></ul></li></ol><blockquote><p>Coloring  Transformations</p></blockquote><p>如果白化懂了，那么剩下的上色就很容易理解了。因为上色本质上就是白化操作的逆操作。承接上文中的矩阵<script type="math/tex">S</script>经白化后得到的样本<script type="math/tex">W_i,i=1,2,..., n.</script>，现在想把<script type="math/tex">S</script>恢复为协方差矩阵为<script type="math/tex">\Sigma</script>，且均值为<script type="math/tex">\mu</script>的矩阵。那么也需要两步操作</p><ol><li><p>化协方差矩阵为非单位阵</p><ul><li><p>首先，对协方差矩阵<script type="math/tex">\Sigma</script>进行特征值分解</p><script type="math/tex; mode=display">\Sigma = \Phi ~ \Lambda ~\Phi^T = \Phi ~ \Lambda^{\frac{1}{2}} ~ \Lambda^{\frac{1}{2}} ~ \Phi^T</script><p>其中，<script type="math/tex">\Lambda</script>是一个具有特征值<script type="math/tex">\lambda_i</script>的对角矩阵，<script type="math/tex">\Phi</script>是相对应的特征向量矩阵。由于<script type="math/tex">\Phi^T</script>中的特征向量互为正交，所以<script type="math/tex">\Phi^{-1} = \Phi^T</script></p></li><li><p>进行白化中第二步的逆操作，得到<script type="math/tex">Y</script></p><script type="math/tex; mode=display">Y = \Lambda^{\frac{1}{2}} ~ S</script></li></ul></li><li><p>化协方差矩阵为非对角阵，进行白化第一步操作的逆操作</p><script type="math/tex; mode=display">X = \Phi Y = \Phi ~ \Lambda^{\frac{1}{2}} ~ S</script></li></ol><p>至此，上色操作就结束了~</p><h2 id="1-3-Wavelet-Transforms"><a href="#1-3-Wavelet-Transforms" class="headerlink" title="1.3 Wavelet Transforms"></a>1.3 Wavelet Transforms</h2><p>这一段内容，主要是对参考文献<strong>[1]</strong>中部分相关资料的整理。由于非通信专业，对信号学的知识理解不多，感谢原作者分享的知识~。假设现在有一个信号<script type="math/tex">X= [90 ~~ 70 ~~ 100 ~~ 70]</script>，我想要对其进行压缩该怎么办呢？</p><ul><li><p>记<script type="math/tex">x_0=90 ~~ x_1=70 ~~ x_2=100 ~~ x_3=70</script>，现在我们通过如下计算重新获取它们的值</p><script type="math/tex; mode=display">\begin{cases} x_0^{'} = \frac{x_0 + x_1}{2} = 80 \\ x_1^{'} = \frac{x_0 - x_1}{2} = 10 \end{cases}</script><p>其中，80是平均数，而10则是它们的波动范围。同理，</p><script type="math/tex; mode=display">\begin{cases} x_2^{'} = \frac{x_2 + x_3}{2} = 85 \\ x_3^{'} = \frac{x_2 - x_3}{2} = 15 \end{cases}</script><p>其中，85是平均数，而15是它们的波动范围</p></li><li><p>通过上述计算得到的<script type="math/tex">\{80 ~~ 10 ~~ 85 ~~ 15\}</script>四个值</p><ul><li>其中80和85是局部平均值，反映该信号大的总体状态，是相对平缓的值；可以认为它们是<strong>信号的低频部分，记作<script type="math/tex">L</script></strong>。</li><li>而10和15是小范围波动的值，局部变化较快，可以认为<strong>它们是信号的高频部分，记作<script type="math/tex">H</script></strong>。</li></ul></li><li><p>现在，我们将<script type="math/tex">\{80 ~~ 10 ~~ 85 ~~ 15\}</script>重新排列，得到<script type="math/tex">[80 ~~ 85 ~~ 10 ~~ 15]</script>，记作<script type="math/tex">[L ~~ L ~~ H ~~ H]</script>。然后，我们重复第一步的操作，得到</p><script type="math/tex; mode=display">\begin{cases} x_0^{''} = \frac{x_0^{'} + x_1^{'}}{2} = 82.5 \\ x_1^{''} = \frac{x_0^{'} - x_1^{'}}{2} = -2.5 \\ x_2^{''} = \frac{x_2^{'} + x_3^{'}}{2} = 12.5 \\ x_3^{''} = \frac{x_2^{'} - x_3^{'}}{2} = -2.5 \end{cases}</script></li><li><p>通过上述计算得到的<script type="math/tex">\{82.5 ~~ -2.5 ~~ 12.5 ~~ -2.5\}</script>四个值，将它们重新排列，记作<script type="math/tex">[LL ~~ LH ~~ HL ~~ HH]</script>。</p></li></ul><p>现在，我们使用线性代数的知识，利用矩阵运算完成上述操作</p><ul><li><p>对于<script type="math/tex">x_0=90 ~~ x_1=70</script>，若要得到<script type="math/tex">x_0^{'}=80 ~~ x_1^{'}=10</script>，那么需要做如下初等变换</p><script type="math/tex; mode=display">[90 ~~ 70]\left[ \begin{matrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & -\frac{1}{2} \end{matrix} \right] = [80 ~~ 10]</script><p>这个右乘的矩阵<script type="math/tex">\frac{1}{2}\left[ \begin{matrix} 1 & 1 \\ 1 & -1 \end{matrix} \right]</script>是不是很熟悉，有点接近于<script type="math/tex">Haar ~~ Wavelet</script>了。</p></li><li><p>那么</p><script type="math/tex; mode=display">[L ~~ L ~~ H ~~ H] = [90 ~~ 70 ~~ 100 ~~ 70]\left[ \begin{matrix} \frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & -\frac{1}{2} \end{matrix} \right] = [80 ~~ 85 ~~ 10 ~~ 15]</script></li></ul><p>以此类推，大家可以通过熟悉的线性代数得到最后的<script type="math/tex">[LL ~~ LH ~~ HL ~~ HH]</script>。注意，<strong>为了得到<script type="math/tex">[LL ~~ LH ~~ HL ~~ HH]</script>，每次右乘的矩阵都是正交方阵，也就是说其可逆且<script type="math/tex">AA^T=I</script>，这个性质在后面的论文中会用到</strong>。</p><h1 id="2-Universal-Style-Transfer-via-Feature-Transforms-WCT"><a href="#2-Universal-Style-Transfer-via-Feature-Transforms-WCT" class="headerlink" title="2. Universal Style Transfer via Feature Transforms(WCT)"></a>2. Universal Style Transfer via Feature Transforms(<script type="math/tex">WCT</script>)</h1><blockquote><p>针对什么问题？</p></blockquote><p>现有的基于前向传播网络的风格迁移方法，只能局限于已见过的风格，难以推广到未见过的风格。此外，生成的图像质量一般。</p><blockquote><p>提出什么方法？</p></blockquote><p>在风格迁移中，引入<script type="math/tex">Whitening and coloring transforms</script>，将内容图的特征协方差直接与指定风格图的进行匹配。</p><blockquote><p>效果怎么样？</p></blockquote><p>能实现任意风格迁移，且生成图像质量较好。</p><h2 id="2-1-Model-Architecture"><a href="#2-1-Model-Architecture" class="headerlink" title="2.1 Model Architecture"></a>2.1 Model Architecture</h2><p>本文的模型设计如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/2-1-1.jpg" alt></p><ul><li><p>首先见图(a)，使用预训练好的VGG-19作为编码器完成特征抽取；并训练了与编码器对称的解码器，将VGG-19抽取的图像特征还原为原始的像素图像。</p></li><li><p>其次见图(b)，这是进行风格迁移的主要步骤，使用的方法为WCT，也就是前文提到的方法，稍微有些不同，下文会详细讲。Encoder层接收到内容图和风格图之后，分别提取它们的特征，再进行WCT操作。先白化内容图，去除图像之间的相关性；再通过上色操作，匹配风格图的协方差。</p></li><li><p>最后见图(c)，作者训练了多级别的风格化以得到更好的风格化图。为什么要这么做呢？作者首先使用VGG不同的特征层<script type="math/tex">Relu\_X\_1(X=1,2,...,5)</script>展示了相关的输出风格化图，如下：</p><p><img src="/2020/02/17/photorealistic-style-transfer/2-1-2.jpg" alt></p><p>可以看出：高层的特征捕捉了更为复杂的局部结构，而低层特征捕捉了更多的低级特征(如：颜色)。这是因为随着感受野的增大，特征的复杂度也会增大。所以，使用全部5层的特征应该可以从低到高的捕捉一个风格的所有特征。</p></li></ul><h2 id="2-2-WCT-Operation"><a href="#2-2-WCT-Operation" class="headerlink" title="2.2 WCT Operation"></a>2.2 WCT Operation</h2><ol><li><p>Whitening transfrom</p><ul><li><p>定义Encoder抽取到的内容特征为<script type="math/tex">f_c</script>，风格特征为<script type="math/tex">f_s</script>；首先进行白化操作，去中心化<script type="math/tex">f_c</script>即各通道减去均值，再进行如下转换：</p><script type="math/tex; mode=display">\hat{f_c} = E_cD_c^{-\frac{1}{2}}E_c^Tf_c</script><p>下面，我们对这个公式进行分析</p><ul><li>结合预备知识之所讲的，<script type="math/tex">E_c ~ D_c</script>分别为<script type="math/tex">f_c</script>协方差矩阵<script type="math/tex">f_cf_c^T</script>的特征向量矩阵和特征值矩阵，其中特征值矩阵<script type="math/tex">D_c</script>是对角矩阵。</li><li><script type="math/tex">E_c^Tf_c</script>使得其协方差矩阵为对角阵，<script type="math/tex">D_c^{-\frac{1}{2}}E_c^Tf_c</script>使得其协方差矩阵对单位阵。一直到这一步都是我们上面讲过的内容。</li><li>那么<script type="math/tex">E_cD_c^{-\frac{1}{2}}E_c^Tf_c</script>是什么呢，其本质是将特征又投影回原始的特征空间中，且其协方差矩阵依然为单位矩阵，即<script type="math/tex">\hat{f_c}\hat{f_c}^T = I</script>。源自于<script type="math/tex">ZCA降维</script>。有兴趣的童鞋可至参考链接<strong>[4]</strong>。</li></ul></li><li><p>通过上述的操作，原始的内容特征被转化为了白化特征。作者将其输入到了预训练好的解码器中，得到下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/2-1-3.jpg" alt></p><p>这说明白化后的内容特征依然保持了全局结构，但却移除了其自带的风格。这为上色操作做好了准备。</p></li></ul></li><li><p>Coloring transform</p><ul><li><p>首先，去中心化风格图即各通道减去均值，再进行如下转换：</p><script type="math/tex; mode=display">\hat{f_{cs}} = E_sD_s^{\frac{1}{2}}E_s^T\hat{f_c}</script><p>下面，我们对该公式进行分析</p><ul><li><p>同理，<script type="math/tex">E_s ~ D_s</script>分别为<script type="math/tex">f_s</script>协方差矩阵<script type="math/tex">f_sf_s^T</script>的特征向量矩阵和特征值矩阵，其中特征值矩阵<script type="math/tex">D_s</script>是对角矩阵。</p></li><li><p>即进行了上色操作，所有操作都是白化操作的逆操作，但是用的是<script type="math/tex">f_s</script>的相关特征。从而使得：</p><script type="math/tex; mode=display">\hat{f_{cs}}\hat{f_{cs}}^T = f_sf_s^T</script></li></ul></li><li><p>最后，再恢复中心化<script type="math/tex">\hat{f_{cs}}</script>，加上风格特征通道的均值，如下：</p><script type="math/tex; mode=display">\hat{f_{cs}} = \hat{f_{cs}} + m_s</script></li></ul></li><li><p>最后再进行如下操作来调整生成的风格化图中：内容和风格的强度。</p><script type="math/tex; mode=display">\hat{f_{cs} = \alpha\hat{f_{cs}}} + (1- \alpha)f_c</script></li></ol><h2 id="2-3-Decoder"><a href="#2-3-Decoder" class="headerlink" title="2.3 Decoder"></a>2.3 Decoder</h2><p>我们再来说说编码器Decoder，文中训练了多个Decoder，具体细节如下：</p><ul><li><p>分别使用<script type="math/tex">VGG19 ~ Relu\_X\_1(X=1,2,...,5)</script>的输出特征训练如下损失</p><script type="math/tex; mode=display">L = ||I_o - I_i||_2^2 + \lambda||\Phi(I_o) - \Phi(I_i)||_2^2</script><p>其中，<script type="math/tex">I_o ~ I_i</script>分别为重构的输出图像和输入图像，<script type="math/tex">\Phi()</script>就是<script type="math/tex">VGG</script>不同层输出的特征。<script type="math/tex">\lambda</script>是一个用来平衡两个损失的权重参数。</p></li><li><p>Decoder使用<script type="math/tex">COCO</script>数据集训练，一旦训练完毕，参数就固定了。</p></li></ul><h2 id="2-4-Experiment"><a href="#2-4-Experiment" class="headerlink" title="2.4 Experiment"></a>2.4 Experiment</h2><ul><li><p>本文首先将自己的方法和众多方法进行对比，如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/2-4-1.jpg" alt></p><ul><li>基于迭代优化的方法虽然可以进行任意风格迁移，但是会遭遇到局部极小的问题</li><li>基于前向网络的方法为了提升效率，牺牲了质量和普遍性，且会生成重叠的内容图像中的特征。</li><li><script type="math/tex">AdaIN</script>虽然也是任意风格迁移，但仅调整了内容图像，使其具备和风格图像一样的均值和方差，难以捕捉风格的高级表示。</li><li>基于内容块相似度的方法太过于约束内容特征，使得其只能捕捉风格图像低级的信息。</li></ul><p>相比之下，本文的方法不需要先去学习风格，就可以进行任意的风格迁移；同时，能够极大的保留原始的内容特征。</p></li><li><p>其次，比较了各方法的速度和用户偏爱程度，如下表</p><p><img src="/2020/02/17/photorealistic-style-transfer/2-4-2.jpg" alt></p><ul><li>可以看出，本文的方法受用户偏爱程度最高</li><li>且本文的方法速度非常快，虽然和<script type="math/tex">Adain</script>比相对慢了，但主要原因是为了计算协方差矩阵的特征值和特征向量而进行的特征值分解操作。</li></ul></li><li><p>接着，作者又展示了自己方法的生成图像上的灵活度，如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/2-4-3.jpg" alt></p><ul><li><p>由于网络参数已固定，通过调整风格图像的输入尺度，可以得到不同的风格化图像。</p></li><li><p>通过调整<script type="math/tex">\alpha</script>，即白化上色操作的最后一步，可以直接调整风格和内容的程度。</p></li><li><p>此外，当使用者想调用不同的风格图编辑同一张内容图时，可以使用空间控制，通过使用<script type="math/tex">M \bigodot f_c</script>替换<script type="math/tex">f_c</script>。这里的<script type="math/tex">\bigodot</script>是一个简单的掩码操作。效果如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/2-4-4.jpg" alt></p></li></ul></li><li><p>最后，作者又展示了自己方法在纹理合成上的优势。将内容图变为随机噪音图像如高斯噪音，那么本文的方法可以合成纹理，如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/2-4-5.jpg" alt></p><p>上图中，每对图像的左边是原始纹理，右边是合成纹理。作者说，多做几次WCT操作还是很有帮助的，合成的纹理会更好。此外，本文的方法还可以通过<script type="math/tex">\hat{f_{cs}} = \beta\hat{f_{cs1}} + (1-\beta)\hat{f_{cs2}}</script>合成两个不同的纹理，效果如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/2-4-6.jpg" alt></p><p>可以看出，效果还是很不错的。</p></li></ul><h1 id="3-A-Closed-form-Solution-to-Photorealistic-Image-Stylization-PhotoWCT"><a href="#3-A-Closed-form-Solution-to-Photorealistic-Image-Stylization-PhotoWCT" class="headerlink" title="3. A Closed-form Solution to Photorealistic Image Stylization($PhotoWCT$)"></a>3. A Closed-form Solution to Photorealistic Image Stylization($PhotoWCT$)</h1><blockquote><p>针对什么问题？</p></blockquote><p>现有的方法如$WCT$生成的风格化图存在着：1）有明显的伪影空间，2）相同空间却风格化不一致的问题。</p><blockquote><p>提出什么方法？</p></blockquote><p>提出新的风格迁移方法包括两个步骤：1）风格化，即$PhotoWCT$，2）平滑，即流行排序。各步骤效果如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-0-1.jpg" alt></p><p>其中，$F_1 ~ F_2$各代表第一、二步，可以看出效果主要体现在第二步。</p><blockquote><p>效果怎么样？</p></blockquote><p>方法跑的又快（被后面提出的$WCT^2$强力打脸），且生成的效果又逼真。</p><h2 id="3-1-First-Step"><a href="#3-1-First-Step" class="headerlink" title="3.1 First Step"></a>3.1 First Step</h2><p>本文提出方法的第一步很简单，仅对原$WCT$的模型做了简单的修改，如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-1-1.jpg" alt></p><ul><li><p>原始的$WCT$方法在解码器中使用的是上采样的方法，而本文将其替换成了上池化，以便保留更多的内容结构，使得生成的图像不再出现伪影的现象<strong>（这一说法被后文的$WCT^2$用指标强势打脸）</strong>。</p></li><li><p>可能很多童鞋会疑惑，难道不断地上采样或者上池化就可以完成图像的重构了？想想都不可能啊？对，你们想的没错，是不可能。每个上采样或者上池化操作之后都会附加一个卷积操作，还是得学习参数的。我去翻阅了下源码，的确也是这样的，如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-1-2.jpg" alt></p><p>可以看出，上池化之后先做了Padding操作，又接了一层卷积；这类似于自己实现了个反卷积的操作。</p></li></ul><p>本文的第一步就这样结束了，关键在于第二步~</p><h2 id="3-2-Second-Step"><a href="#3-2-Second-Step" class="headerlink" title="3.2 Second Step"></a>3.2 Second Step</h2><p>通过第一步生成的结果如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-2-1.jpg" alt></p><p>虽然效果是挺好的，但是被圈出来的天空有着风格不一致的问题，这也是很多风格迁移方法的通病。那么怎么样才能使得：</p><ul><li>生成的风格化图中局部内容相似的地方（如天空），有着一致的风格呢？</li><li>同时，又能保证最终的输出图，和第一步输出的图像内容相差不大？这样才能维持全局风格化的效果。</li></ul><p>本文选择的方法是“流行排序”。首先上述的问题被转化成以下的优化目标：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-2-2.jpg" alt></p><ul><li><p>式子中，$r$代表的是最终生成的图像，$y$代表的是第一步生成的图像。</p></li><li><p>故式子中的第二项很好理解，即满足上文提到的第二个目标：保证最终的输出图，和第一步输出的图像内容相差不大。</p></li><li><p>我们的主要目的是理解上式中的第一项，即：使得生成的风格化图中局部内容相似的地方有着一致的风格。其实这个目标又可以转化为：<strong>对于第一步生成的图像中的每个局部内容块，寻找与它在语义上最接近的那几个内容块，然后优化它们，使得它们之间的距离变短。体现出来的效果就是，天空具有一致化的风格了。</strong>如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-2-3.jpg" alt></p><ul><li>如果将图像的局部内容看作图中的圈，对于每个圈1来说，内容语义上离它最近的应该是圈2，3等，以此类推。</li><li>那么寻找这些最短的距离的内容块的过程就可以是：针对某一局部内容块，计算它与图中所有内容块之间的距离并将大小排序，选择离它最近的那几个，使得它们的语义相似即可。</li></ul></li></ul><p>说到距离度量，大家肯定会想到使用“欧氏距离”就行啦，为什么非要用“流行排序”呢？</p><ul><li><p>这是因为图像可以理解为高维空间中的低维流形，如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-2-4.jpg" alt></p><ul><li>如果把这些雕像的图像拉成一维向量，那么它们可以看成这个向量维度所在的空间中的散点图。</li><li>但并不是这个空间中的每个散点都是这个雕像的图像，如上图中的红点就不是。所以所有雕像的图像根本就撑不满这个高维的空间，那么雕像所处的空间就称为高维空间中的低维流形。</li><li>再举个通俗易懂的例子：二维空间中的点、线条；三维空间中的点、线条、平面、球体，都是高维中的低维流形。因为它们撑不满整个空间。</li></ul></li><li><p>那么对于一个图像流形中的点（即图像中的局部内容块），使用欧氏距离度量它们之间的距离合适吗？如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-2-5.jpg" alt></p><ul><li>图中的曲面为流形；红色的箭头度量的是流形中两个点的欧氏距离；</li><li>但流形中真正的距离应该为黄色的那条线；所以欧氏距离的度量不好，要用“流形排序”的方法。</li></ul></li><li><p>再举个通俗点的例子，如果将地球看作高维空间，那地球上的陆地平面就是低维流形，如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-2-6.jpg" alt></p><p>当距离很短时，两点之间的直线距离（即欧氏距离）可以近似度量两点之间的真正距离。但当两点之间的距离很大了，再使用欧氏距离（图中的黄线）来度量，就不准确了。</p></li></ul><p>以上就是第二步的内容，至于”流形排序“的具体算法，有兴趣的同学可以查看<strong>参考链接5</strong>，我们这里不多做展开。</p><h2 id="3-3-Experiment"><a href="#3-3-Experiment" class="headerlink" title="3.3 Experiment"></a>3.3 Experiment</h2><ul><li><p>本文最后的风格化图的确很好，如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-3-1.jpg" alt></p><p>可以看出来，不管是整体还是细节上，本文的方法都高于同行。</p></li><li><p>在第二步的优化目标中，有一个$\lambda$平衡着两项损失，本文做了实验以获取较好的$\lambda$，如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-3-2.jpg" alt></p><p>可以看出，在$\lambda=10^{-4}$的时候，能够取得较好的效果。</p></li><li><p>接着，作者有比较了自己方法的速度，如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-3-3.jpg" alt></p><p>可以发现，大部分的时间消耗都集中在第二步上，不过总体上来说还是很快的（被$WCT^2$啪啪打脸）</p></li><li><p>最后，作者展示了失败的例子，如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/3-3-4.jpg" alt></p><p>平滑步骤反而使得风格没有被迁移的那么明显，当然已有的方法也没有做到很好。</p></li></ul><h1 id="4-Photorealistic-Style-Transfer-viaWavelet-Transforms-WCT-2"><a href="#4-Photorealistic-Style-Transfer-viaWavelet-Transforms-WCT-2" class="headerlink" title="4. Photorealistic Style Transfer viaWavelet Transforms(WCT^2)"></a>4. Photorealistic Style Transfer viaWavelet Transforms(<script type="math/tex">WCT^2</script>)</h1><blockquote><p>针对什么问题？</p></blockquote><p>从<script type="math/tex">WCT</script>再到<script type="math/tex">PhotoWCT</script>，生成的图片效果是一次一次刷新眼界。但<script type="math/tex">WCT</script>和<script type="math/tex">PhotoWCT</script>都包含了多个Decoder，虽然会使得生成的图像保留更多的内容细节，但也会引入伪信号。此外，无论是最大池化还是上池化都会丢失原始图像的空间信息及细节。</p><blockquote><p>提出什么方法？</p></blockquote><p>本文使用Wavelet Pooling和Wavlet Unpooling取代最大池化和上池化，并构建了单一的Encoder和Decoder来取代多个Decoder的网络结构。</p><blockquote><p>效果怎么样？</p></blockquote><p>引用该文章摘要中的一句话：这是第一个也是唯一一个，可以在4.7秒内风格化<script type="math/tex">1024 \times 1024</script>分辨率图像的端到端模型，无需任何处理就能获得令人愉悦的真实感。</p><h2 id="4-1-Haar-wavelet-pooling-and-unpooling"><a href="#4-1-Haar-wavelet-pooling-and-unpooling" class="headerlink" title="4.1 Haar wavelet pooling and unpooling"></a>4.1 Haar wavelet pooling and unpooling</h2><blockquote><p>平均池化，最大池化出了什么问题？</p></blockquote><p>$WCT^2$的精髓就在于引入小s波变换充当池化和反池化层。</p><ul><li><p>从作者的提供材料里可以了解到，若将图像看作信号，若是想要恢复一个完整的信号<script type="math/tex">f</script>，那么就必须满足下列条件：</p><script type="math/tex; mode=display">\begin{cases} \tilde{\Phi}\Phi^T = I \\ z = \Phi f \\ f = \tilde{\Phi}z = \tilde{\Phi}\Phi f = f \end{cases}</script><p>​    这里面的<script type="math/tex">f</script>可以看作原始图像，<script type="math/tex">\Phi</script>看作某种新的池化操作，<script type="math/tex">\tilde{\Phi}</script>看作该新池化操作的反池化。从公式中可以看出，一旦满足<script type="math/tex">\tilde{\Phi}\Phi^T = I</script>，那么原始图像就可以被完整的恢复。</p></li><li><p>若从线性代数的角度来解释的话就是池化操作<script type="math/tex">\Phi</script>可逆，且该操作的逆为<script type="math/tex">\tilde{\Phi}</script>；或者说原始图像<script type="math/tex">f</script>经过两次基变换后能够回到原始的特征空间。</p></li><li>那么问题来了，<strong>我们熟悉的平均池化操作(Mean Pooling)或者最大池化操作(Max Pooling)存在逆操作吗？答案是不存在</strong>。一旦进行平均池化或者最大池化，新的特征空间无法保留原先特征空间的所有信息。一种更容易理解的方式就是，以平均池化为例，平均池化的逆操作只会得到数值一摸一样的特征点。</li></ul><blockquote><p>小波池化就能做到恢复所有的原始特征吗？</p></blockquote><p>答案是Yes。在<script type="math/tex">WCT^2</script>中主要使用的是<script type="math/tex">Haar ~ wavelets</script>来做池化和反池化。</p><ul><li><p>$Haar ~ wavelets$一共有四个滤波器，分别为$\{LL^T ~ LH^T HL^T HH^T \}$，其中</p><script type="math/tex; mode=display">L^T = \frac{1}{\sqrt[]{2}}[1 ~~~ 1], ~~~ H^T = \frac{1}{\sqrt[]{2}}[-1 ~~~ 1]</script><p>​    其中低通滤波器专门用来捕获图像中光滑的表面和纹理，这其实也就是图像中的大部分信息了。高通滤波器用来提取垂直、水平和对角的边缘类信息，这就对对应了图像中的细节信息。</p></li><li><p>这里，我们可以先计算出所有的滤波器，为后文滤波器的运用做铺垫。计算的结果如下（该数值经过作者源码的验证，有兴趣可以自己去查看一下源码，写的很干净）：</p><script type="math/tex; mode=display">LL^T=\frac{1}{2}\left[ \begin{matrix} 1 & 1 \\ 1 & 1 \end{matrix} \right] ~~~~~~~~~~~~~ LH^T=\frac{1}{2}\left[ \begin{matrix} -1 & 1 \\ -1 & 1 \end{matrix} \right]</script><script type="math/tex; mode=display">HL^T=\frac{1}{2}\left[ \begin{matrix} -1 & -1 \\ 1 & 1 \end{matrix} \right] ~~~~~~~~ HH^T=\frac{1}{2}\left[ \begin{matrix} 1 & -1 \\ -1 & 1 \end{matrix} \right]</script></li><li><p>作者在文中说到：<strong>通过对原始信号的镜像操作，就可以重建原信号，具体操作就是先进行转置卷积，再将所有转置卷积的结果求和</strong>。这个是小波分解运用的核心在将它之前，我们需要看一下模型的结构</p></li></ul><h2 id="4-2-Model-Architecture"><a href="#4-2-Model-Architecture" class="headerlink" title="4.2 Model Architecture"></a>4.2 Model Architecture</h2><p>本文具体的模型结构如下：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-model-1.jpg" alt></p><ul><li><p>上图清晰的展示了<script type="math/tex">WCT^2</script>的模型结构，可以发现它有以下几个特点</p><ul><li>使用<script type="math/tex">Wavelet ~ Pooling</script>以及<script type="math/tex">Wavelet ~ Unpooling</script>取代了最大池化，根据<script type="math/tex">Haar ~  Wavelet</script>小波4个滤波器的性质，所有的池化窗口应该为<script type="math/tex">2 \times 2</script>。</li><li>取消了多Decoder的模式，而分别在Encoder和Decoder中对称的做了多次的<script type="math/tex">WCT</script>操作。</li><li>因为<script type="math/tex">Haar ~ wavelets</script>有四个滤波器，那么输出的通道应该为4层。其中<script type="math/tex">LL</script>对应低频信息，<script type="math/tex">LH ~ HL ~ HH</script>对应高频信息。在Encoder中，仅有低频特征<script type="math/tex">LL</script>前传，而剩余的高频特征<script type="math/tex">LH ~ HL ~ HH</script>都先保留下来，并传递给了Decoder中与Encoder对称的地方。</li></ul></li><li><p>再结合我们上一小结提到的：通过对原始信号的镜像操作，就可以重建原信号，具体操作就是先进行转置卷积，再将所有转置卷积的结果求和。在理解这句话之前，我们先提几个问题？<strong>为什么小波变换能够保留更多的原始信息？</strong>，<strong>为什么转置之后求和，求和之后真的能恢复原始图像中的所有信息吗？</strong>现在我们进行解答</p><ul><li><p>定义原始特征层为<script type="math/tex">f</script>，那么经过池化后可以得到<script type="math/tex">LL ~ LH ~ HL ~ HH</script>的表达式如下</p><script type="math/tex; mode=display">\begin{cases} LL = LL^Tf \\ LH = LH^Tf \\ HL = HL^Tf \\ HH = HH^Tf \end{cases}</script></li><li><p>若是我们暂不考虑图像特征在前向传播中的改变，即图像在Decoder中对称的<script type="math/tex">LL</script>不变，那么进行转置卷积之后，再求和的结果是什么呢？</p><script type="math/tex; mode=display">\begin{cases} LL^{'} = LL^TLL^Tf = \frac{1}{2}\left[ \begin{matrix} 1 & 1 \\ 1 & 1 \end{matrix} \right]f \\ LH^{'} = LH^TLH^Tf = 0 \\ HL^{'} = HL^THL^Tf = 0 \\ HH^{'} = HH^THH^Tf = \frac{1}{2}\left[ \begin{matrix} 1 & -1 \\ -1 & 1 \end{matrix} \right]f \end{cases}</script><p>那么，<script type="math/tex">LL^{'} + LH^{'} + HL^{'} + HH^{'} = \frac{1}{2}\left[ \begin{matrix} 2 & 0 \\ 0 & 2 \end{matrix} \right]f = If = f</script>，没错就是这么神奇，原始信号经过两次小波变换再求和得到的就是原始信号。所以说<script type="math/tex">Haar ~ Wavelet</script>作为池化层和反池化层的卷积核能够完整的重建信号。</p></li><li><p>即使在前向传播的过程中，原始特征图的特征改变了，但原始图像的低频信息和高频信息都很好的被保留了下来，这使得生成的图像更加细腻、更加逼真。</p></li></ul><p>至此，本文中关于小波变换的部分讲解就到此结束了，下面我们还需要考虑模型的其他信息。</p></li><li><p>虽然说在Decoder中，进行转置卷积并求和有着理论上的正确性，但在实际操作过程中，作者并未采取求和的办法，而是类似于<script type="math/tex">U-Net</script>的拼接操作。这虽然会增加训练参数，但是作者说生成的图像会更加的清晰，如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-model-2.jpg" alt></p><p>不过说实话，我感觉区别不大，可能是区别大的图像没有放出来。如果能在保证理论上的正确性又能取得很好的效果就更加完美了。</p></li><li><p>在上面的模型图中，大家可以看到在Encoder和Decoder中做了3次对称的WCT操作。如果再增加WCT的次数，可能会有更好的效果，效果如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-model-3.jpg" alt></p><p>可以看出WCT次数的增加会使得生成图像的风格更加鲜艳（会发现风格图的排版未对齐，这是作者留给观众的彩蛋吗？），但是也会涉及到更多的<script type="math/tex">SVD</script>操作，因为要分解出特征图的特征向量和特征对角阵，这会导致时间的增加。此外，按道理来说，Encoder中通过跳级连接传递到Decoder中的<script type="math/tex">LH ~ HL ~ HH</script>应该也要做WCT操作，但是为了降低计算量就取消了。</p></li></ul><h2 id="4-3-Experiments"><a href="#4-3-Experiments" class="headerlink" title="4.3 Experiments"></a>4.3 Experiments</h2><ul><li><p>由于作者认为：图像中的低频部分代表的是光滑的表面和纹理，高频部分代表的是边缘。故<script type="math/tex">LL</script>应该会影响到生成图像的整体纹理或者表面，而<script type="math/tex">LH ~ HL ~ HH</script>应该影响边缘信息。故作者做了一个实验，即在恢复图像的过程中仅保留<script type="math/tex">LL</script>的信息，效果如下：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-exp-1.jpg" alt></p><ul><li>可以看到，丢失了高频信息的图像中建筑的边缘并没有被风格化，而保留了低高频信息的建筑都被风格化了，这就验证了作者的观点。</li><li>此外，这还证明了其他研究人员提出来的观点：在风格迁移中使用平均池化会使得生成的图像更加有吸引力，因为仅保留<script type="math/tex">LL</script>的操作就类似于平均池化，只不过对于<script type="math/tex">2 \times 2</script>平均池化除4取平均，而<script type="math/tex">LL^T</script>滤波器除2取平均。</li></ul></li><li><p>作者为了验证<script type="math/tex">Wavelet ~ Pooling</script>的有效性，将其和<script type="math/tex">Split ~ Pooling</script>，<script type="math/tex">Learning~ Pooling</script>做对比，效果如下图：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-exp-2.jpg" alt></p><p>这个<script type="math/tex">Learning~ Pooling</script>的效果就惨不忍睹了，不过这个<script type="math/tex">Split ~ Pooling</script>看起来还不错。<script type="math/tex">Split ~ Pooling</script>也是使用<script type="math/tex">2 \times 2</script>的滤波器进行池化，且也能捕获全局信息，不过其表达的能力差了点，所以陆地上的草效果看起来就比较差劲。</p></li><li><p>当然，本文中最值得一提的实验就是使用<script type="math/tex">SSIM</script>以及<script type="math/tex">Gram ~ loss</script>作为验证指标，如下图所示：</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-exp-3.jpg" alt></p><ul><li>首先解释一下这幅图的横纵坐标。横坐标代表的是<script type="math/tex">SSIM</script>，越高说明生成的图像越好。纵坐标代表的是<script type="math/tex">Gram ~ Loss</script>，肯定是越小越好，越小就说明生成的图像和风格图的风格越接近。但注意<strong>纵轴坐标的值是递减而不是递增的</strong>，这样图中代表方法的点如果越靠近右上角，说明该方法越好。</li><li>从图中可以看到，靠近右上角基本都是<script type="math/tex">WCT^2</script>。其中<script type="math/tex">DPST</script>的闪光点在于其风格损失很小，因为它直接优化了风格损失。然后，作者在论文中加粗了一句话打脸了<script type="math/tex">PhotoWCT</script>提出使用<script type="math/tex">Unpooling</script>可以提升生成图像的质量。因为从图上来看，使用最大池化的<script type="math/tex">WCT</script>效果反而比使用<script type="math/tex">Unpooling</script>的好。所以，作者明确指出<script type="math/tex">PhotoWCT</script>生成图像质量好的主要原因还是第二阶段中经过“流形排序”处理了。</li></ul></li><li><p>最后再看一下，<script type="math/tex">WCT^2</script>的速度</p><p><img src="/2020/02/17/photorealistic-style-transfer/4-exp-4.jpg" alt></p><p>可以看出基于迭代过程的<script type="math/tex">DBSP</script>和<script type="math/tex">WCT</script>系列完全不是一个量级的。而<script type="math/tex">PhotoWCT</script>的第二步处理需要占用大量的内存和时间。所以<script type="math/tex">WCT^2</script>的速度优势很明显。</p></li></ul><h1 id="5-References"><a href="#5-References" class="headerlink" title="5. References"></a>5. References</h1><ol><li>小波变换轻松入门（<a href="https://blog.csdn.net/jtxhe/article/details/42005685?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task）" target="_blank" rel="noopener">https://blog.csdn.net/jtxhe/article/details/42005685?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task）</a></li><li>反卷积、上采样、上池化（<a href="https://blog.csdn.net/a_a_ron/article/details/79181108）" target="_blank" rel="noopener">https://blog.csdn.net/a_a_ron/article/details/79181108）</a></li><li>白化上色（<a href="https://www.projectrhea.org/rhea/images/1/15/Slecture_ECE662_Whitening_and_Coloring_Transforms_S14_MH.pdf）" target="_blank" rel="noopener">https://www.projectrhea.org/rhea/images/1/15/Slecture_ECE662_Whitening_and_Coloring_Transforms_S14_MH.pdf）</a></li><li>ZCA降维（<a href="http://ufldl.stanford.edu/tutorial/unsupervised/ExercisePCAWhitening/）" target="_blank" rel="noopener">http://ufldl.stanford.edu/tutorial/unsupervised/ExercisePCAWhitening/）</a></li><li>流行排序（<a href="https://blog.csdn.net/davidsmith8/article/details/78515747?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task）" target="_blank" rel="noopener">https://blog.csdn.net/davidsmith8/article/details/78515747?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task）</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文泛读2：基于Adjustable Parameters的Style Transfer</title>
      <link href="/2020/02/15/extensive-reading2/"/>
      <url>/2020/02/15/extensive-reading2/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Adjustable-Real-Time-Style-Transfer-ICLR-2020"><a href="#1-Adjustable-Real-Time-Style-Transfer-ICLR-2020" class="headerlink" title="1. Adjustable Real-Time Style Transfer (ICLR 2020)"></a>1. Adjustable Real-Time Style Transfer (ICLR 2020)</h1><blockquote><p>针对什么问题</p></blockquote><p>​    本文指出基于现有风格迁移算法训练得到的模型，只能生成固定内容结构及风格模式的Stylized Images(风格化图)。此外，选取不同的风格图作为输入，总是得调整损失函数中每一层卷积特征的权重$w$，一旦使用者对生成的结果不满意，就要重新训练该模型。</p><blockquote><p>提出什么方法</p></blockquote><p>​    本文提出一种可以在训练和测试阶段自动调整关键参数的方法，使得训练得到的风格迁移模型可以通过调整参数，生成风格迥异的Stylized Images。</p><blockquote><p>效果怎么样</p></blockquote><p>​    本文的方法通过训练一个单一的模型，和多次训练得到的不同风格迁移模型，在生成的Style Images上达到相似的效果。</p><h2 id="1-1-Model"><a href="#1-1-Model" class="headerlink" title="1.1 Model"></a>1.1 Model</h2><p>本文的模型如下图所示：</p><p><img src="/2020/02/15/extensive-reading2/1-model.jpg" alt></p><ul><li><p>模型的构造很简单，和快速风格迁移的模型类似，最大的区别在于新加入的可调节损失权重$\alpha_c, \alpha_s$，用于替代原始的快速风格迁移模型中计算内容损失$L_c$和风格损失$L_s$的的权重。</p></li><li><p>此外, <script type="math/tex">\alpha_c, \alpha_s</script>不仅加入到损失函数中，还通过训练前馈神经网络$\Lambda$并以CIN(Conditional Instance Normalization)的方式得到<script type="math/tex">\gamma_\alpha, \gamma_\alpha</script>以改变激活层的输出（如下面公式所示）。因此，在测试的时候可以通过调节<script type="math/tex">\alpha_c, \alpha_s</script>以调节生成器的输出。</p><script type="math/tex; mode=display">z = \gamma_{\alpha}(\frac{x - \mu}{\sigma}) + \beta_{\alpha}</script><script type="math/tex; mode=display">\gamma_{\alpha}, \beta_{\alpha} = \Lambda(\alpha_c, \alpha_s)</script></li><li><p>本文的实验提到了训练中$\alpha_c, \alpha_s$的取值。</p><ul><li><p>本文采用预训练的Vgg19提取特征。</p></li><li><p>本文使用$conv3$提取内容特征，故$\alpha_c = 1$，也就是说$\alpha_c$的取值固定不变。</p></li><li>本文使用$conv2, conv3, conv4$提取风格特征，所以这三层中$\alpha_s$的取值随即从$U(0, 1)$中采样。</li></ul></li></ul><h2 id="1-2-Optimization"><a href="#1-2-Optimization" class="headerlink" title="1.2 Optimization"></a>1.2 Optimization</h2><p>本文采用的就是常规的风格迁移算法损失：</p><script type="math/tex; mode=display">L_c(p) = \underset{l \in C}\sum\alpha_c^lL_c^l(p)</script><script type="math/tex; mode=display">L_s(p) = \underset{l \in S}\sum\alpha_s^lL_s^l(p)</script><p>最大的区别就是，将原始的损失权重$w$替换成了可调节的$\alpha$</p><h2 id="1-3-Experiment"><a href="#1-3-Experiment" class="headerlink" title="1.3 Experiment"></a>1.3 Experiment</h2><p>本文做的实验主要有以下几方面</p><ul><li>验证可调节参数的有效性</li><li>通过可调节参数，生成随机化的Stylized Images</li><li>和已有的BaseLine作比较</li></ul><p>下面挑选几个实验做讲解</p><p><img src="/2020/02/15/extensive-reading2/1-exp1.jpg" alt></p><ul><li>由于实验中，$\alpha_c$固定为1， $\alpha_s$的三个值可调节，故作者分别固定$\alpha_s$中其他两个值不变，剩余的一个值从0至1调节，得到上图的结果。可以发现，当调节不同层所对应的$\alpha_s$时，生成的Stylized Image风格变化的模式是不同的。这也凸显了作者方法的有效性。</li></ul><p><img src="/2020/02/15/extensive-reading2/1-exp2.jpg" alt></p><ul><li>在上面的实验中，作者通过常规的方法调节参数训练单一模型作为base，并调节关键参数使得自己的模型权重和单一模型一致。可以看出，作者的方法生成的图像和多个单一训练的模型生成的图像效果较为接近，这也说明了通过本文的方法，训练单一的模型在一定程度上能取代多个模型。</li></ul><p><img src="/2020/02/15/extensive-reading2/1-exp3.jpg" alt></p><ul><li>在上面的实验中，作者通过改变$\alpha$值和往内容图中填充高斯噪音以获得不同效果的Stylized Images。图中第一行是改变$\alpha$值，图中第二行是往内容图田中噪音，图中第三行是两种方法的结合。可以看到，生成的Stylized Images的确取得了不同的生成效果。</li></ul><h1 id="2-Sym-Parameterized-Dynamic-Inference-for-Mixed-Domain-Image-Translation-ICCV-2019"><a href="#2-Sym-Parameterized-Dynamic-Inference-for-Mixed-Domain-Image-Translation-ICCV-2019" class="headerlink" title="2. Sym-Parameterized Dynamic Inference for Mixed-Domain Image Translation(ICCV 2019)"></a>2. Sym-Parameterized Dynamic Inference for Mixed-Domain Image Translation(ICCV 2019)</h1><blockquote><p>针对什么问题</p></blockquote><p>​    本文指出现有的图像翻译方法在没有数据集的情况下，难以学习到领域知识。比如说跨领域知识。如有晴天、阴天、雨天三种数据集，现有的方法只能生成各自领域的知识，却无法学习到混合领域的知识。</p><blockquote><p>提出什么方法</p></blockquote><p>​    本文提出将多领域的概念扩展到损失领域，并可以动态的生成混合领域的图像。说白了，就是通过在网络中加入参数，动态调节生成图像中3种领域的程度以生成混合领域的图像。</p><blockquote><p>效果怎么样</p></blockquote><p>​    生成的图像可通过参数Sym-parameters的调节，动态生成混合领域的图像。</p><h2 id="2-1-Model"><a href="#2-1-Model" class="headerlink" title="2.1 Model"></a>2.1 Model</h2><p>本文的模型如下图所示：</p><p><img src="/2020/02/15/extensive-reading2/2-model.jpg" alt></p><ul><li><p>可以看出，该模型的创新指出在于</p><ul><li>三种领域的数据集混合训练</li><li>引入Sym-parameters的CCAM模块</li></ul></li><li><p>我们先来了解以下这三种领域数据集的训练方法</p><ul><li>单独一个生成网络获得生成图像</li><li>生成图像分别和三种领域的数据集做以下损失<ul><li>Reconstruction Loss</li><li>Perceptual Loss</li><li>Adversarial Loss</li></ul></li><li>然后三种损失通过引入Sym-parameters联合起来更新整个模型</li></ul></li><li><p>然后，我们看看Sym-parameters的含义</p><ul><li>由于该模型被三种数据集以不同的损失进行训练，故可以通过加入Sym-parameters <script type="math/tex">S=(s_1, s_2, s_3)</script>同步在模型的网络和模型的损失中控制生成图像占据的领域知识比例。</li><li><script type="math/tex">s_1 + s_2 + s_3 = 1</script>，其维度和损失的个数相同。</li><li>S基于狄利克雷分布采样</li><li>模型训练完毕后，可以通过调整S来改变生成图像中的混合领域知识的程度</li></ul></li><li><p>最后，我们讲解一下CCAM模块，如下图所示</p><p><img src="/2020/02/15/extensive-reading2/2-model-ccam.jpg" alt></p><ul><li>CCAM模块接收上一层的图像特征和Sym-parameters向量作为输入</li><li>Sym-parameters通过MLP升维再和AvgPool拼接在一起，最后通过全连接层的方式转换为通道注意力权重，和上一层原始特征相乘，得到输出特征</li></ul></li></ul><h2 id="2-2-Optimization"><a href="#2-2-Optimization" class="headerlink" title="2.2. Optimization"></a>2.2. Optimization</h2><p>本文的损失很简单，如下：</p><script type="math/tex; mode=display">L_G = s_1L_{rec} + s_2L_{adv} + s_3L_{per}</script><script type="math/tex; mode=display">L_D = -s_2L_{adv}</script><p>为了保持一致性，判别器损失也要加上相应的Sym-parameter。</p><h2 id="2-3-Experiment"><a href="#2-3-Experiment" class="headerlink" title="2.3 Experiment"></a>2.3 Experiment</h2><p>本文做的实验主要体现以下几方面内容</p><ul><li>证明Sym-parameters的有效性</li><li>验证是否能够学到混合领域知识</li></ul><p>下面挑选几个实验讲解</p><p><img src="/2020/02/15/extensive-reading2/2-exp1.jpg" alt></p><ul><li>在上面的实验中，作者通过1-D的玩具模型同时训练一个MLP网络完成分类和回归的任务。左图是数据集可视化后的结果，右边三张图是在1-D玩具模型中引入Sym-parameters。通过训练单一的基于Sym-parameters的模型，以及基于Hyper-parameters的多个模型，得到以下结论：<ul><li>当Sym-parameters和Hyper-parameters一致时，模型的损失值相同，说明Sym-parameters训练的单一模型的确能够取代多个模型。</li><li>若固定损失中的Sym-parameters不变，却动态的改变模型中的Sym-parameters，会导致最后的损失值和Hyper-parameters的不一致，说明模型和损失函数中的Sym-parameters需要同步变化。</li></ul></li></ul><p><img src="/2020/02/15/extensive-reading2/2-exp2.jpg" alt></p><ul><li>在上面的实验中，作者展示了在Sym-parameter控制下，<script type="math/tex">A, B, C</script>三个领域互相转化的效果。第一行是<script type="math/tex">A\to B</script>，以此类推。最后一行是随机调整Sym-parameters的值，生成图像的情况。值得一提的是，右下角的图像将中间一项设为1.5，使得该领域知识被大大加强。</li></ul><p><img src="/2020/02/15/extensive-reading2/2-exp3.jpg" alt></p><ul><li>在上图的实验中，作者将本文提出的SGAN和其他多领域转换方法SingleGAN在Sym-parameter于<script type="math/tex">(1,0,0), (0,1,0), (0,0,1)</script>的情况下进行比较。可以看出，SingGAN的输出连原始领域的图像都无法转换，但SGAN因为有重构损失的参与，可以更多的还原原始领域的细节。</li></ul><p><img src="/2020/02/15/extensive-reading2/2-exp4.jpg" alt></p><ul><li>最后，作者还研究了Sym-parameter的不同注入方法，结果如上图所示。可以看出，CCAM方法生成的图像在风格模式和内容结构上都优于其他方法。</li></ul><p>最后，感谢前辈们的付出，Respect!</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Seeing What a GAN Cannot Generate</title>
      <link href="/2020/01/03/gan-can-not-generate/"/>
      <url>/2020/01/03/gan-can-not-generate/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1. Abstract"></a>1. Abstract</h1><ul><li>虽然GAN很成功，但是模式崩塌(Mode Collapse)的问题依然存在，而现在少有论文去理解并量化GAN到底抛弃了哪些模式。</li><li>故本文在两个级别的基础上可视化了模式崩塌：分布级(Distribution Level)和实例级(Instance Level)。<ul><li>首先，对生成数据集和目标数据集使用语义分割网络，统计两个数据集中被分割出来的物体的分布。分布的不同将会透露出GAN在生成过程中所遗漏的物体。</li><li>其次，当确定被遗忘的物体类别后，直接可视化GAN遗忘的过程。即比较特定图像的原始版本和通过GAN反演后的版本</li></ul></li><li>为了达到可视化的目的，本文放宽了反演问题，解决了GAN某一层反演的问题来替代整个GAN结构的反演。</li><li>最后，本文使用所提出的框架来分析最近的一些GAN，鉴定他们典型失败的例子。</li></ul><h1 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h1><ul><li>本文提出一个很有意思的问题：How can we know what a GAN is unable to generate?<ul><li>也即，我们怎样才能知道GAN不能生成什么。</li><li>虽然GAN很强大，但是模式崩塌(Model Collapse)和模式下降(Model Dropping)的问题依然存在，这表明GAN在生成图像的过程中遗忘了某些目标分布。</li></ul></li><li>在下面的描述中，该文章会逐渐将：GAN不能生成什么这个问题转化，然后逐渐提出自己的方法<ul><li>首先，作者表明该文章不是为了判断生成分布和真实分布之间的距离有多远，而是为了理解目标数据集中的真实图像和生成的假图像到底哪里不同。</li><li>其次，引出问题：Does a GAN deviate from the target distribution by ignoring difficult images altogether? 也即，GAN是否会去忽略比较难生成的图像，从而偏离目标分布。<strong>这里我想插一句，学习过GAN的人都应该知道，生成多样性的图像也是GAN的一大挑战。而GAN生成的图像几乎都是数据集中出现概率比较高的图像，所以特殊的复杂的图像的确比较难生成。这个问题问的…感觉挺一般。</strong></li><li>最后，再将问题转化为：how can we detect and visualize these missing concepts that a GAN does not generate? 也即，我们如何去检测和可视化这些GAN未能生成的概念呢？</li></ul></li><li>为此，作者提出在：分布级(Distribution Level)和实例级(Instance Level)上，通过分析场景生成器，来直接理解模式下降。<ul><li>首先，分割生成图像和真实数据集中的图像，比较分割出来的类别的分布。</li><li>其次，一旦被遗忘的类别找到了，生成这些被遗忘的类别特例的图像，来验证这些类别在生成的过程中是否的确会被GAN所遗忘。</li></ul></li><li>最后，本文基于自己的结论做检测，发现<ul><li>被GAN遗忘的类别并不会以扭曲形变的结果展示在生成图上，而是直接被忽略，就好像它们不是场景的一部分。</li><li>GAN的确会忽略比较难以生成的类别，同时输出高平均视觉质量的图像</li></ul></li></ul><p>本文的结论没有想象中惊艳，在不做实验得情况下，通过分析已有GAN的生成结果也可以猜测得出。所以本文的亮点不在于结论，而在于他提出的如何反演GAN的过程，也就是下面要讲的Method。</p><h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h1><p>​    根据上文，本文使用两步来可视化和理解GAN无法生成的语义概念</p><h2 id="3-1-在分布级上量化模式崩塌"><a href="#3-1-在分布级上量化模式崩塌" class="headerlink" title="3.1 在分布级上量化模式崩塌"></a>3.1 在分布级上量化模式崩塌</h2><ul><li><p>本文利用图像的场景层次结构来分析GAN的误差，举个例子：使用LSUN bedrooms数据集，GAN若是渲染卧室，也会渲染一些窗帘。若是GAN生成的图像中窗帘的统计数据与真实数据不符合，那么窗帘就是GAN的一个缺陷，即无法生成。</p></li><li><p>为此，本文使用统一的语义分割网络分割真实图像和生成图像，测量每个对象类的总面积（以像素为单位）和平均值以协方差，并图示了出现次数较高的类别，如下图所示</p><p><img src="/2020/01/03/gan-can-not-generate/3-method1.jpg" alt></p><ul><li>mean area表明的是类别的面积，红色是生成图像中类别的平均面积，蓝色是真实数据集中各个类别的平均面积。可以看到，WGAN-GP生成的图像中，某几个类别如：chest，canopy等平均面积极其稀少，说明GAN遗忘了它们。StyleGAN的效果比WGAN-GP好很多，但还是出现了遗忘的情况。</li><li>relative delta表明的是相关偏差，即比较生成图像和真实图像中类别的比例，图中的纵坐标也给出了相关的计算公式。可以看到，这里的值和mean area相互照应，部分类别的确生成的过于稀少，被GAN所遗忘</li></ul></li><li><p>同时，本文仿照<strong>FID</strong>，提出了<strong>FSD</strong>(Frechet Segmentation Distance)这一指标</p><script type="math/tex; mode=display">FSD = ||\mu_g - \mu_t||^2 + Tr(\sum_g + \sum_t - 2(\sum_g\sum_t)^{\frac{1}{2}})</script><ul><li>其中，$\mu_t$是每个物体类别的平均像素值</li><li>$\sum_t$是这些像素数量的协方差</li></ul><p>FSD被用来衡量生成图像和真实图像中分割出来的物体的差异，越低说明差异越小，越好。</p></li></ul><h2 id="3-2-在实例级上量化模式崩塌"><a href="#3-2-在实例级上量化模式崩塌" class="headerlink" title="3.2 在实例级上量化模式崩塌"></a>3.2 在实例级上量化模式崩塌</h2><p>​    虽然上述的FSD能够统计出GAN在生成的过程中，遗漏了哪些特定的类别，却没有指出：GAN在生成图像时，哪里需要物体而它并未去生成。故本文又提出了一种方法来可视化GAN在生成图像时，所遗漏的类别对象。那具体怎么做呢？下面是我在自己阅读论文并理解后的概括。</p><ul><li><p>首先，我们得把问题定义好，所谓的反演就是</p><ul><li>当我们通过分布级的统计了解到一个训练好的GAN在生成图像时，会忽略哪些类别后</li><li>我们选定几张存在这些类别的图像，并希望使用GAN生成这些图像时，这些类别是否的确被遗忘了？</li><li>一旦这些类别在生成的图像中的确不存在，那么就可以得到结论：GAN难以生成什么。</li></ul></li><li><p>但可能有人会说，让GAN生成图像不是很简单的事情吗，为何需要后面的长篇大论。这也是我一开始困惑的，当我仔细理解后豁然开朗，原因如下</p><ul><li>让GAN生成图像的确很简单，但是<strong>让GAN生成指定内容的图像很困难，因为你不知道指定内容的图像所对应的输入向量是怎么样的。</strong></li><li>再换个角度解释一下，当一个GAN训练完后，你只能随机采样噪音向量作为输入，生成随机的图像，当然这些图像理论上是符合真实数据集分布的。但是，你无法指挥GAN生成指定内容的图像，你能给的生成条件仅是作为输入的噪音向量。</li></ul></li><li><p>那么可能就会有同学会想了：我们可以通过反编码的方式得到GAN生成图像的输入向量呀，的确有论文这样去做。当GAN网络层较少的时候是行得通的，但是一旦层数变多，那么效果就很差，文中后面的实验部分也证明了这一观点。</p></li><li><p>于是，本文定义了一个易于处理的反演问题</p><ul><li><p>将生成器分解成以下形式</p><script type="math/tex; mode=display">G = G_f(g_n(...((g_1(z)))))</script><p>其中$g_1, …, g_n$是生成器的前几层，$G_f$是生成器后面所有层的集合</p></li><li><p>作者指出，任何G能够生成的图像，$G_f$也能够生成，并做了如下定义</p><script type="math/tex; mode=display">range(G) \subset range(G_f)</script><p>也就是说，G能生成的图像是$G_f$生成图像的子集。若是$G_f$难以生成的图像，G同样也生成不了。(为什么？可能是因为$G_f$包含了更多的变化，输入向量的维度远超$z$)</p></li><li><p>所以问题就变成反演$G_f$，而不是$G$</p><script type="math/tex; mode=display">x^{'} = G_f(r^*),\\ where \quad r* = \underset{r}{argmin}L(G_f(r), x).</script></li><li><p>即最终不是寻找输入向量$z$，而是一个中间表示$r$来作为$G_f$的输入，得到指定内容图像的重构</p></li></ul></li><li><p>围绕着上面的定义，作者提出了自己的方法，一共分为三步，如下图所示</p><p><img src="/2020/01/03/gan-can-not-generate/3-method2.jpg" alt></p><ul><li><p>第一步，训练一个编码器E，使得$E(G(z)) \rightarrow z$。那么同学们可能就有疑惑了，不是说直接获得$z$很难吗，那为什么还要得到$z$，岂不画蛇添足。文中是这样解释的，虽然作者的目的是找到一个中间表示$r$来作为$G_f$的输入，但是一个较好的初始$z$将会对寻找$r$有很大的帮助，即得到一个更有和$z$相关的$r$。这一块的关键是如何训练编码器</p><ul><li><p>编码器的训练不是采用端到端的反向传播方式，而是$layer-wise$的方法。专门有相关的论文研究该方法，简单来说就是独立每一层网络的训练，而不是一起训练。</p></li><li><p>为此，作者对应着$g_i \in \left\{ g_1, g_2, …, g_n, G_f \right\}$将编码器$E$分解为$\left\{ e_1, e_2, … \right\}$，并定义了如下表示</p><script type="math/tex; mode=display">r_i = g_i(r_{i - 1}), \\ r_{i-1} = e_i(r_i)</script><p>即$e_i$能够还原$g_i$生成的特征，为了使得$e_i$较好的保留$g_i$的输出特征，又定义</p><script type="math/tex; mode=display">r_i \approx g_i(e_i(r_i))</script></li><li><p>训练$e_i$的损失函数如下所示，能够保证双向输出的特征被很好的保留</p><script type="math/tex; mode=display">L_L \equiv E_z[||r_{i-1} - e(g_i(r_{i-1}))||_1],\\ L_R \equiv E_z[||r_i - g_i(e(r_i))||_1], \\ e_i = \underset{e}{argmin}\quad L_L + \lambda_RL_R</script><p>其中$||.||<em>1$是$L_1$损失，$\lambda_R = 0.01$来强调$r</em>{i - 1}$的重构能力。</p></li></ul><p>那么，编码器$E$就可以表示为</p><script type="math/tex; mode=display">E^* = e_1(e_2(\dots(e_n(e_f(x))))).</script></li><li><p>第二步，在得到$z_0$之后，就可以得到初始化的$r_0 = g_n(…(g_1(z_0)))$。</p></li><li><p>第三步，不断的更新$r$找到最适合的那个$r^*$，最后输入到GAN的$G_f$部分中来恢复对指定生成图像的还原。</p><ul><li><p>寻找到$r^*$的方式是，对生成器早些层不断地做扰动，学习扰动参数$\delta$，如下所示</p><script type="math/tex; mode=display">z_0 \equiv E(x), \\ r \equiv \delta_n + g_n(\dots(\delta_2 + g_2(\delta_1 + g_1(z_0)))),\\ r^* = \underset{r}{argmin}(l(x, G_f(r)) + \lambda_{reg}\underset{i}{\sum}||\delta_i||^2)</script></li><li><p>其中，$l$是基于$VGG$的Perceptual  Loss。</p></li><li><p>根据文中描述，需要训练的参数就是$\delta$。</p></li></ul></li></ul></li></ul><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p>在讲解实验之前，说一下实验得配置</p><ul><li>作者在三个GAN模型上验证了自己的方法，分别是：WGAN-GP，Progressive GAN以及StyleGAN。</li><li>数据集是:LSUN bedroom images</li><li>使用：Unified Perceptual Parsing network 来分割图像</li></ul><h2 id="4-1-生成图的分割统计"><a href="#4-1-生成图的分割统计" class="headerlink" title="4.1 生成图的分割统计"></a>4.1 生成图的分割统计</h2><p>这个在前文的方法中讲过了，作者在实验中直接使用上文提出的$FSD$来衡量真实数据集和生成数据集类别统计的差异，如下表所示：</p><p><img src="/2020/01/03/gan-can-not-generate/4-exp1.jpg" alt></p><h2 id="4-2-敏感度测试"><a href="#4-2-敏感度测试" class="headerlink" title="4.2 敏感度测试"></a>4.2 敏感度测试</h2><p>这个敏感度测试，应该是为了验证自己的统计分布的方法并只针对整个数据集有意义，随机采样数据并统计信息也会得到类似的结果。表明了自己方法的泛化性。</p><h2 id="4-3-确认抛弃的模式"><a href="#4-3-确认抛弃的模式" class="headerlink" title="4.3 确认抛弃的模式"></a>4.3 确认抛弃的模式</h2><p>根据分布级统计到的数据，GAN总是选择性跳过比较困难的子任务，如在生成分布中，人出现的概率很低。通过反演的方法得到的包含被遗漏对象的图像的重构图，也都表明了模型难以生成这些对象。也就是说，GAN并不是以低质量的结果展示这些类别，而是完全遗忘了这些类别，所以这些类别在生成的图像中完全不存在。</p><h2 id="4-4-比较Layer-wise和其他方法"><a href="#4-4-比较Layer-wise和其他方法" class="headerlink" title="4.4 比较Layer-wise和其他方法"></a>4.4 比较Layer-wise和其他方法</h2><p>这是文中比较大型的实验，主要通过和其他方法对比，以及消融实验，来阐明自己设计的方法是比较合理有效的。整个实验效果图图下，</p><p><img src="/2020/01/03/gan-can-not-generate/4-exp2.jpg" alt></p><p>图中的前三列是对比试验，后三列是消融实验。第一行是重构图；由于是对GAN的生成图进行重构，存在$z$的ground-truth，所以第一行是$z$的协相关系数以及图示；第二行是第四层特征的协相关系数；第三行是整个生成图像的协相关系数。下面将对每一列进行详细的解释</p><ul><li>(a)中的方法通过梯度下降优化$z$来最小化重构损失，适用于网络层数较少的生成器，一旦用于层数较多的如15层的Progressive GAN，效果就很差。</li><li>(b)中构造了编码器E来反演生成器但是没有使用layer-wise的优化方法，虽然比起(a)好了很多，但是效果还是很差。</li><li>(c)中先通过方法(b)来初始化$z$，再通过方法(a)对$z$进行梯度更新，效果是好了很多，但是重构的结果仍然不如人意。</li><li>(d)是消融实验，它先通过layer-wise的优化方法得到编码器E，得到对于$z$的初始猜测$z_0 = E(x)$；再直接通过$G(z_0)$的方式得到重构后的图像$x^{‘}$，而未去学习扰动参数$\delta$来得到中间层$r$。实验效果表明，通过layer-wise得到的$z_0$是比较好的，生成的图像比较令人满意，这证明了layer-wise的有效性。</li><li>(e)的实验是在(d)之后，对$z$继续优化来最小化图像的重构损失。虽然定性的结果，重建的图很好，但是$z$的协相关系数很差，这导致我们难以判断重构的错误出在哪里。</li><li>(f)是完整的一套方法，可以接近完美的重建图像，而且各个协相关系数也都很好。所以一旦重建的图像缺斤少两了，并不是重建效果的不好，而是GAN本身问题，是GAN遗漏了那些类别物体。</li></ul><p>除了使用生成的图像来做实验，本文还采用了真实图像来测试，以获得定性的结果，如下图所示：</p><p><img src="/2020/01/03/gan-can-not-generate/4-exp3.jpg" alt></p><p>当然，结果也可以证明，本文提出的方法在协相关系数上最高，也就是说重构图最贴近原图。</p><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h1><p>最后，我总结以下本文的主要内容</p><ul><li>首先，作者提出问题：GAN存在model dropping的问题，那么GAN到底难以生成哪些东西呢？或者说，GAN到底生成不了什么。</li><li>为此，作者从分布级和实例级对这个问题进行了回答<ul><li>在分布级上，作者对生成图像和真实图像进行实力分割，统计各类物体的总面积以及相关系数，找到了生成图像中出现次数较少的类别</li><li>在实例级上，由于找到了出现次数较少的类别，可否再通过GAN重构某些存在这个类别物体的图像，来判断GAN是否遗漏了这些目标</li></ul></li><li>故本文提出了基于layer-wise的反演方法，并通过一系列实验证明了其有效性，这也是本文的两点，特别是使用协相关系数。</li><li>但文中的结论过于简单，那就是：当类别太难了，GAN就不生成了。我觉得是否可以将其归结于多样性的问题，比如数据集中床和窗帘比较多，而人比较少，所以生成的人的概率就小了</li></ul><p>最后，感谢本文作者的贡献，respect! </p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文泛读1：基于Disentangle的Image-to-Image Translation</title>
      <link href="/2019/12/11/extensive-reading1/"/>
      <url>/2019/12/11/extensive-reading1/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Unsupervised-Image-to-Image-Translation-Networks"><a href="#1-Unsupervised-Image-to-Image-Translation-Networks" class="headerlink" title="1. Unsupervised Image-to-Image Translation Networks"></a>1. Unsupervised Image-to-Image Translation Networks</h1><blockquote><p>针对什么问题？</p></blockquote><p>图像翻译旨在将图像从一个领域映射到另一个领域。在监督的设定下，数据集是成对的，领域之间的映射是可行的。但在非监督的情况下，即数据集不是成对的，而是分成两个彼此独立的领域，图像翻译的任务将会变得艰难。而现实生活中，成对的数据是很难获得的，故无监督的图像翻译更实在。</p><blockquote><p>提出什么方法？</p></blockquote><p>本文指出<strong>图像翻译的主要挑战是学习两个不同领域之间的联合分布</strong>。在无监督条件下，两个数据集合分别包括在不同领域中来自两个边缘分布的图像，而图像翻译的目的正是从这两个边缘分布得到联合分布。而耦合理论指出：<strong>一般情况下，存在无穷多的联合分布可以到达给定的边缘分布</strong>。因此，从边缘分布推断联合分布是一个不适合的方法。为了解决这个问题，本文对联合分布提出了额外的假设。</p><ul><li>本文提出一个共享隐藏空间的假设，即不同领域中的一对相关图像可以被映射到共享隐藏空间中的相同隐藏表达。</li><li>基于上述假设，本文提出了结合了GAN和VAE的UNIT框架</li></ul><blockquote><p>效果怎么样？</p></blockquote><p>本文将提出的框架应用于多种无监督的图像翻译问题并取得了高质量的翻译结果，这也从侧面反映了共享隐藏空间暗含了Cycle-Consistency Constraint（循环一致损失）。</p><h2 id="1-1-Model"><a href="#1-1-Model" class="headerlink" title="1.1 Model"></a>1.1 Model</h2><p>本文提出的UNIT框架如下图所示：</p><p><img src="/2019/12/11/extensive-reading1/1-model.jpg" alt></p><ul><li>其中，(a)图示了隐藏空间的含义，即Domain$X_1, X_2$中的图像$x_1, x_2$都被映射到了相同的隐藏空间$z$中</li><li>(b)图示了整个框架的结构<ul><li>Domain$X_1, X_2$中的图像$x_1, x_2$分别通过$E_1, E_2$映射到隐藏空间$z$中，再分别通过$G_1, G_2$还原成自我重构图像$\tilde{x}_1^{1 \rightarrow 1}, \tilde{x}_2^{2 \rightarrow 2}$或者领域翻译后的图像$\tilde{x}_1^{1 \rightarrow 2}, \tilde{x}_2^{2 \rightarrow 1}$。<ul><li>$E_1, E_2$共享了最后几层网络，$G_1, G_2$共享了前面几层网络</li><li>$\left\{ E_1, G_1 \right\}$, $\left\{ E_2, G_2 \right\}$对各自的领域$X_1, X_2$都构成了$VAE$的结构</li></ul></li><li>判别器$D_1, D_2$负责验证翻译后的图像是否真实</li></ul></li><li>本文指出，权重共享的约束并不能保证两个领域中相关的图像能够得到相同的隐藏空间编码。因为在无监督条件下，这两个领域中没有对应的图像能够训练网络输出相同的隐藏编码。即使隐藏编码是相同的，它们在不同的领域将会具有不同的语义意义。但话又说回来，通过对抗损失的训练，两个领域中对应的图像可以被映射到相同的隐藏空间，并可以被反射回不同的领域。</li></ul><h2 id="1-2-Optimization"><a href="#1-2-Optimization" class="headerlink" title="1.2 Optimization"></a>1.2 Optimization</h2><p>本文通过联合寻来你解决了$VAE_1, VAE_2, GAN_1, GAN_2$的学习问题，损失如下</p><script type="math/tex; mode=display">\underset{E_1, E_2, G_1, G_2}{min} \underset{D_1}{max}L_{VAE_1}(E_1, G_1) + L_{GAN_1}(E_1, G_1, D_1) + L_{cc_1}(E_1, G_1, E_2, G_2)</script><script type="math/tex; mode=display">\underset{E_1, E_2, G_1, G_2}{min} \underset{D_2}{max}L_{VAE_2}(E_2, G_2) + L_{GAN_2}(E_2, G_2，D_2) + L_{cc_2}(E_2, G_2, E_1, G_1)</script><ul><li>其中$L_{VAE}$旨在最小化变量的上界</li><li>$L_{GAN}$确保了翻译后图像的真实性</li><li>$L_{CC}$类似于循环一致性约束，确保了两次翻译后的图像与输入图像一致</li></ul><p>以上损失的具体形式请参考原论文</p><h2 id="1-3-Experiment"><a href="#1-3-Experiment" class="headerlink" title="1.3 Experiment"></a>1.3 Experiment</h2><ul><li>验证指标<ul><li>average pixel accuracy over the images in the test set</li><li>…</li></ul></li><li>对比模型<ul><li>DANN</li><li>DTN</li><li>CoGAN</li></ul></li><li>数据集<ul><li>map dataset</li><li>Synthetic to real</li><li>Dog breed conversion</li><li>…</li></ul></li></ul><p>下面展示本文部分的实验结果</p><p><img src="/2019/12/11/extensive-reading1/1-exp1.jpg" alt></p><p><img src="/2019/12/11/extensive-reading1/1-exp2.jpg" alt></p><h1 id="2-Multimodal-Unsupervised-Image-to-Image-Translation"><a href="#2-Multimodal-Unsupervised-Image-to-Image-Translation" class="headerlink" title="2. Multimodal Unsupervised Image-to-Image Translation"></a>2. Multimodal Unsupervised Image-to-Image Translation</h1><blockquote><p>针对什么问题？</p></blockquote><p>本文指出，在许多场景中，对于交叉领域的映射是多模态的，而现有的Image-to-Image Translation方法，如Pix2Pix, CycleGAN经仅仅只能得到一个确定性的映射。这是什么意思呢？</p><ul><li>举个例子，一个冬天的场景可能会有多个状态，天气、时间、光线等因素都会使得冬天的场景看起来不一样。</li><li>而现有的方法，若是做 夏天-&gt;冬天，一张夏天的场景仅能转换出到一种冬天领域下的场景，无法做到多种不同的输出。</li><li>特别是，本文指出，有些方法<strong>通过注入噪音完成图像中信息的改变，但是经过训练后的网络会忽略这些噪音，从而使得噪音的注入无效。</strong></li></ul><blockquote><p>提出什么方法？</p></blockquote><p>故本文针对以上问题，提出了<strong>MUNIT</strong>模型，并做了以下的假设</p><ul><li><p>首先，假设图像的潜在空间(Latent Space)可以被分解为<strong>内容空间</strong>(Content Space)和<strong>风格空间</strong>(Style Space)。</p><ul><li>内容空间编码了在图像翻译时需要保留的内容</li><li>风格空间代表了剩余那些输入图片中不需要保留的变量</li></ul></li><li><p>其次，假设处于不同领域中的图像共享一个共同的内容空间，但是不共享风格空间。如下图所示，其中$X_1, X_2$共享同一个内容空间，但是风格空间彼此独立。</p><p><img src="/2019/12/11/extensive-reading1/2-explain.jpg" alt></p></li></ul><p>那么，在进行领域转换的时候，将内容编码和不同的随机采样的风格编码相结合，就可以得到多样化和多模态的输出。</p><blockquote><p>效果怎么样?</p></blockquote><p>实验表明该方法在多模态输出建模中的有效性，且能生成质量更好的图像。</p><h2 id="2-1-Model"><a href="#2-1-Model" class="headerlink" title="2.1 Model"></a>2.1 Model</h2><p><img src="/2019/12/11/extensive-reading1/2-model.jpg" alt></p><p>上图为模型的概述。整个模型包含了两个<strong>auto-encoders</strong>，也就是模型中的红线和蓝线，各自代表一个domain。</p><ul><li>在图(a)中，两个auto-encoders的作用如下<ul><li>对于$x_1$，针对domain1的auto-encoders将其分解成风格空间$s_1$和内容空间$c_1$，再通过$L_1$损失重构成原图$\hat{x_1}$</li><li>对于$x_2$，针对domain2的auto-encoders将其分解成风格空间$s_2$和内容空间$c_2$，再通过$L_1$损失重构成原图$\hat{x_2}$</li></ul></li><li>在图(b)中，对于拆分后的$c_1, c_2$操作如下<ul><li>通过采样获取风格编码$x_1$, 将其和$c_2$一起输入到domain1的auto-encoders中，使其能够重构为$\hat{s_1},\hat{c_2}$。</li><li>通过采样获取风格编码$s_2$, 将其和$c_1$一起输入到domain2的auto-encoders中，使其能够重构为$\hat{s_2},\hat{c_1}$。</li></ul></li></ul><p>其中，auto-encoders的构造如下图</p><p><img src="/2019/12/11/extensive-reading1/2-autoencoder.jpg" alt></p><ul><li>可以看到，内容编码和风格编码都有各自的Encoder才获得<ul><li>对于Content Encoder，它由几个跨步卷积紧跟着几个残差块获得</li><li>对于Style Encoder，它由几个跨步卷积紧跟着几个全局的池化，并接上了全连接层</li></ul></li><li>之后，作者通过使用AdaIN方法将Content Code和Style Code结合了在一起，具体操作如下<ul><li>对于Content Code，它继续接几个残差块来不断地传播语义特征</li><li>对于Style Code，它通过MLP获得AdaIN的参数，在Content Code传播的过程中结合AdaIN参数</li></ul></li><li>最后，再通过上采样获得最后的重构图像</li></ul><p>其中，AdaIN的公式如下</p><script type="math/tex; mode=display">AdaIN(z, \gamma, \beta) = \gamma(\frac{z - \mu(z)}{\sigma(z)}) + \beta</script><ul><li>z代表卷积后输出的激活值</li><li>$\mu$代表通道均值</li><li>$\sigma$代表通道标准差</li><li>$\gamma, \beta$代表MLP生成的参数</li></ul><h2 id="2-2-Optimization"><a href="#2-2-Optimization" class="headerlink" title="2.2 Optimization"></a>2.2 Optimization</h2><ul><li><p>首先，是图像重构损失(Image Reconstruction)，如下</p><script type="math/tex; mode=display">L_{recon}^{x_1} = E_{x_1 \thicksim p(x_1)}[||G_1(E_1^c(x_1), E_1^s(x_1)) - x_1||_1]</script><ul><li>即从Domain1中采样数据$x_1$，通过Domain1的Auto-Encoders来提取内容空间和风格空间，再通过Domain1的$G_1$转换为重构后的图像，和原始图像$x_1$做$L_1$损失。</li><li>同理，可以推出$L_{recon}^{x_2}$损失。</li></ul></li><li><p>其次，是隐藏重构损失（Latent Reconstruction）</p><script type="math/tex; mode=display">L_{recon}^{c_1} = E_{c_1 \thicksim p(c_1),s_2 \thicksim q(s_2)}[||E_2^c(G_2(c_1, s_2)) - c_1||]</script><script type="math/tex; mode=display">L_{recon}^{s_2} = E_{c_1 \thicksim p(c_1),s_2 \thicksim q(s_2)}[||E_2^c(G_2(c_1, s_2)) - s_2||]</script><ul><li>其中$q(s_2)$是先验分布$N(0, I)$中采样的，$p(c_1)$由$c_1 = E_1^c(x_1)$得到，$x_1 \thicksim p(x_1)$。</li><li>$L_{recon}^{c_1}$通过$G_2$将$c_1, s_2$转化为Domain2中的图像，并通过Domain2中的内容解码器抽取内容空间，得到重构后的$c_1$的内容空间和原$c_1$做$L_1$损失</li><li>$L_{recon}^{s_2}$通过$G_2$将$c_1, s_2$转化为Domain2中的图像，并通过Domain2中的风格解码器抽取风格空间，得到重构后的风格空间和原$s_2$做$L_1$损失</li><li>同理，可以推出<script type="math/tex">L_{recon}^{x_2}, L_{recon}^{c_2}, L_{recon}^{s_1}</script>。</li></ul></li><li><p>最后，是对抗损失(Adversarial Loss)</p><script type="math/tex; mode=display">L_{GAN}^{x_2} = E_{c_1 \thicksim p(c_1), s_2 \thicksim p(s_2)}[log(1 - D_2(G_2(c_1, s_2)))] + E_{x_2 \thicksim p(x_2)}[logD_2(x_2)]</script><ul><li>判别器$D_2$尝试区分由$x_1$翻译到$x_2$中的图像与$X_2$中的真实图像</li><li>同理，可以推出$D<em>1$的损失$$L</em>{GAN}^{x_1}$$</li></ul></li></ul><p>根据以上的单一损失描述，可以得到本文的总损失如下：</p><script type="math/tex; mode=display">\underset{E_1, E_2, G_1, G_2}{min} \underset{D_1, D_2}{max}L(E_1, E_2, G_1, G_2, D_1, D_2) = L_{GAN}^{x_1} + L_{GAN}^{x_2} + \\ \lambda_x(L_{recon}^{x_1} + L_{recon}^{x_2}) + \lambda_c(L{recon}^{c_1} + L_{recon}^{c_2}) + \lambda_s(L_{recon}^{s_1} + L_{recon}^{s_2})</script><p>其中，$\lambda_x, \lambda_c, \lambda_s$都是超参数。</p><h2 id="2-3-Experiment"><a href="#2-3-Experiment" class="headerlink" title="2.3 Experiment"></a>2.3 Experiment</h2><ul><li>验证指标<ul><li>Human Preference，即人们对图像的欣赏度</li><li>LPIPS Distance，验证翻译后的多样性</li><li>Inception Score，验证多模态翻译后的真实度</li></ul></li><li>对比模型<ul><li>UNIT</li><li>CycleGAN</li><li>CycleGAN*</li><li>BicycleGAN</li></ul></li><li>数据集<ul><li>Edges &lt;-&gt; Shoes/handbags</li><li>Animal Image Translation</li><li>Street Scene Images</li><li>Summer &lt;-&gt; Winter</li></ul></li></ul><p>下面是挑选的实验中的几张效果图，总之就是比其他方法真实，且多样性好</p><p><img src="/2019/12/11/extensive-reading1/2-exp1.jpg" alt></p><p><img src="/2019/12/11/extensive-reading1/2-exp2.jpg" alt></p><p><img src="/2019/12/11/extensive-reading1/2-exp4.jpg" alt></p><h1 id="3-Diverse-Image-to-Image-Translation-via-Disentangled-Representations"><a href="#3-Diverse-Image-to-Image-Translation-via-Disentangled-Representations" class="headerlink" title="3. Diverse Image-to-Image Translation via Disentangled Representations"></a>3. Diverse Image-to-Image Translation via Disentangled Representations</h1><p>在写这篇论文泛读之前想说的话：<strong>本文是本次所有泛读文章中，文笔最好，实验最多的；若是对Image-to-Image Translation中Disentangle做法感兴趣，可以先从这篇读起。</strong></p><blockquote><p>针对什么问题？</p></blockquote><p>本文指出Image-to-Image Translation旨在学习两个视觉域中的映射关系，但存在以下两个挑战</p><ul><li>缺乏对齐的训练数据</li><li>输入单一的图像，缺乏多样化的输出结果</li></ul><blockquote><p>提出什么方法？</p></blockquote><p>为了生成多样化的输出且不依靠对齐的训练数据，本文将图像嵌入到两个空间中</p><ul><li>领域不变的内容空间(Domain-Invariant Content Space)，来捕捉共享信息</li><li>领域专有的属性空间(Domain-Specific Attribute Space)</li></ul><p>如下图所示</p><p><img src="/2019/12/11/extensive-reading1/3-method.jpg" alt></p><ul><li>其中，CycleGAN将$x, y$映射到分离的隐层空间</li><li>UNIT将$x, y$映射到共享的隐层空间</li><li>MUNIT和DRIT将$x, y$分别映射到共享的内容空间，和专有的属性空间</li></ul><blockquote><p>效果怎么样？</p></blockquote><p>大量的实验表明本文提出的方法能够生成多样化和真实的图像，并且能够被运用于多领域的图像翻译来生成多样性的输出。</p><h2 id="3-1-Model"><a href="#3-1-Model" class="headerlink" title="3.1 Model"></a>3.1 Model</h2><p>本文的目标是在没有成对训练数据的情况下，学习两个视觉领域之间的多模态映射。其中，本文完整的模型如下图所示</p><p><img src="/2019/12/11/extensive-reading1/3-model.jpg" alt></p><ul><li><p>针对领域$x, y$，存在各自的内容编码器$E_x^c, E_y^c$和属性编码器$E_x^a, E_y^a$，以及各自的解码器$G_x, G_y$。</p></li><li><p>在训练过程中，我们以领域$x$中的图像为例。第一步，对于领域$x$中的图像使用$E_x^a$抽取专有属性，再使用$E_y^c$抽取领域$y$中的内容信息，并通过领域$x$的解码器$G_x$将映射到领域$x$中。第二步，使用$E_x^a, E_x^c$抽取映射后图像的内容信息和专有属性，再使用$E_y^c$抽取领域$y$中的内容信息，最后通过领域$x$的解码器$G_x$将其映射回领域$x$，即得到重构后的$\hat{x}$。领域$y$中的图像同理。</p></li><li><p>在测试过程中，分为两种情况</p><ul><li>一是使用随机属性进行测试。输入图像$x$，使用$E_x^c$抽取图像内容，并采样属性信息，通过$G_y$将其映射到领域$y$中。</li><li>二是使用给定的图像抽取专有属性。输入图像$x$，使用$E_x^c$抽取领域$x$中的图像信息，再使用$E_y^a$抽取领域$y$中的图像专有属性，最后通过解码器$G_y$生成领域$y$中的图像。</li></ul></li><li><p>为了取得更好的分解效果，本文还采用了两个策略</p><ul><li><p>参数共享(Weight-Sharing)。基于两个领域共享相同的隐层空间，本文共享了$E_X^c, E_Y^c$的最后几层和$G_X, G_Y$的第一层。在参数共享的策略下，内容表达能够被强制映射到同一个空间，即使得判别器无法判断学习到的内容表达到底属于领域$x$还是领域$y$。</p></li><li><p>内容判别器(Content Discriminator)。内容的对抗损失如下</p><script type="math/tex; mode=display">L_{adv}^{content}(E_X^c, E_Y^c, D^c) = E_x[\frac{1}{2}logD^c(E_X^c(x)) + \frac{1}{2}log(1 - D^c(E^c_X(x))) + \\ E_y[\frac{1}{2}logD^c(E_Y^c(y)) + \frac{1}{2}log(1 - D^c(E^c_Y(y)))]</script></li></ul></li></ul><p>本文除了双领域之间的转换，还尝试了多领域之间的转化，我们这里也提一下，模型如下图</p><p><img src="/2019/12/11/extensive-reading1/3-multiI2I.jpg" alt></p><p>模型和双领域转换类型，存在两方面的不同</p><ul><li>多了领域编码(Domain Code)，其实就是$One-hot$向量，领域编码将会在$E_a$抽取领域的专有属性时嵌入到特征里。</li><li>内容判别器不仅需要判别图像是否真实，还需要判别来自哪一个领域，应该就是条件GAN判别器的做法，输入分为图像内容和条件。</li></ul><h2 id="3-2-Optimization"><a href="#3-2-Optimization" class="headerlink" title="3.2 Optimization"></a>3.2 Optimization</h2><p>除了上述的内容判别器损失，本文还使用了交叉循环一致性损失(Cross-cycle Consistency Loss)，使得模型可以将任意图像的内容和目标领域中另一个图像的属性表达结合在一起。</p><ul><li><p>交叉循环一致性损失分为两个步骤：前向翻译和后向翻译，也就是上文模型图的内容，即映射和重构。损失如下</p><script type="math/tex; mode=display">L_1^{cc}(G_X, G_Y, E_X^c, E_Y^c, E_X^a, E_Y^a) = E_{x,y}[||G_X(E_Y^c(v), E_X^a(u)) - x||_1 + \\ ||G_Y(E_X^c(u), E_Y^a(v)) - y||_1]</script></li></ul><p>除了内容对抗损失和交叉循环一致性损失，本文还使用了其他损失来确保网络的训练，如下图所示</p><p><img src="/2019/12/11/extensive-reading1/3-loss.jpg" alt></p><ul><li>$L_{1}^{recon}$在训练中完成自我重构</li><li>$L_{KL}$从先验高斯分布中采样，对齐属性的表达</li><li>$L_{adv}^{domain}$鼓励生成器G在每个领域生成真实的图像</li><li>$L_1^{latent}$对隐藏属性进行重构</li><li>$L_{ms}$进一步提升多样性</li></ul><p>损失的详情请看原文。</p><h2 id="3-3-Experiment"><a href="#3-3-Experiment" class="headerlink" title="3.3 Experiment"></a>3.3 Experiment</h2><ul><li>验证指标<ul><li>FID</li><li>LPIPS</li><li>JSD and NDB</li><li>User Performance</li></ul></li><li>对比模型<ul><li>DRIT</li><li>MUNIT</li><li>Cycle/Bicycle GAN</li></ul></li><li>数据集<ul><li>Winter -&gt; Summer</li><li>Cat -&gt; Dog</li></ul></li></ul><p>本文的实验极其丰富，此外还尝试了生成高分辨率的图像，建议大家阅读原文，我下面就稍微贴几张。</p><p><img src="/2019/12/11/extensive-reading1/3-exp1.jpg" alt></p><p><img src="/2019/12/11/extensive-reading1/3-exp2.jpg" alt></p><p><img src="/2019/12/11/extensive-reading1/3-exp3.jpg" alt></p><h1 id="4-Image-to-Image-Translation-for-Cross-Domain-Disentanglement"><a href="#4-Image-to-Image-Translation-for-Cross-Domain-Disentanglement" class="headerlink" title="4. Image-to-Image-Translation-for-Cross-Domain-Disentanglement"></a>4. Image-to-Image-Translation-for-Cross-Domain-Disentanglement</h1><p>本文的主要思想和上述论文类似，主要不同点在于引入了$GRL$模块使得编码器能分别提取共享信息和专有属性。</p><blockquote><p>针对什么问题？</p></blockquote><p>从场景的固有属性出发，将光照、阴影、视点、物体方向等场景事件分离出来，一直是计算机视觉长期追求的目标。当应用于深度学习时，这允许模型了解实体独立的变量因素，若信息与当前任务无关，那么模型可以沿着特定的变化因素边缘化信息。这样的分解过程对于基于表示学习的任务提供了更加精准的控制。故本文旨在将分解表示和图像翻译结合在一起，以达到更好的效果。</p><blockquote><p>提出什么方法？</p></blockquote><p>本文将分解目标和图像翻译结合，引入了跨域分解的概念，其目的是将域内专有的属性和跨域共享的属性分开。为了做到这一点，本文将交叉领域中图像的表示分解为三部分：交叉领域共有的信息，以及各自领域专有的信息，如下图</p><p><img src="/2019/12/11/extensive-reading1/4-concept.jpg" alt></p><ul><li>图中两个数字领域共享的是没有颜色信息的数字</li><li>专有的信息是，数字的背景信息等</li></ul><blockquote><p>效果怎么样？</p></blockquote><ul><li>多样性样本(Sample diversity)。可以基于给定的输入图片输出多样化的结果</li><li>跨域检索(Cross Domain Retrieval)。可以根据域之间共享的表示部分在两个域中检索相似的图像</li><li>专有域的图像迁移(Domain-specific Image Transfer)。领域专有的特征可以在图像间传输</li><li>专有域的插值(Domain-specific Interpolation)。可以在两个图像间插入领域专有的特征。</li></ul><h2 id="4-1-Model"><a href="#4-1-Model" class="headerlink" title="4.1 Model"></a>4.1 Model</h2><p>本文的模型如下图，左边是图像翻译模块，右边是跨域自动编码器。</p><p><img src="/2019/12/11/extensive-reading1/4-model.jpg" alt></p><ul><li>首先要分清楚图中各模块的作用，本文对模块的命名不是很友好，建议先看右边的跨域编码器。我们以右图中对于领域$X$的操作为例，领域$X$的编码器$G_e$将图像分解为共享的信息部分$S^x$和专有的属性部分$E^x$；再将$E^x$和来自领域$Y$中由编码器$F_e$抽取的图像的共享信息$E^y$相结合，输入到领域$X$的解码器$F_d$，可以得到重构后的图像$x$。同理对于领域$Y$的操作。</li><li>我们再看图中左边的图像翻译。我们以领域$X$操作为例，依然通过$G_e$提取共享信息$S_x$以及专有特征$E^x$。<ul><li>首先讲一下GRL操作。在黄色虚线那部分，作者认为由于$E^x$中体现的是专有属性，必不能包含领域$Y$中的信息，所以仅仅使用$E^x$不可能生成领域$Y$中的图像。为了强化这种认知，作者用了我看起来很“奇葩”的行为，他尝试使用$E^x$生成领域$Y$中的图像，却有积极的引导特征学习来防止这种情况发生。故作者专门设置了一个GRL(Gradient Reversal Layer)模块来学习与域无关的特性。在网络的前向传播过程中，它作为指示函数；在反向传播的时候，返回相应分支的梯度。</li><li>剩下的就是常规操作，将$S^x$和采样得到的属性结合，并通过解码器$G_d$将$x$翻译到领域$Y$中，再通过领域$Y$的编码器提取共享信息和专有属性。</li></ul></li></ul><h2 id="4-2-Optimization"><a href="#4-2-Optimization" class="headerlink" title="4.2 Optimization"></a>4.2 Optimization</h2><ul><li><p>重构隐层空间。用于翻译的解码器接收共享的表示$S$以及用于充当专有属性的随机输入噪音作为输入，在这里本文从8维的噪音向量$z$中采样，这也要求专有属性表示的能够满足合理的分布。为此，作者在这里加入了原始对抗损失，训练使得能从$N(0, I)$中采样接近于$E^x$分布的特征。</p></li><li><p>重构后的图像必须与解码器的输入匹配，因此需要在原图像和重构后的图像$S^X$中加入$L_1$损失。</p><script type="math/tex; mode=display">L_{recon}^X = E_{x \thicksim X}[||G_e(G_d(S^X, z)) - (S^X, z)||]</script></li><li><p>为了使得翻译后的图像接近于真实数据分布，还需要引入对抗损失，本文作者采用了$WGAN-GP$的损失，以获得稳定的训练和高质量的输出</p><script type="math/tex; mode=display">L_{Disc}^X = E_{\tilde{x} \thicksim \tilde{X}}[D(\tilde{x})] - E_{x \thicksim X}[D(x)] + \lambda \cdot E_{\hat{x} \thicksim \hat{X}}[(||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2]</script><script type="math/tex; mode=display">L_{Gen}^X = -E_{\tilde{x} \thicksim \tilde{X}}[D(\tilde{x})]</script></li><li><p>最后在跨域自动编码器中，重建的图像需要接近于原图，故有以下损失</p><script type="math/tex; mode=display">L_{auto}^X = E_{x \thicksim X}[||x^{'} - x||]</script></li></ul><p>损失的详情请查看原文</p><h2 id="4-3-Experiment"><a href="#4-3-Experiment" class="headerlink" title="4.3 Experiment"></a>4.3 Experiment</h2><ul><li>验证指标<ul><li>引入某篇论文中相关的读量协议</li></ul></li><li>对比方法<ul><li>BicycleGAN</li><li>Pix2Pix</li></ul></li><li>数据集<ul><li>MNIST-CD</li><li>MNIST-CB</li><li>3D car models </li><li>3D chair models</li></ul></li></ul><p>最后，给一些实验截图，有兴趣的朋友可自行阅读原文。</p><p><img src="/2019/12/11/extensive-reading1/4-exp.jpg" alt></p><h1 id="5-Conditional-Image-to-Image-translation"><a href="#5-Conditional-Image-to-Image-translation" class="headerlink" title="5. Conditional Image-to-Image translation"></a>5. Conditional Image-to-Image translation</h1><blockquote><p>针对什么问题？</p></blockquote><p>现有的图像翻译方法缺乏控制翻译结果的能力，从而导致生成的结果缺乏多样性。</p><blockquote><p>提出什么方法？</p></blockquote><p>本文提出了条件图像翻译的概念，即可以在给定的图像条件下，将源于转换为目标域中的图像，如下图示</p><p><img src="/2019/12/11/extensive-reading1/5-concept.jpg" alt></p><ul><li>图a以人脸作为条件，生成指定领域的人脸</li><li>图b以背包作为条件，生成指定领域的背包</li></ul><p>该模型要求生成的图像必须从目标域中继承条件图像的特定域特征。</p><blockquote><p>效果怎么样？</p></blockquote><p>本文进行了人脸转换、轮廓到背包等实验，并证明了该方法的有效性。</p><h2 id="5-1-Model"><a href="#5-1-Model" class="headerlink" title="5.1 Model"></a>5.1 Model</h2><p>本文提出的模型如下图所示</p><p><img src="/2019/12/11/extensive-reading1/5-model.jpg" alt></p><ul><li>上图中有两个编码器$e_A, e_B$以及两个解码器$g_A, g_B$。<ul><li>编码器用来抽取特征，输入图像后输出两类特征：领域无关特征(Domain-Independent Features)及领域专有特征(Domain-Specific Features)。（感觉名字取得很不友好）<ul><li>在翻译过程中，领域无关特征会保留。如将男人脸转为女人脸时，会保留脸的边缘，眼睛，鼻子</li><li>在翻译过程中，领域专有特征会改变，如脸的头发及风格。（例子也举得不友好）</li></ul></li><li>解码器充当生成器，将源域中图像的领域无关特征和目标域中图像的领域专有特征作为输入，输出属于目标域中的生成图像</li></ul></li><li>在编码器中，编码器网络会被切分成两个分支：一个接上卷积网络用来抽取领域无关特征，另一个接上全连接层用来抽取领域专有特征，通过损失函数的更新实现两分支不同的功能。</li><li>现在以领域$X$为例，讲一下模型的流程。<ul><li>输入图像$x_A$，编码器$e_A$提取领域不变特征$x_A^i$以及领域专有特征$x_A^S$，编码器$e_B$提取图像$x_B$的领域专有特征$x_B^s$。</li><li>接着，将<script type="math/tex">s_A^i</script>和<script type="math/tex">x_A^S</script>结合并输入解码器<script type="math/tex">g_B</script>得到映射到领域<script type="math/tex">B</script>中的图像<script type="math/tex">x_{AB}</script>。</li><li>判别器<script type="math/tex">d_B</script>接收<script type="math/tex">x_{AB}</script>及<script type="math/tex">x_B</script>以判断生成数据是否真实。</li><li>最后，将<script type="math/tex">x_{AB}</script>通过编码器<script type="math/tex">e_B</script>拆分出<script type="math/tex">x_{AB}</script>的领域不变重构特征<script type="math/tex">\hat{x}_A^i</script>以及领域专有重构特征<script type="math/tex">\hat{x}_B^s</script>，并将<script type="math/tex">\hat{x}_A^i</script>和<script type="math/tex">x^s_A</script>结合输入解码器<script type="math/tex">g_A</script>，以得到重构后的图像<script type="math/tex">\hat{x}_A</script>。</li></ul></li><li>领域$Y$同理。</li></ul><h2 id="5-2-Optimization"><a href="#5-2-Optimization" class="headerlink" title="5.2 Optimization"></a>5.2 Optimization</h2><ul><li><p>对抗损失(GAN Loss)。为了保证<script type="math/tex">x_{AB},x_{BA}</script>都能映射到相关的领域中，本文设计了判别器<script type="math/tex">d_A,d_B</script>损失如下</p><script type="math/tex; mode=display">l_{GAN} = log(d_A(x_A)) + log(1 - d_A(x_{BA})) + log(d_B(x_B)) + log(1 - d_B(x_{AB}))</script></li><li><p>双重学习损失(Dual Learning Loss)，使得模型最小化重构损失，如下</p><p><script type="math/tex">l_{dual}^{im}(x_A, x_B) = ||x_A - \hat{x}_A||^2 + ||x_B - \hat{x}_B||^2</script>,</p><p><script type="math/tex">l_{dual}^{di}(x_A, x_B) = ||x_A^i - \hat{x}_A^i|| + ||x_B^i - \hat{x}_B^i||^2</script>,</p><p><script type="math/tex">l_{dual}^{ds}(x_A, x_B) = ||x_A^s, \hat{x}_A^s||^2 + ||x_B^s - \hat{x}_B^s||^2</script>.</p></li></ul><h2 id="5-3-Experiment"><a href="#5-3-Experiment" class="headerlink" title="5.3 Experiment"></a>5.3 Experiment</h2><ul><li>验证指标<ul><li>本文的都是视觉性实验，通过观赏性来验证模型，且输入图像为64x64大小，怀疑是经费不够</li></ul></li><li>对比模型<ul><li>DualGAN</li><li>DualGAN-c</li><li>GAN-c</li></ul></li><li>数据集<ul><li>men-&gt;women</li><li>edges-&gt;shoes</li></ul></li></ul><p>下面贴几张模型的效果图</p><p><img src="/2019/12/11/extensive-reading1/5-exp.jpg" alt></p><p>最后，感谢前辈们的付出，Respect!</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Self-Ensembing with GAN-based Data Augmentation for Domain Adaption in Sematic Segmentation</title>
      <link href="/2019/12/10/self-ensembing/"/>
      <url>/2019/12/10/self-ensembing/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1. Abstract"></a>1. Abstract</h1><ul><li>基于深度学习的语义分割有着固有的缺陷：需要大量的数据</li><li>本文引入self-ensembling，想基于无监督下的领域自适应来解决数据问题，但通过self-ensembling中微调过的人工数据难以缩减语义分割中巨大的领域距离(Domain Gap)</li><li>为此，本文提出一个由两部分组成的框架<ul><li>首先，基于GAN提出一个数据增强方法，能有效促进领域的对齐(Domain Alignment)</li><li>其次，基于增强后的数据，将self-ensembling运用到分割网络中以提升模型的能力</li></ul></li></ul><h1 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h1><ul><li><p>语义分割的任务：给图像中的每个像素点都分类。</p></li><li><p>大量基于深度学习的算法能够获得较好的效果，但过于依赖数据。为避免人工标注的繁琐和耗时，研究人员利用计算机图形学得到合成数据及所对应的分割标签。而合成数据训练得到的分割模型难以媲美使用真实数据训练得到的，因为存在了称之为领域迁移(Domain Shift)的分布不同(Distribution Difference)。</p></li><li><p>Unsupervised domain adaptation通过将标记过的数据集中的知识迁移到未标记过的数据集，来解决Domain Shift的问题。</p><ul><li>最近的方法多集中在对齐源数据和目标数据中抽取到的特征。如基于对抗训练，通过域混肴(Domain Confusion)来最小化领域之间的差异(Domain Discrepancy)。</li><li>然而，对抗方法也有缺陷，为了对齐两个不同领域的全局分布可能会造成负迁移(Negative Transfer)，即将目标特征对齐到了源特征中错误的语义分类。<strong>通俗点说，就是将天空的特征或者风格，迁移到了马路上</strong>。特别是，数据集中的某个类别比较稀少，更容易产生负迁移。</li></ul></li><li><p>为解决负迁移，本文引入了<strong>self-ensembling</strong>。</p><ul><li>self-ensembling由学生网络和教师网络组成<ul><li>学生网络被迫基于老师提供的目标数据，做出协同的预测</li><li>教师网络的参数是学生网络的均值，所以教师在目标数据上做的预测可以看作是学生的伪标签</li></ul></li><li>虽然self-ensembling在图像分类上有很好的效果，若想用于成功对齐领域，它需要大力调节过的人工增强数据。此外，虽然self-ensembling得到的几何变化较大的数据能有效用于分类，它并不适合于语义分割中的Domain Shift。</li></ul></li><li><p>为改进self-ensembling，本文设计了新的数据增强方法</p><ul><li>基于GAN，生成能够保留语义内容的增强图像。因为未保留语义内容的图像，将会破坏分割的性能，由于<strong>增强后的图像和作为源标签的图像在像素上的不匹配</strong>。</li><li>为解决上述问题，该方法于生成器中加入了语义约束(Semantic Constraint)，来保留全局和局部的结构。</li><li>此外，本文提出了目标导向的生成器，能够基于目标领域抽取的风格信息来生成图片。这样，该方法生成的图像又能保留语义信息，又能只迁移目标图像的风格。</li></ul></li><li><p>大多数Image-to-Image Translation都是依赖于不同形式的Cycle-Consistency</p><ul><li>有两个限制<ul><li>需要多余的模块，比如两个生成器</li><li>若是数据不平衡的话，源领域和目标领域的约束过于强烈。<strong>也就是说，不管怎么生成，就那几种图像，类似于Model Collapse吧</strong></li></ul></li><li>而本文的方法由于它的设计，就不需要考虑Cycle-consistency了</li></ul></li><li><p>整个模型的框架如下图所示</p><p><img src="/2019/12/10/self-ensembing/Figure1.jpg" alt="Figure1"></p><ul><li>步骤一，给定有标签的合成数据及无标签的真实数据，生成带有标签的增强数据(我认为，这里的标签就是源标签)</li><li>步骤二，作者将关键信息写到了这里，使用两个分割网络作为教师和学生，以使用Self-ensembling算法。这两个网络都使用了合成数据、增强后的数据以及真实数据。在训练过程中，教师网络会将知识迁移到学生网络中。</li></ul></li></ul><h1 id="3-Proposed-Method"><a href="#3-Proposed-Method" class="headerlink" title="3. Proposed Method"></a>3. Proposed Method</h1><p>​    提升self-ensembling用于语义分割的兼容性的方法是，基于GAN增强后的数据来对齐源领域和目标领域之间的表达，而不是self-ensembling用于分类中的几何转变。为了这个目的，本文设计了以下的模型。</p><h2 id="3-1-Target-guided-generator"><a href="#3-1-Target-guided-generator" class="headerlink" title="3.1 Target-guided generator"></a>3.1 Target-guided generator</h2><ul><li><p>TGCF-DA的模型图如下所示</p><p><img src="/2019/12/10/self-ensembing/Figure2.jpg" alt="Figure2"></p><ul><li>基于假设，图像表达可被分为两部分：内容和图像，设计了以上结构<ul><li>使用Souce Encoder来抽取源图像的内容表达</li><li>再使用Target Encoder来抽取目标图像的风格表达</li></ul></li><li>为了结合源图像的内容和目标图像的风格，使用AdaIN算法</li></ul></li><li><p>根据上面的设计，生成器G得到的图像将会在保留源图像内容的同时，迁移目标图像的风格。<strong>也就是改变源图像，如GTA5的虚拟风格，变成现实风格</strong>。最后，将生成图像作为fake data，目标领域作为real data，输入判别器。</p></li></ul><h2 id="3-2-Semantic-constraint"><a href="#3-2-Semantic-constraint" class="headerlink" title="3.2 Semantic constraint"></a>3.2 Semantic constraint</h2><ul><li>由于没有使用Cycle-consistency，本文使用Semantic Constriant来约束生成图像的语义信息。具体的做法如下<ul><li>设定一个预训练好的分割模型，本文使用的是FCN-8s</li><li>将生成的图像输入得到分割后的掩码</li><li>分割后的掩码和源图像的标签做交叉熵</li></ul></li><li>这个做法类似于相当于风格迁移中，计算内容损失的做法。</li></ul><h2 id="3-3-Target-guided-and-cycle-free-data-augmentation"><a href="#3-3-Target-guided-and-cycle-free-data-augmentation" class="headerlink" title="3.3 Target-guided and cycle-free data augmentation"></a>3.3 Target-guided and cycle-free data augmentation</h2><ul><li><p>本文对于GAN框架的构建如下</p><ul><li><p>Discriminator的构建是基于Hpix2pix的，详见原文中的参考文献</p></li><li><p>使用LSGAN的损失作为对抗损失，并基于spectral normalization的方法稳定GAN的训练</p><script type="math/tex; mode=display">L_{GAN}(G, D) = E_{(x_s, s_t)\thicksim(P_S, P_T)}[D(G(s_s, s_t))^2] + E_{x_t\thicksim P_t}[(D(x_t) - 1)^2].</script></li></ul></li><li><p>对抗损失能够保证G生成的新图像在视觉上和目标图像相似。由于分割模型$f_{seg}$固定了，可以联合训练生成器和判别器来优化总损失</p><script type="math/tex; mode=display">L_{TGCF-DA} = L_{GAN} + \lambda L_{sem}</script></li><li><p>经过上述损失预训练后的生成器，将被用来合成增强数据，为后续的self-ensembling做准备。</p></li></ul><h2 id="3-4-Self-ensembling"><a href="#3-4-Self-ensembling" class="headerlink" title="3.4 Self-ensembling"></a>3.4 Self-ensembling</h2><ul><li><p>构建了教师网络$f_T$和学生网络$f_S$。步骤t中，教师网络的参数$t_i$由学生网络根据下列公式计算得到</p><script type="math/tex; mode=display">t_i = \alpha t_{i-1} + (1 - \alpha)s_i</script></li><li><p>在训练的过程中，每个mini-batch都会包含以下数据</p><ul><li>source samples  源样本</li><li>augmented samples  增强样本</li><li>target samples  目标样本</li></ul></li><li><p>源样本和增强样本将会被用来计算监督损失$L_{sup}$，即对于语义分割的交叉熵。这个损失函数能使得学生网络，对于源数据和增强数据，都产生语义上更加精准的预测。</p></li><li><p>一致性损失$L_{con}$是学生网络和教师网络生成的预测图的均方差</p><script type="math/tex; mode=display">L_{con}(f_S, f_T) = E_{x_t \thicksim P_T}[||\sigma (f_S(x_t)) - \sigma (f_T(x_t))||^2]</script><p>这里的$\sigma$是softmax函数，用来计算预测图的概率</p></li><li><p>总体的损失如下</p><script type="math/tex; mode=display">L_{total} = L_{sup} + \delta_{con} L_{con}</script><p>这里的$\delta_{con}$是一致性损失的权重</p></li></ul><h2 id="3-5-Data-augmentation-for-target-samples"><a href="#3-5-Data-augmentation-for-target-samples" class="headerlink" title="3.5 Data augmentation for target samples"></a>3.5 Data augmentation for target samples</h2><p>​    看到这一块内容，稍微有些迷糊，特别是作者的第一句话：这里的对于目标样本的数据增强和TGCF-DA并不相关，差点就被带偏了，不知道理解到什么地方了。不过仔细看了一下后面的内容，得到如下理解</p><ul><li>这里对target samples增强是为了在self-ensembling中计算consistency loss。对于目标样本的随机数据增强，是为了强迫学生网络针对相同的目标样本得到不同的预测，以更好的训练学生和教师网络。</li><li>根据前文，常规self-ensembling中的几何变换对于像素级别的预测任务并无帮助。因此，本文<strong>在目标样本中注入高斯噪声，并分别喂给学生和教师网络</strong>。此外，对于网络参数还是用了Dropout。</li><li>因此，学生网络在目标样本有扰动的情况下，还必须得产生和教师网络一致的预测，这也变相了提升了模型的性能。</li></ul><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p>​    在做实验之前，作者略微详细的介绍了用到的数据集如：GTA5，Cityscapes等。对于实验的配置，作者说的很详细，建议详细阅读。比如TGCF-DA的具体构造，生成器和判别器都是挑了比较好的结构，一起一些超参数的设置。此外，还讲了self-ensembling中，对于分割网络的选择，选择哪一层计算损失等。</p><h2 id="4-1-Experimental-results"><a href="#4-1-Experimental-results" class="headerlink" title="4.1 Experimental results"></a>4.1 Experimental results</h2><ul><li><p>作者将自己的方法与CycleGAN，MCD，DCAN等一堆方法进行比较。该实验首先在GTA5或者SYNTHIA数据集上，训练分割网络，并在Cityscapes的验证集上验证。实验得结果如下表所示</p><p><img src="/2019/12/10/self-ensembing/Table1.jpg" alt="Table1"></p><ul><li>表中的<strong>Self-Ensembling</strong>，代表着由源数据和目标数据(未加入增强数据)训练得到的分割网络的性能</li><li>表中的<strong>TGCF-DA</strong>表明由源数据、目标数据、TGCF-DA生成的增强数据，共同训练的分割网络</li><li>表中的<strong>Ours(TGCF-DA + SE)</strong>表明结合了TGCF-DA和Self-Enembling方法，得到的分割网络</li><li>表中的<strong>mIoU*</strong>代表的是，13个常见类别的mIoU，因为数据集中有些类别出现次数不多。但具体是哪13个类并没有讲，可能会在作者提供的附件里</li><li>表中的<strong>Source Only</strong>声明了只在源数据集上训练的分割模型的效果</li><li>表中的<strong>Target Only</strong>声明了在监督设定下训练的分割模型的效果</li></ul></li><li><p>作为刚接触Domain Adaption的小白，对于这个实验一直耿耿于怀。首先是对于表中的Baseline(Source Only)的不理解，其次是对Target Only的不了解。经反复思考后，理解如下</p><ul><li><p><strong>Baseline(Source Only)</strong>既然都说了是Source Only，而且文中又把增强后的数据叫做Augmented Data。对应上面的表格，Baseline应该指的就是GTA5和SYNTHIA中的原始训练集，训练得到的分割模型，那效果肯定差。</p></li><li><p>接下来一系列的方法，CycleGAN，MCD，DCAN等，应该都是使用进行Domain Shift后的数据配合原始的mask标签，来训练分割网络，那为什么效果会差呢？根据文中的描述，原因有二</p><ul><li>首先是，经过Domain Shift图像的内容结构可能被破坏了，导致原始的mask标签不匹配，从而产生语义上的损失。<strong>如，车子经过domain shift之后形状变了，但对应的mask掩码标签却没变，那效果肯定差</strong>。</li><li>其次是，负迁移造成的影响，有些类别过于稀疏，在Domain Shift的时候导致转换有误，<strong>如，前面的例子，将马路的知识迁移到了天空上</strong>。</li></ul></li><li><p>然后就是纯Self-Ensembling方法了，效果有一定提升，但是没本文方法提升的多。但作者在前文就指出原始的Self-Ensembling适用于分类网络，但却不太适用于分割网络。<strong>我理解是：由于Self-Ensembling是通过形变原始数据得到增强数据，来提升分类模型的性能；而在分割网络中，形变原始数据会导致其和mask标签不匹配，破坏了内容结构，所以效果变差</strong>。</p></li><li><p>最后就是文中提到的两个方法</p><ul><li><p>先是只使用了<strong>TGCF-DA</strong>，效果并没有好到哪里去，甚至比CycleGAN，MCD，DCAN等方法都要低几个点。这说明，抛弃Cycle Consistency，理想化的将图像分为content和style，并通过AdaIN方法进行结合等一系列操作，并没有达到作者预期的效果。<strong>我觉得作者可能一开始只提出了这个方法，做了实验之后发现效果居然没有好太多，再考虑将Self-ensembling的方法加入来提升网络的性能</strong>。不过，这个方法还是很有创意，巧妙地将任意风格迁移和GAN结合在了一起，值得我思考。</p></li><li><p>然后就是<strong>TGCF-DA + Self-ensembling</strong>，效果简直超神，顺利毕业。原理就是，通过TGCF-DA预训练模型生成增强数据，然后配合学生、老师网络进行训练，最后得到一个更好的分割模型。如下图</p><p><img src="/2019/12/10/self-ensembing/Figure3.jpg" alt="Figure3"></p></li></ul></li></ul></li></ul><h2 id="4-2-Ablation-studies"><a href="#4-2-Ablation-studies" class="headerlink" title="4.2 Ablation studies"></a>4.2 Ablation studies</h2><ul><li><p>首先，做了Self-Ensembling的消融实验，如下图</p><p><img src="/2019/12/10/self-ensembing/Figure4.jpg" alt="Figure4"></p><ul><li>在上一实验的表格中，就发现纯Self-Ensembling效果很差</li><li>这就说明了，主要的功劳并不在Self-Ensembling的应用上，而是TGCF-DA + Self-Ensembling上</li></ul></li><li><p>其次，做了TGCF-DA的消融实验</p><ul><li>在上一实验表格中，发现TGCF-DA能有效提升分割模型的效果。</li><li>其次，通过图示风格模型的mIOU，发现纯Self-Ensembling在第8个epoch就达到了极大值，继续训练效果变差，而TGCF-DA + Self-Ensembling效果持续上升，说明二者结合才是王道。也从侧面说明了TGCF-DA的重要性</li></ul></li><li><p><strong>在消融实验中，作者想表明TGCF-DA + Self-Ensembling结合的重要性，并将主要功劳放在TGCF-DA上。但正如我前文所说，TGCF-DA的效果和其他方法相差不多，那是否将其他方法和Self-Ensembling结合，也会得到更好的效果呢？</strong>不过，这也是我鸡蛋里挑骨头了，本文主要的贡献已经很多了，就比如TGCF-DA + Self-Ensembling结合训分割模型，提升性能，也是本文的卖点。</p></li></ul><h1 id="5-Analysis"><a href="#5-Analysis" class="headerlink" title="5. Analysis"></a>5. Analysis</h1><p>​    在这块内容，作者可视化了几个模块的结果，并进行进一步的分析。</p><h2 id="5-1-Visualizaton"><a href="#5-1-Visualizaton" class="headerlink" title="5.1 Visualizaton"></a>5.1 Visualizaton</h2><ul><li><p>首先是对于Self-Ensembling中间结果的可视化，如下图</p><p><img src="/2019/12/10/self-ensembing/Figure5.jpg" alt="Figure5"></p><ul><li>图中表明，教师网络能够很好的指导学生网络进行训练。</li><li>此外，根据热力图可以发现，consistency loss在训练中会逐渐关注到物体的轮廓，从而微调轮廓提升预测的效果。</li></ul></li><li><p>其次是，可视化自己的增强数据</p><p><img src="/2019/12/10/self-ensembing/Figure6.jpg" alt="Figure6"></p><ul><li>根据作者的描述，大多数方法都扰乱了物体的轮廓</li><li>甚至，有的方法发生了“负迁移”，就是将天空迁移到了马路上，但作者在这里没提“负迁移”，提了一个叫“spills over”的概念，我思索一下就是“负迁移”。</li><li>从而，得出自己的方法计算得快，在视觉程度上效果还好。</li></ul></li></ul><h2 id="5-2-Analysis-of-self-ensembling-with-per-class-IoUs"><a href="#5-2-Analysis-of-self-ensembling-with-per-class-IoUs" class="headerlink" title="5.2 Analysis of self-ensembling with per-class IoUs"></a>5.2 Analysis of self-ensembling with per-class IoUs</h2><ul><li><p>为了更好的理解self-ensembling，作者比较了在使用self-ensembling下，不同类别精度的提升情况，如下图</p><p><img src="/2019/12/10/self-ensembing/Figure7.jpg" alt="Figure7"></p></li><li><p>可以看到，每个类别提升的程度不同。作者人为原因是各个类别数据的不平衡。数据越多的类，提升的效果也越明显。在Self-ensembling中，这个效果会越明显。因为学生网络持续学习到教师网络的预测，那么将会不断的在稀少类别上做出错误的预测。这也印证了作者说的：教师网络的预测是学生网络的伪标签。</p></li><li>这一块实验个人感觉精华就是：类别越多，提升性能越大</li></ul><h1 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h1><ul><li>作者在文章的结尾，还解释了各种超参数的设计和原因，若是对文章感兴趣的朋友可以自行阅读。</li><li>那么，我在看完这篇文章后，总结如下<ul><li>Domain Adaption可以通过迁移真实数据的知识到虚拟数据中，获得增强后的数据，以缓解基于深度学习的语义分割任务中，对于大量标注数据的需求。</li><li>但是，现有的Domain Adaption方法有以下问题<ul><li>生成的增强图像内容信息损失，如结构紊乱，和源数据的标签不匹配了</li><li>生成的增强图像发生了“负迁移”，导致知识迁移的位置不正确，如“天空”迁移到“马路”上</li><li>现有的Domain Shift方法大多基于Cycle-consistency，参数多耗时耗力</li></ul></li><li>故本文提出了基于GAN的数据增强方法TGCF-DA<ul><li>两个Encoder，一个抽取源数据的内容，另一个抽取目标数据的风格</li><li>抽取到的内容和风格通过AdaIN结合在一起构成Generator，生成fake图像</li><li>fake图像，和目标数据集中的图像作为Discriminator的输入，更新Generator</li></ul></li><li>但仅用TGCF-DA生成的增强数据训练分割网络和其他的方法效果差不多，故本文又引入了Self-ensembling来训练分割网络。稍微不同的是，原始的Self-ensembling改变图像的形状，但本文是给图像注入高斯噪音。因为改变几何形状会破坏图像和mask标签的匹配性</li><li>最后，TGCF-DA + Self-ensembling的结合，在实验上取得了令人瞩目的效果</li></ul></li><li>我认为本文的亮点如下<ul><li>TGCF-DA网络的构造，结合了GAN+任意风格迁移，很新颖</li><li>TGCF-DA + Self-ensembling，说白了就是将Domain Shit方法和Self-ensembling方法结合在一起。</li></ul></li><li>我认为本文未解释清楚的就是，没有做实验证明， 其他的Domain Shift方法和Self-ensembling结合在一起，是否也会得到很好的效果</li></ul><p>最后，感谢本文作者的贡献，respect!</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第二章 K近邻法</title>
      <link href="/2019/12/05/knn/"/>
      <url>/2019/12/05/knn/</url>
      
        <content type="html"><![CDATA[<h1 id="1-题目分析"><a href="#1-题目分析" class="headerlink" title="1. 题目分析"></a>1. 题目分析</h1><p>给定一个二维空间数据集T={正实例：(5, 4), (9, 6), (4, 7)；负实例：(2, 3), (8, 1), (7, 2)}，试基于欧氏距离，找到数据点S(5, 3)的最近邻(k=1)，并对S点进行分类预测</p><ul><li>输入：训练集数据$T = \left\{(5, 4, 1), (9, 6, 1), (4, 7, 1), (2, 3, 2), (8, 1, 2), (7, 2, 2)\right\}$；测试集$S = \left\{(5, 3, _)\right\}$；</li><li>输出：测试集中样本所属类别y；<ul><li>根据欧氏距离，在训练集T中找出与测试集中S最邻近的k个点</li><li>统计这k个点中，类别个数最多的点，作为测试样本的类别</li></ul></li></ul><p>​    k近邻法没有显式的学习过程</p><h1 id="2-算法分析"><a href="#2-算法分析" class="headerlink" title="2. 算法分析"></a>2. 算法分析</h1><p>K近邻完整算法请走<a href="https://github.com/GodWriter/Statistical-Learning-Method/blob/master/Chapter%203/KNN.ipynb" target="_blank" rel="noopener">传送门</a>，下面将拆解分析</p><h2 id="2-1-初始化"><a href="#2-1-初始化" class="headerlink" title="2.1 初始化"></a>2.1 初始化</h2><pre class=" language-lang-python"><code class="language-lang-python"># 定义训练集和测试数据training_set = np.array([[5, 4, 1],                         [9, 6, 1],                         [4, 7, 1],                         [2, 3, 2],                         [8, 1, 2],                         [7, 2, 2]])rows = training_set.shape[0]cols = training_set.shape[1]S = np.array([[5, 3]])</code></pre><ul><li>将训练数据存储为2行3列的array，最后一列是每条数据所属类别</li><li>我们以1作为正例，以2作为负例</li></ul><h2 id="2-2-图示样本"><a href="#2-2-图示样本" class="headerlink" title="2.2 图示样本"></a>2.2 图示样本</h2><pre class=" language-lang-python"><code class="language-lang-python"># 方法：画散点图def plot(sample):    plt.xlabel('x')    plt.ylabel('y')    plt.axis([0, 10, 0, 10])    x = sample[0]    y = sample[1]    cat = sample[2]    if cat == 1:        plt.scatter(x, y, c='r', marker='o')    else:        plt.scatter(x, y, c='b', marker='x')</code></pre><ul><li>首先，定义好坐标轴刻度，都控制在(0, 10)之间</li><li>其次，画点<ul><li>若为正例，画红色圈</li><li>若为负例，画蓝色叉</li></ul></li></ul><h2 id="2-3-欧氏距离"><a href="#2-3-欧氏距离" class="headerlink" title="2.3 欧氏距离"></a>2.3 欧氏距离</h2><pre class=" language-lang-python"><code class="language-lang-python"># 方法：计算样本之间的欧式距离def compute_distance(x_train, x_test):    distance = np.sqrt(np.square((x_train[0] - x_test[0])) +                        np.square((x_train[1] - x_test[1])))    return np.array([distance, x_train[2]])</code></pre><ul><li>即计算训练集中样本和测试集样本的距离</li><li>需要保留训练集样本所属的类别，便于后面比较距离时调用，故返回的是一个array([两点之间距离，训练样本所属类别])</li></ul><h2 id="2-4-遍历样本"><a href="#2-4-遍历样本" class="headerlink" title="2.4 遍历样本"></a>2.4 遍历样本</h2><pre class=" language-lang-python"><code class="language-lang-python"># 遍历训练集并计算距离distance_array = np.zeros([1, 2])for row in range(rows):    x_train = training_set[row, :]    x_test = S[0]    distance = compute_distance(x_train, x_test)    distance_array = np.row_stack((distance_array, distance))    plot(x_train)</code></pre><ul><li>首先，定义一个distance_array，用于保存测试样本与训练集中所有样本的距离</li><li>遍历训练集中的所有样本<ul><li>计算两点之间的欧式距离</li><li>动态扩展distance_array的大小<ul><li>np.row_stack()可在distance_array中加入子项</li><li>即插入行数据，每行数据为compute_distance()返回的结果</li></ul></li><li>同时，将此刻用到的训练集样本在坐标系中画出</li></ul></li></ul><h2 id="2-5-判断类别"><a href="#2-5-判断类别" class="headerlink" title="2.5 判断类别"></a>2.5 判断类别</h2><pre class=" language-lang-python"><code class="language-lang-python"># 选取k值，并得到类别k = 5distance_array = distance_array[np.argsort(distance_array[:, 0])]cat_array = distance_array[1: (k+1), 1].astype(np.int32)max_cat = np.argmax(np.bincount(cat_array))test_value = np.column_stack((S,                               np.array([max_cat],                               dtype=np.int32)))[0]print("The test data belongs to ", max_cat)</code></pre><ul><li>首先，选取K值，这里我们设置为5，即比较所有数据与测试样本的距离</li><li>其次，我们调用np.argsort()将测试样本与训练集中样本的根据距离从小到大进行排序，得到排序后的distance_array并返回</li><li>接着，由于我们初始化distance_array时，默认存入[0, 0]，经排序后放在第一行；故删除第一行，取剩下所有行；且只保留类别信息存入cat_array</li><li>最后，我们调用np.bincount()对cat_array中出现的类别进行统计，并调用np.argmax()取出出现次数最大值，作为测试样本的类别；并将类别加入到测试样本中，便于图示</li></ul><h2 id="2-6-图示样本集"><a href="#2-6-图示样本集" class="headerlink" title="2.6 图示样本集"></a>2.6 图示样本集</h2><pre class=" language-lang-python"><code class="language-lang-python"># 图示所有点的位置plot(test_value)plt.text(test_value[0], test_value[1]+0.2, 'S')plt.show()</code></pre><ul><li><p>先画出测试样本在图中的位置</p></li><li><p>并根据y坐标，标记其符号S</p></li><li><p>整体展示所有样本</p><p><img src="/2019/12/05/knn/plot.png" alt="超平面"></p></li></ul><h1 id="3-Sklearn实现"><a href="#3-Sklearn实现" class="headerlink" title="3. Sklearn实现"></a>3. Sklearn实现</h1><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.neighbors import KNeighborsClassifier# 1. 定义K值k = 5# 2. 根据参数要求得到训练集和测试集x_train = training_set[:, :2]y_train = training_set[:, 2]x_test = S# 3. 进行KNN分类clf = KNeighborsClassifier(n_neighbors=k, n_jobs=1)clf.fit(x_train, y_train)y_predict = clf.predict(x_test)print("The test data belongs to ", y_predict[0])</code></pre>]]></content>
      
      
      <categories>
          
          <category> 统计学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一章 感知机</title>
      <link href="/2019/12/03/perceptron/"/>
      <url>/2019/12/03/perceptron/</url>
      
        <content type="html"><![CDATA[<h1 id="1-题目分析"><a href="#1-题目分析" class="headerlink" title="1. 题目分析"></a>1. 题目分析</h1><p>已知训练数据集D，其正实例点是$x_1 = (3, 3)$，$x_2 = (4, 3)$, 负实例点是$x_3 = (1, 1)^T$，求感知机模型</p><ul><li>输入：训练数据集$T = \left\{(x_1, 1), (x_2, 1), (x_3, 1)\right\}$；学习率$\eta(0 &lt; \eta \leq1)$;</li><li>输出：w, b；感知机模型$f(x) = sign(w\cdot{x} + b)$.<ul><li>选取初值$w_0, b_0$</li><li>在训练集中选取数据$(x_i, y_i)$</li><li>如果$y_i(w\cdot{x} + b) \leq 0$<ul><li>$w \leftarrow w + \eta{y_ix_i}$</li><li>$b \leftarrow b + \eta{y_i}$</li></ul></li><li>转至(2)，直至训练集中没有误分类点</li></ul></li></ul><p>直观上的解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w和b，使得分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。</p><h1 id="2-算法分析"><a href="#2-算法分析" class="headerlink" title="2. 算法分析"></a>2. 算法分析</h1><p>感知机完整算法请走<a href="https://github.com/GodWriter/Statistical-Learning-Method/blob/master/Chapter%202/Perception.ipynb" target="_blank" rel="noopener">传送门</a>，下面将拆解分析</p><h2 id="2-1-初始化"><a href="#2-1-初始化" class="headerlink" title="2.1 初始化"></a>2.1 初始化</h2><pre class=" language-lang-python"><code class="language-lang-python"># 定义数据集data = np.array([[3, 3, 1],                 [4, 3, 1],                 [1, 1, -1]])rows = data.shape[0]cols = data.shape[1]</code></pre><ul><li>将训练数据存储为2行3列的array，最后一列是每条数据的标签</li><li>分别得到矩阵的行数rows和列数cols，为后续遍历做准备</li></ul><pre class=" language-lang-python"><code class="language-lang-python"># 选取初值w0, b0w = np.ones(cols - 1)b = 0thea = 0.001</code></pre><ul><li>初始化$w_0 = (1, 1)$，$b = 0$</li><li>初始化$theta$，用于图示超平面，会在下面讲解</li></ul><h2 id="2-2-判断条件"><a href="#2-2-判断条件" class="headerlink" title="2.2 判断条件"></a>2.2 判断条件</h2><pre class=" language-lang-python"><code class="language-lang-python"># 方法：判别是否存在分类错误的点def sign(data, w, b):    flag = False    rows = data.shape[0]    cols = data.shape[1]    for row in np.arange(rows):        x_i = data[row, :-1]        y_i = data[row, -1]        if y_i * (np.matmul(w, x_i.T) + b) <= 0:            flag = True            break    return flag</code></pre><ul><li>对应算法中的第(3)步，筛选数据集中是否存在样本，使得$y_i(w\cdot{x} + b) \leq 0$成立</li><li>设置$flag$变量，若为True则存在，若为False则不存在</li><li>一旦发现不存在，马上跳出循环</li></ul><h2 id="2-3-图示超平面"><a href="#2-3-图示超平面" class="headerlink" title="2.3 图示超平面"></a>2.3 图示超平面</h2><pre class=" language-lang-python"><code class="language-lang-python"># 方法：画散点图def plot(data, w, b):    plt.xlabel('x1')    plt.ylabel('x2')    plt.axis([0, 6, 0, 6])    x = data[:, 0]    y = data[:, 1]    plt.scatter(x, y)    x_line = [p for p in np.arange(10)]    if w[1] == 0:        y_line = [(-w[0]*x - b)/(w[1] + thea) for x in x_line]    else:        y_line = [(-w[0]*x - b)/w[1] for x in x_line]    plt.plot(x_line, y_line)</code></pre><ul><li><p>首先，定义好坐标轴的刻度，都控制在(0, 6)之间</p></li><li><p>其次，先将数据集中的点画出来</p></li><li><p>接着，根据当前的$w, b$值，选取$x_1, x_2$</p><ul><li><p>$x_1$我们选定为(1, 10)中的整数</p></li><li><p>根据$w_1x_1 + w_2x_2 + b = 0$，已知$x_1$，求得$x_2$如下</p><p>$x_2 = \frac{-W_1x_1 - b}{w_2}$</p></li><li><p><strong>由于$w_2$可能为0，导致$x_2$求值出现问题，故我们加上一个$\theta$变量，一旦$w_2$为0， 则使得$x_2$为</strong></p><p>$x_2 = \frac{-W_1x_1 - b}{w_2 + \theta}$</p></li></ul></li><li><p>最后，画出超平面即可</p></li></ul><h2 id="2-4-遍历求解"><a href="#2-4-遍历求解" class="headerlink" title="2.4 遍历求解"></a>2.4 遍历求解</h2><pre class=" language-lang-python"><code class="language-lang-python"># 遍历数据集while sign(data, w, b):    for row in np.arange(rows):        x_i = data[row, :-1]        y_i = data[row, -1]        if y_i * (np.matmul(w, x_i.T) + b) <= 0:            w = w + y_i * x_i            b = b + y_i    plot(data, w, b)</code></pre><ul><li><p>以$sign()$函数作为条件，判断有无误分类点，一旦有就遍历循环</p><ul><li><p>遍历每一条数据</p><ul><li>若是满足条件$y_i(w\cdot{x} + b) \leq 0$</li><li>更新$w, b$</li></ul></li><li><p>将当前的超平面画出</p><p><img src="/2019/12/03/perceptron/plot.png" alt="超平面"></p></li></ul></li></ul><h1 id="3-Sklearn实现"><a href="#3-Sklearn实现" class="headerlink" title="3. Sklearn实现"></a>3. Sklearn实现</h1><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.linear_model import Perceptron# 1. 定义数据集data = np.array([[3, 3, 1],                 [4, 3, 1],                 [1, 1, -1]])# 2. 定义模型，求解perceptron = Perceptron()perceptron.fit(data[:, :2], data[:, 2])# 3. 打印w,b，并图示print("w: ", perceptron.coef_, "b: ", perceptron.intercept_)plot(data, perceptron.coef_[0, :], perceptron.intercept_[0])# 4. 测试模型准确率res = perceptron.score(data[:, :2], data[:, 2])print("correct rate:{:.0%}".format(res))</code></pre>]]></content>
      
      
      <categories>
          
          <category> 统计学习方法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>N way to write</title>
      <link href="/2019/09/25/pape-word/"/>
      <url>/2019/09/25/pape-word/</url>
      
        <content type="html"><![CDATA[<h1 id="1-常用词的N种说法"><a href="#1-常用词的N种说法" class="headerlink" title="1. 常用词的N种说法"></a>1. 常用词的N种说法</h1><h2 id="1-1-Besides"><a href="#1-1-Besides" class="headerlink" title="1.1 Besides"></a>1.1 Besides</h2><ul><li>Moreover</li><li>In addition</li></ul><h2 id="1-2-Get"><a href="#1-2-Get" class="headerlink" title="1.2 Get"></a>1.2 Get</h2><ul><li>obtain</li></ul><h2 id="1-3-Show"><a href="#1-3-Show" class="headerlink" title="1.3 Show"></a>1.3 Show</h2><ul><li>exhibit</li><li>demonstrate</li><li>present</li></ul><h2 id="1-4-Improve"><a href="#1-4-Improve" class="headerlink" title="1.4 Improve"></a>1.4 Improve</h2><ul><li>be boosted to</li></ul><h2 id="1-5-Compare"><a href="#1-5-Compare" class="headerlink" title="1.5 Compare"></a>1.5 Compare</h2><ul><li>In contrast</li></ul><h2 id="1-6-Can"><a href="#1-6-Can" class="headerlink" title="1.6 Can"></a>1.6 Can</h2><ul><li>allow to</li><li>attempt to </li><li>be able to</li><li>be exploited for</li><li>play the role of</li><li>be capable of </li></ul><h2 id="1-7-Many"><a href="#1-7-Many" class="headerlink" title="1.7 Many"></a>1.7 Many</h2><ul><li>diverse    多种多样的</li><li>multiple</li></ul><h2 id="1-8-Motivated-by"><a href="#1-8-Motivated-by" class="headerlink" title="1.8 Motivated by"></a>1.8 Motivated by</h2><ul><li>Inspired by</li></ul><h2 id="1-9-Suppose"><a href="#1-9-Suppose" class="headerlink" title="1.9 Suppose"></a>1.9 Suppose</h2><ul><li>assume</li></ul><h2 id="1-10-Use"><a href="#1-10-Use" class="headerlink" title="1.10 Use"></a>1.10 Use</h2><ul><li>utilize</li><li>employ</li><li>apply … to …</li><li>enforce</li><li>be easily integrated with</li><li>deploy … as …</li><li>be taken as </li><li>be employed as</li></ul><h2 id="1-11-solve"><a href="#1-11-solve" class="headerlink" title="1.11 solve"></a>1.11 solve</h2><ul><li>address</li></ul><h2 id="1-12-define"><a href="#1-12-define" class="headerlink" title="1.12 define"></a>1.12 define</h2><ul><li>refer to sth as sth</li><li>sth be identical to sth</li><li>be represented as </li></ul><h2 id="1-13-for-example"><a href="#1-13-for-example" class="headerlink" title="1.13 for example"></a>1.13 for example</h2><ul><li>for instance</li><li>take … as an example</li></ul><h2 id="1-14-come-from"><a href="#1-14-come-from" class="headerlink" title="1.14 come from"></a>1.14 come from</h2><ul><li>drift from</li></ul><h2 id="1-15-aim-to"><a href="#1-15-aim-to" class="headerlink" title="1.15 aim to"></a>1.15 aim to</h2><ul><li>for the purpose of</li></ul><h1 id="2-专有名词"><a href="#2-专有名词" class="headerlink" title="2. 专有名词"></a>2. 专有名词</h1><h2 id="2-1-Image-to-Image-Translation"><a href="#2-1-Image-to-Image-Translation" class="headerlink" title="2.1 Image-to-Image Translation"></a>2.1 Image-to-Image Translation</h2><h3 id="2-1-1-Word"><a href="#2-1-1-Word" class="headerlink" title="2.1.1 Word"></a>2.1.1 Word</h3><ul><li>stylised image    转换后的风格图</li><li>paired image    成对的图片</li><li>be transfered to</li><li>a fixed target style</li><li>multimodal translation    多迁移</li><li>deconvolution layer  反卷积层</li><li>stable training</li><li>uniform sampling</li><li>randomly sampled from</li></ul><h3 id="2-1-2-Advantage"><a href="#2-1-2-Advantage" class="headerlink" title="2.1.2 Advantage"></a>2.1.2 Advantage</h3><ul><li>deterministic one-to-one mapping transfer a soure image into the target style</li><li>these works can be divided into two categories according to the controllabiliy of the target styles</li><li>transfer the images in S into the style of T</li><li>transfer source images into the target style</li><li>By jointly optimizing all modules, CycleGAN model is able to transfer source images into the target sytle and v.v.</li><li>We take the S-&gt;T direction as an example, and the other direction can be similarly applied.</li><li>After the model is learnt, source images can only be translated to a fixed style</li></ul><h3 id="2-1-3-Disadvantage"><a href="#2-1-3-Disadvantage" class="headerlink" title="2.1.3 Disadvantage"></a>2.1.3 Disadvantage</h3><ul><li>Most previous studies for GAN-baed Image-to-Image translation methods rely on various forms of cycle-consistency$^{[1]}$.</li></ul><h2 id="2-2-Domain-Adaption"><a href="#2-2-Domain-Adaption" class="headerlink" title="2.2 Domain Adaption"></a>2.2 Domain Adaption</h2><h3 id="2-2-1-Word"><a href="#2-2-1-Word" class="headerlink" title="2.2.1 Word"></a>2.2.1 Word</h3><ul><li>source domain    源域</li><li>target domain     目标域</li><li>intermediate domain    中间域</li><li>generalization ability    生成能力</li><li>domainness variable    邻域变量</li><li>pixel level</li><li>synthetic data    合成数据</li><li>real scenario  真实场景</li><li>be proportional to  和…成比例</li><li>recover … from …</li><li>be implemented with  被实施</li><li>spread along … from … to …</li><li>cross-domain semantic segmentation problem</li><li>a mixture of different target styles</li><li>shift from … to …</li><li>domain gap</li><li>domain alignment</li><li>distribution difference</li><li>semantic constraint  语义约束</li></ul><h3 id="2-2-2-Advantage"><a href="#2-2-2-Advantage" class="headerlink" title="2.2.2 Advantage"></a>2.2.2 Advantage</h3><ul><li>Domain adaptation aims to utilize a labeled source domain to learn a model that performs well on an unlabeled target domain</li><li>Domain generalization aims to learn a model that could be generalized to an unseen target domain by using multiple labeled source domains</li><li>align the image distributions for two domains</li><li>Object function of GAN can be seen as a lower bound of the Hessen-Shannon divergence</li><li>translate each source image into an arbitrary intermediate domain</li><li>Unsupersived domain adaption seeks to adapt the model trained on the source domain to the target domain$^{[1]}$.</li><li>Self-ensembling is composed of a teacher and a student network, where the student is compelled to produce consistent predictions provided by the teacher on target data$^{[1]}$.</li></ul><h3 id="2-2-3-Disadvantage"><a href="#2-2-3-Disadvantage" class="headerlink" title="2.2.3 Disadvantage"></a>2.2.3 Disadvantage</h3><ul><li>require a large amount of data with pixel-level annotations</li><li>The adversarial loss may trigger a negative transfer, which aligns the target feature with the source feature in an incorrect semantic category$^{[1]}$.</li><li></li></ul><h2 id="2-3-Style-Transfer"><a href="#2-3-Style-Transfer" class="headerlink" title="2.3 Style Transfer"></a>2.3 Style Transfer</h2><h3 id="2-3-1-Word"><a href="#2-3-1-Word" class="headerlink" title="2.3.1 Word"></a>2.3.1 Word</h3><ul><li>style patterns</li><li>the iterative optimization process</li><li>feedforward methods</li><li><strong>insufficient visual quality</strong></li><li>global feature statistics</li><li>in the feature space</li><li>encoder-decoder module</li><li>global color distribution</li><li>texture</li><li>local style patterns</li><li>bush strokes</li><li>receptive field</li></ul><h3 id="2-3-2-Sentence"><a href="#2-3-2-Sentence" class="headerlink" title="2.3.2 Sentence"></a>2.3.2 Sentence</h3><ul><li>Arbitrary style transfer aims to synthesize a content image with the style of an image to create a third image that has never been seen before.</li><li>The ultimate goal of arbitrary style transfer is to simultaneously achieve and preserve generation, quality, and efficiency.</li><li>We use the encoder(a pre-trained VGG-19) to compute the loss function for training …</li></ul><h1 id="3-常用短语"><a href="#3-常用短语" class="headerlink" title="3. 常用短语"></a>3. 常用短语</h1><ul><li>draw increasing attention</li><li>most existing works</li><li>the benefit of</li><li>training phase    训练阶段</li><li>inference phase    测试阶段</li><li>be injected into    被注入</li><li>perform… by …</li><li>network structure</li><li>tent to </li><li>at the beginning</li><li>at the end</li><li>each of sth</li><li>computer vision tasks</li><li>achieve good performance</li><li>fill the gap between .. and …</li><li>conduct experiments on </li><li>training policy</li><li>verify the ability of </li><li>as expectedoop</li><li><strong>give comparable result</strong></li><li>by further using</li><li>from the first column, …</li><li>find it challenging to …</li><li>the seminal work of   …的开创性工作</li><li>computational cost of …</li><li>computationally expensive operations</li><li>the proposed model</li><li>network structure</li><li>experimental settings</li><li>less appealing</li><li>run time performance</li><li>in real time</li><li>be compelled to 被迫</li></ul><h2 id="3-1-Solve-the-problem"><a href="#3-1-Solve-the-problem" class="headerlink" title="3.1 Solve the problem"></a>3.1 Solve the problem</h2><ul><li>To adress this challenging issue</li><li>To overcome this limitation</li></ul><h1 id="4-常用衔接语"><a href="#4-常用衔接语" class="headerlink" title="4. 常用衔接语"></a>4. 常用衔接语</h1><ul><li>on one hand…, on the other hand</li><li>Unlike the above work</li><li>Our work is partialy inspired by</li><li>Formally, …</li><li>In particular, …</li><li>In this way, …</li><li>When …., …; and when, …</li><li>no longer aim to …, but to …</li><li>Specifically</li><li>Due to the usage of …</li><li>With regard to …</li><li>The motivation is as follows, …</li><li>Accordingly, …</li><li>In other words, …</li><li>In this section, …</li><li>It can be observed that</li><li>More interestingly, …</li><li>As discussed in </li><li>Note, …</li><li>In this paper, …</li><li>Experimental results demonstrate that …</li><li>Significant efforts have been made to …</li><li>Despite valuable efforts</li><li>Despite recent advances, …</li><li>In many cases, …</li><li>Similar to …</li></ul><h1 id="5-长句型"><a href="#5-长句型" class="headerlink" title="5. 长句型"></a>5. 长句型</h1><ul><li><strong>Specifically</strong>, we <strong>introduce two</strong> discriminators, $D_S(x)$ <strong>to</strong> distinguish $M^{(z)}$ and S, and $D_T(x)$ <strong>to </strong>distinguish $M^{(z)}$ and T, <strong>respectively</strong>.</li><li>… in two settings. In the first setting, … . In the second setting, … .</li><li>Two cases are considered, … . For the first case, … . For the second case, … .</li><li>… in two scenarios. Firstly, … . Secondly, … .</li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li>Choi, Jaehoon &amp; Kim, Taekyung &amp; Kim, Changick. (2019). Self-Ensembling with GAN-based Data Augmentation for Domain Adaptation in Semantic Segmentation. </li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Faster-RCNN</title>
      <link href="/2019/09/07/faster-rcnn/"/>
      <url>/2019/09/07/faster-rcnn/</url>
      
        <content type="html"><![CDATA[<h1 id="1-几个问题"><a href="#1-几个问题" class="headerlink" title="1. 几个问题"></a>1. 几个问题</h1><ul><li>Anchor和全卷积输出值之间的关系</li><li>“回归系数”是什么，有什么用？</li><li>在选择RPN Boxes时，既要考虑RPN Boxes与Ground Truth的IOU来筛选，又要考虑每个RPN boxes为物体前景的概率。那么在筛选RPN Boxes的时候，哪个先考虑，哪个后考虑，还是说同时考虑？如果说是同时考虑，又怎么考虑？</li></ul><h1 id="2-Anchor-Generation-Layer"><a href="#2-Anchor-Generation-Layer" class="headerlink" title="2. Anchor Generation Layer"></a>2. Anchor Generation Layer</h1><ul><li>Anchor主要为了在输入的图像上产生多个可能存在物体的bounding box，所以Anchor肯定是要分布在整个图像上，才能包括所有的物体。那么，在Faster-RCNN中Anchor是如何产生的呢？Anchor的产生下图所示：</li></ul><p><img src="/2019/09/07/faster-rcnn/Anchor-Generation-Layer.png" alt></p><ul><li>初学者很容易犯的错误就是：Anchor其实是神经网络产生的，最后要通过训练才能得到Anchor及图中那么多bounding box。其实不然，Anchor就是通过机械化的操作产生覆盖整张图的bounding box，数量多且杂。<strong><font color="red">和Anchor有关的神经网络做的并不是产生bounding box，而是产生相对应的bounding box的回归系数，回归系数正好也是4个值</font></strong>。我们先来理解什么是所谓的“机械化”，再来理解“回归系数”。</li><li>对“机械化”的理解可以从上图解释。Faster-RCNN设置了aspect ratio(0.5, 1, 2)，以及Anchor Scales(8, 16, 32)。大部分博客说到这里就结束了，他们会说3x3=9，最后一共产生9个anchors。那么到底是怎么计算的呢？详细可以看我通过源码阅读得到的计算过程，传送门在<a href="https://godwriter.github.io/2019/09/03/fast-rcnn-code/#more">这里</a>。如下面公式所示：</li><li>综上，我们知道了<ul><li>Anchors不是神经网络产生的，而是人为设计的，通过指定<strong>aspect ratio</strong>和<strong>anchor scales</strong>两个参数来生成。</li><li>Anchors的原始比例是定义在特征图上的，若是返回到原图的话，需要乘以相应的倍数</li><li><font color="red">仍有一个问题没有解决，Anchors和回归系数之间的关系？我们继续往下看</font></li></ul></li></ul><h1 id="3-Region-Proposal-Layer"><a href="#3-Region-Proposal-Layer" class="headerlink" title="3. Region Proposal Layer"></a>3. Region Proposal Layer</h1><ul><li><p>在Faster-RCNN之前，R-CNN通过selective search method来生成region proposal，但是速度慢、效率低，故Faster-RCNN通过Anchors来生成候选框，如上述。然而，Anchors生成的框就可以直接用了吗？我们看到那么多密密麻麻的生成框，而且还是最最简单的通过比例放缩得到的，肯定是需要进行筛选和回归的。那么，<strong>Region Proposal Layer就是完成了筛选和回归的工作</strong>，具体如下：</p><ul><li>从一系列生成的anchors中，确定哪些是前景，哪些是背景</li><li>通过一系列的<strong>“regression coefficients”即回归系数</strong>修正anchors的位置、宽度及高度，来提升anchors的质量。</li></ul><p>Region Proposal Layer一共包含三层：Proposal Layer， Anchor Target Layer以及Proposal Target Layer。整体结构如下图所示：</p><p><img src="/2019/09/07/faster-rcnn/Region-Proposall-Layer.png" alt></p><p>如上图所示，rpn_net通过两个1x1的卷积核，输出两个支路：rpn_cls_score_net以及rpn_bbx_pred_net，它们的高度、宽度相同，唯一不同的是深度、即通道个数。rpn_cls_score_net的通道个数为2x9=18，因为它指的是feature map上每个单元格产生的9个anchor是前景和背景的概率。rpn_bbx_pred_net的通道个数为4x9=36，因为它生成的是feature map上每个单元格产生的9个anchor的<strong>回归系数</strong>。</p></li></ul><h1 id="4-Bounding-Box-Regression-Coefficients-回归系数"><a href="#4-Bounding-Box-Regression-Coefficients-回归系数" class="headerlink" title="4. Bounding Box Regression Coefficients(回归系数)"></a>4. Bounding Box Regression Coefficients(回归系数)</h1><p>​    在详细讲解Region Proposal Layer三层结构之前，我们把困扰到现在的一个问题解决，那就是：这个回归系数究竟是什么？和Anchors有什么联系？</p><ul><li><p>“回归系数”即<font color="red"><strong>对anchors进行微调，使anchors更加接近ground truth的一组参数($t_x, t_y, t_w, t_h$)</strong></font>。再用大白话解释一下，我们通过anchors“机械”方式生成的RPN bboxes就算能够和ground truth比较接近，但是仍存在偏差，那么就需要一组参数对这些RPN bboxes进行回归，这也就是我们的“回归系数”bounding box regression coefficients。</p></li><li><p>那么，这些回归系数该怎么计算呢？我们先做如下定义</p><ul><li><p>$(T_x, T_y, T_w, T_h)$为目标边界框，即目标边界框的(左上角坐标x，左上角坐标y，宽，高)</p></li><li><p>$(O_x, O_y, O_w, O_h)$为原始边界框，即原始边界框的(左上角坐标x，左上角坐标y，宽，高)</p></li><li><p>那么$(t_x, t_y, t_w, t_h)$的计算过程如下：</p><p>​    $t_x=\frac{T_x-O_x}{O_w}$           $t_y=\frac{T_y - O_y}{O_h}$           $t_w=log(\frac{T_w}{O_w})$          $t_h=log(\frac{T_h}{O_h})$</p></li><li><p>若是不了解原始框和目标框的定义，不妨将原始框看作RPN Bbox，将目标框看作Ground Truth Bbox。这样就可以通过Anchors生成的RPN Bbox及它们各自对应的Ground Truth Bbox来计算回归系数。</p></li></ul></li><li><p>在了解了什么是bounding box regression coefficients后，我们需要注意一点：当一个图片未做过剪切，只是做了仿射变换如：放大、缩小，那么该图中的bbox所对应的回归系数是不变的，因为做了等比例的转换。为什么要强调这一点，因为在后面计算分类损失的时候，目标的回归系数将按照原始纵横比计算；而分类网络输出的回归系数，是基于方块特征图经ROI池化后计算得到的。下图解释的很明白</p><p><img src="/2019/09/07/faster-rcnn/regression-coefficients.png" alt></p></li></ul><p>​    通过对回归系数的理解后，我们可坚定的知道：<font color="red">rpn_bbx_pred_net输出的并不是一个框的坐标，而是每个RPN box所对应的回归系数，来调整自身的位置更好的接近Ground Truth</font>。</p><h1 id="5-Region-Proposal-Layer"><a href="#5-Region-Proposal-Layer" class="headerlink" title="5. Region Proposal Layer"></a>5. Region Proposal Layer</h1><p>Region Proposal Layer一共包含三层：Proposal Layer， Anchor Target Layer以及Proposal Target Layer。下面，我们将一层层来理解。</p><h2 id="5-1-Proposal-Layer"><a href="#5-1-Proposal-Layer" class="headerlink" title="5.1 Proposal Layer"></a>5.1 Proposal Layer</h2><ul><li>Proposal Layer使用基于前景分数的non-maximum suppression来筛选anchors生成的RPN box的数量。</li><li>此外，还要<strong>将“回归系数”应用到RPN boxes上来生成变换后的bounding boxes</strong>。可以通过逆推公式获得bounding boxes。这里推出的bounding box是所谓的Target boudning box，是网络的预测值，并不是Groud truth box。公式里的原始边界框指的是RPN boxes。</li><li>Proposal Layer的工作如下图所示，具体细节待补充：</li></ul><p><img src="/2019/09/07/faster-rcnn/Proposal-Layer.png" alt></p><ul><li>从图中的输出来看，此时的ROIs已经是变换坐标后的bounding box。在做非极大值抑制之前，就已经完成了坐标的转换。</li></ul><h2 id="5-2-Anchor-Target-Layer"><a href="#5-2-Anchor-Target-Layer" class="headerlink" title="5.2 Anchor Target Layer"></a>5.2 Anchor Target Layer</h2><ul><li><p>Anchor Target Layer的目的就是选择有前途的RPN boxes，用来训练RPN网络，使得RPN网络达到以下两个功能：</p><ul><li>能够区分前景和背景</li><li>为作为前景的RPN boxes生成好的“回归系数”</li></ul></li><li><p><strong>RPN的损失</strong>。为了更好的理解Anchor Target Layer，我们首先要看一下RPN的损失是怎么计算的。RPN层的主要目的是：<strong>生成更好的bounding boxes</strong>。故从一群RPN Boxes中，RPN层必须学会区分前景和背景，且要能计算“回归系数”来调整前景boxes的位置，宽度和高度，以得到一个“更好”的前景box。故RPN Loss的设计也是为了这个目的。</p><ul><li><p>RPN总损失由两项损失构成：分类损失，bbox回归损失。</p><ul><li><p>分类损失使用<strong>交叉熵</strong>来惩罚错误分类的bbox</p><p>$CrossEntropy(Predicted<em>{class}, Actual</em>{class})$</p></li><li><p>bbox回归损失使用距离函数度量<strong>真实回归系数（与作为前景的RPN box最为接近的ground truth box，通过上文提到的公式计算得到）</strong>和<strong>网络预测的回归系数</strong>。$L_{loc}$对所有前景RPN boxes的回归损失求和。作为背景的RPN box不需要进行求和，因为没有真实标签，故没意义。</p><p>$L<em>{loc} = \sum</em>{u\in{all \ foreground \ anchors}}l_u$</p></li></ul></li><li><p>$l_u$的计算公式如下所示，计算的是RPN预测的“回归系数”和“真实回归系数”（使用离RPN bbox最近的ground truth得到）之间的差。</p><p>$l<em>u = \sum</em>{i\in{x, y,w,h}}smooth_{L1}(u_i(predicted) - u_i(target))$</p><p>这里的$u_i$展开来就是$(u_x, u_y, u_w, u_h)$，smooth L1函数如下所示：</p><p><img src="/2019/09/07/faster-rcnn/Users/18917/Documents/备份/formula.png" alt></p><p>这里的$\sigma$是任意选择的，为了避免for-if循环，将会使用一个掩码矩阵来计算损失</p></li><li><p>根据上述损失的定义，我们需要计算下面的量</p><ul><li>RPN boxes的类别标签（前景或者背景）及得分，计算类别损失</li><li>前景RPN boxes的目标回归系数，计算位置损失</li></ul></li><li><p>为了得到上面的量，我们将进行以下操作</p><ul><li>首先，选择位于图像范围内的RPN boxes</li><li>其次，通过计算所有RPN boxes和Ground Truth之间的<strong>IOU</strong>重叠来选好的前景框；通过这个<strong>IOU</strong>，两种类型的RPN bbox将会被标记为前景框<ul><li>类型A：对于每个ground truth box，所有与他们IOU值最大的RPN Boxes</li><li>类型B：对于每个ground truth box，所有与他们IOU值超阈值的RPN Boxes</li></ul></li><li>最后，图示这些盒子</li></ul><p><img src="/2019/09/07/faster-rcnn/foreground.png" alt></p></li><li><p>需要注意，只有与Ground Truth的IOU值超过阈值的RPN Boxes才是前景框。这样做是为了避免给RPN带来<strong>“无望的学习任务”</strong>，即由于ground truth距离太远，而导致学习到的“回归系数”过大。同理，IOU值小于负阈值的RPN Boxes被标记为背景框。</p></li><li><p>对于每一个RPN Boxes，并不是只分：前景、背景，还有一个类别：不关心(don’t care)，即又不是前景又不是背景，这些盒子不包括在RPN损失的计算中。当然，<strong>前景框和背景框也有数量限制</strong>，比如说我各取128个。那么，将会从通过测试的前景框或背景框中，随即将多余出来的框标定为“don’t care”。</p></li><li><p>得到用于计算<strong>RPN Loss</strong>的候选框之后，就可以通过公式来计算出<strong>Target regression coefficients</strong>。<font color="red">这里用于计算位置损失的是“回归系数”，其实在理解Faster-RCNN的过程中，这个损失函数一直在困扰我。到底应该是“回归系数”，还是通过上面的公式变换得到的bbox坐标  (左上角x，左上角y，宽，高) </font>但不管是哪一个，通过代码可以详细了解，而且只是值的选择不同，并不影响了解Faster-RCNN的整体流程。</p></li></ul></li><li><p>通过对RPN Loss的理解，我们知道了Anchor Target Layer的流程，我们最后再总结一下</p><ul><li>Anchor Target Layer的输入<ul><li>RPN网络的两路输出：预测的前景背景分数，回归系数</li><li>通过人工机械生成的RPN Boxes</li><li>Ground Truth Boxes</li></ul></li><li>Anchor Target Layer的输出<ul><li>用于计算RPN Loss的前景和背景框，以及与之对应的类别标签</li><li>目标回归系数</li></ul></li></ul></li></ul><h2 id="5-3-Calculating-Classification-Layer-Loss"><a href="#5-3-Calculating-Classification-Layer-Loss" class="headerlink" title="5.3 Calculating Classification Layer Loss"></a>5.3 Calculating Classification Layer Loss</h2><p>​    我们首先来了解如何计算分类层的损失以及哪些信息用来计算分类损失，以方便后续更容易了解proposal target layer, ROI Pooling layer。</p><ul><li><p>和RPN Loss相似，Classification Layer Loss由两项损失组成</p><ul><li>Classification Loss</li><li>Bounding Box Regression Loss</li></ul><p>但，RPN Layer和Classificaiton Layer最大的区别就是：<strong>RPN层只需要处理两个类别：前景和背景；而分类层需要被训练处理所有的目标类别(+背景)</strong></p></li><li><p><strong>Classification Loss</strong>。分类的损失也是交叉熵，如下图所示。Class Scores是预测类别的得分，行数代表样本数量，列数代表总类别个数。$C_i$是每一行代表的样本真正的类别标签。0是背景类别。最后，通过Cross entropy loss，将上述参数输入，得到最后的损失值。</p></li></ul><p><img src="/2019/09/07/faster-rcnn/classification-layer-loss.png" alt></p><p><img src="/2019/09/07/faster-rcnn/classification-layer-loss-formula.png" alt></p><ul><li><strong>Bounding Box Regression Loss</strong>。边框回归损失和RPN Loss计算方法类似，但是“回归系数”是特定于类别的。神经网络将会<strong>为每个对象类都计算回归系数</strong>，但显然，最后的目标回归系数只适用于正确的类，即与该RPN Box重叠最大的Ground Truth框的类别。在计算损失时，使用一个掩码矩阵，它为每个使用到的RPN Box标记正确的对象类。那么，不正确对象类所对应的回归系数将被忽略，这个损失可用矩阵乘法来做，而不需要for-each循环。</li><li>在了解了Classification Layer如何计算之后，我们需要提供以下变量以计算损失<ul><li>分类网络的输出：预测的类别标签，回归系数</li><li>每个RPN Boxes的类别标签</li><li>目标框的回归系数</li></ul></li></ul><h2 id="5-4-Proposal-Target-Layer"><a href="#5-4-Proposal-Target-Layer" class="headerlink" title="5.4 Proposal Target Layer"></a>5.4 Proposal Target Layer</h2><p>​    在了解了Classificaiton Layer的任务和需求之后，我们再先讨论一下Proposal Target Layer。</p><ul><li><p>Proposal Target Layer要做的就是：从Propsoal Layer输出的一系列ROIs中选择有前途的ROIs。这些ROIs将会被用来在头网络提供的特征图中进行裁剪，然后被传递到网络的剩余部分，用于计算预测类别得分和框回归系数。</p></li><li><p>和Anchor Target Layer类似，如何选择传递到分类层的ROIs很重要，如那些与Ground Truth有明显重叠的ROIs，否则我们将要求分类层学习一个“无望的学习任务”。选择合适的ROIs的原则如下：</p><ul><li>计算所有ROIs与Ground Truth的重叠值(IOU)，将ROIs分为背景和前景。前景ROIs是IOU超过阈值的ROIs，<strong>背景ROIs是重叠值落在最低阈值和最高阈值之间的ROIs</strong>。这是一个“hard negative ”的典型例子，为的是给分类器提供困难的背景例子。</li><li>额外的逻辑保证前景和背景区域总数恒定。如发现背景区域太少，则尝试随机重复一些背景指数来填补数量的不足。</li></ul></li><li><p>当选择到合适的ROIs后，需要计算每个ROI和离之最近的Ground Truth之间的边界框回归系数（<strong>包括背景ROIs，因为这些ROI也存在与之重叠的Ground Truth</strong>,与前面选择背景ROI的原则互相呼应）。这些回归系数将被扩展到所有类，如下图所示。</p><ul><li>一共有N个ROIs的“回归系数”，根据它们与各自Ground Truth重叠大小，自高向下排列，放在列表最末尾的是标记为“背景”的ROIs的“回归系数”。目标回归系数也是N行，但是每行是(类别数 x 4)，根据每个Ground Truth对应的类别标签，制作bbox_targets。真实类别处为目标回归系数(通过ROIs和Ground Truth计算得到)，其余都为0。背景ROIs的的回归系数都是0，所以也不会参与损失的计算。</li><li>bbox_inside_weights作为掩码，大小和bbox_targets一致，但是只在每个ROI正确的类别对应的位置值为1，其余值为0。对于背景ROIs，它的值也全为0。因此，<strong>当计算classification layer损失中的Bounding Box Regression Loss时，只考虑前景区域的回归系数</strong>。而<strong>计算Classification Loss的时候，前景和背景同时都要考虑</strong>。</li></ul><p><img src="/2019/09/07/faster-rcnn/ROIs.png" alt></p></li></ul><p><strong>讲到这里，很多童鞋都会有疑惑，怎么又要选择合适的框ROIs了？RPN那里不是选择了256个合适的RPN Boxes了吗，还分了前景和背景，这里为何又要重复采样，多此一举？其实，初学者都会有这样的疑惑，导致他们对Faster-RCNN的理解止步于此，因为太混乱了，就假装自己看懂了，不多想，我也曾是其中的一员。就算想通过Faster-RCNN的代码来了解详细的流程，结果又被复杂琐碎的代码劝退。</strong><font color="red"><strong>如果有以上疑问，我想下面的解释会让你们茅塞顿开。要始终记住，Faster-RCNN虽然是End-2-End，但它是2-stage，不像SSD、YOLO等是1-stage。2-stage的意思就是，训练分两个阶段：首先训练RPN，达到检测目的；其次训练网络剩余部分，达到识别目的。那么，我们虽然在训练RPN的时候就完成了前景、背景框的筛选，如最后挑选出256个既包括前景又包括背景的RPN Boxes，但它们是用来训练RPN的，都是阶段1做的事情，也就说和阶段2：Classification Layer层的训练是没有关系的。当你的RPN训练的很好的时候，就能够得到比较准确的前景、背景分数及回归系数，这时候我们再开始训练Classificaiton Layer，也就是说开始阶段2了。由于阶段1和阶段2分开了，阶段2也拿不到那256个RPN Boxes用于训练RPN的样本，所以阶段2需要重新采样，经过筛选得到比较好的用于训练Classification Layer的ROIs样本框。</strong></font></p><ul><li>最后我们再总结一下Proposal Target Layer做的事情<ul><li>Proposal Target Layer的输入<ul><li>proposal layer提供的ROIs</li><li>Ground Truth信息</li></ul></li><li>Proposal Target Layer的输出<ul><li>符合重叠标准的前景和背景ROIs</li><li>确定好类别的ROI目标回归系数</li></ul></li></ul></li></ul><h1 id="6-Crop-Pooling"><a href="#6-Crop-Pooling" class="headerlink" title="6. Crop Pooling"></a>6. Crop Pooling</h1><ul><li><p>Proposal target layer为我们提供了多个有效的，用于训练的ROIs来计算相关的类别标签和回归系数。下一步就是从头网络生成的特征图中抽取与这些ROIs相关的特征，并用剩余网络的来得到每个ROI所代表的：物体类别的概率分布，回归系数。<strong>Crop Pooling layer</strong>的工作就是从卷积特征图中进行特征抽取。</p></li><li><p>在crop pooling后的关键想法是<strong>Spatial Transformation Networks</strong>来抽取特征。CNN提取特征时，通常需要考虑输入样本的局部性、平移不变性、缩小不变性及旋转不变性，即图像的裁剪、平移、缩放、旋转。而实现这些方法就是对图像进行空间坐标变换，如<strong>仿射变化</strong>。<strong>SNT</strong>以一种统一的结构，自适应实现这些变化。SNT不需要关键点的标定，能根据分类或者其他任务自适应地将数据进行空间变换和对齐。幸运的是，Pytorch提供了两个API来实现：torch.nn.functional.affine_grid，torch.nn.functional.grid_sample。</p></li><li><p>Crop Pooling地步骤如下</p><ul><li>ROIs的坐标表示是相对于原图尺寸地(800 x 600)，为了将这些坐标带入输出特征图的空间上，我们必须要将它们除以步长（一般为16）。</li><li>为了使用上述的API，我们需要仿射变换矩阵。我们需要目标特征图上的x,y维度点的数量，这个由配置参数cfg.POOLING_SIZE提供。因此，在crop pooling中，非正方形地ROIs会被用来在特征图上裁剪区域，并转变成固定大小的正方形窗口。这种转换必须要做，因为Crop Pooling地输出将会被进一步传递到卷积和全连接层，而它们需要固定维度特征。</li></ul><p><img src="/2019/09/07/faster-rcnn/crop-pooling.png" alt></p></li></ul><h1 id="7-Classification-Layer"><a href="#7-Classification-Layer" class="headerlink" title="7. Classification Layer"></a>7. Classification Layer</h1><ul><li><p>crop pooling层接收proposal target layer的ROIs及头网络输出的特征图作为输入，输出平方特征图。这个特征图将被传入到4层ResNet在空间维度上做平均池化，对于每个ROI，结果将会是一维特征向量，过程如下：</p><p><img src="/2019/09/07/faster-rcnn/Classification-Layer.png" alt></p></li><li><p>特征图被传递到两个全连接层：bbox_pred_net，cls_score_net。cls_score_net为每个bounding box产生类别分数（可通过softmax转变成概率矩阵）。bbox_pred_net生成特定类别下的边框回归系数，这些回归系数将与proposal target layer的原始边框相结合，生成最终的边界框。</p><p><img src="/2019/09/07/faster-rcnn/Classification-Layer2.png" alt="zui&#39;ha"></p></li><li><p>最好回忆一下两组回归系数之间的差异：RPN网络产生的，classification network产生。</p><ul><li>第一组RPN的回归系数用来训练RPN网络来得到更好的前景盒子（更加紧密的围绕目标边界）。此时目标回归系数，即由anchor得到的RPN Boxes与其最匹配的Ground Trouth计算得到。</li><li>第二组回归系数由Classification Layer生成。这些系数是特定于类别的，即每个对象类都会产生一组回归系数，最后选择正确类别的系数即可。目标回归系数将在Proposal Target Layer得到，也是通过Ground Truth匹配得到，但是需要制作成特定的形式。<strong>值得注意的是，由于仿射变化SNT使得classification network是在正方形的特征图上操作的，故可能会造成回归系数也会收到影响的误解。然而，由于回归系数对无剪切的仿射变化是不变的，因此Proposal target layer和Classification Layer之间的回归系数依然可以进行比较，作为有效的学习信号。</strong></li></ul></li><li><p>值得注意的是，训练Classification Layer时，错误的梯度也会反向传播到RPN网络。这是因为在Crop Pooling时，使用的ROIs本身就是网络的输出，是RPN网络生成回归系数应用到RPN Box的结果。故在反向传播期间，误差梯度将通过crop pooling传播到RPN层。计算和应用这些梯度很难实现，但Pytorch提供了很好的crop pooling API，方便了细节的处理。</p></li></ul><h1 id="8-实现细节：推理"><a href="#8-实现细节：推理" class="headerlink" title="8. 实现细节：推理"></a>8. 实现细节：推理</h1><ul><li><img src="/2019/09/07/faster-rcnn/Inference.png" alt></li><li>Anchor target和Proposal Target Layer不参与此过程。RPN网络将RPN Boxes分类成：前景、背景，并生成良好的框回归系数。Proposal Layer仅仅将回归系数应用到RPN Boxes并进行非极大值抑制来消除大量重叠的框，最后的剩余的框将被送到Classification Layer生成类得分及基于类别的边框回归系数。</li><li><img src="/2019/09/07/faster-rcnn/Inference1.png" alt></li></ul><p>​        红框显示的是排名前6的RPN Boxes。绿框表示应用RPN Boxes的回归系数后的框，显然绿框更加适合目标。在应用回归系数后，矩形依然是矩形，即存在显著的重叠，这种冗余需要通过非极大值抑制来解决。</p><ul><li><img src="/2019/09/07/faster-rcnn/Inference2.png" alt></li></ul><p>​        红色框表示非极大值抑制之前的5个框，绿色框表示非极大值抑制后的5个框。通过抑制重叠框，其他框（得分较低的框）就有机会向上移动了。</p><ul><li><img src="/2019/09/07/faster-rcnn/Inference3.png" alt></li></ul><p>​        通过最后的分类分数数组(dim:n, 21)，我们选择分数最大的作为类别。并选择该类别所对应的框回归因子来调整框，它比其他因子更加适合调整这个特定的类别。最后的检测结果如下图所示：</p><ul><li><img src="/2019/09/07/faster-rcnn/final-result.png" alt></li></ul><h1 id="9-参考资料"><a href="#9-参考资料" class="headerlink" title="9. 参考资料"></a>9. 参考资料</h1><ul><li><a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/</a></li><li><a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31426458</a></li><li><a href="https://blog.csdn.net/u011961856/article/details/77920970" target="_blank" rel="noopener">https://blog.csdn.net/u011961856/article/details/77920970</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Faster RCNN 源码理解</title>
      <link href="/2019/09/03/fast-rcnn-code/"/>
      <url>/2019/09/03/fast-rcnn-code/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Anchor-Generation-Layer"><a href="#1-Anchor-Generation-Layer" class="headerlink" title="1. Anchor Generation Layer"></a>1. Anchor Generation Layer</h1><p>对于生成anchors的源码理解主要来源于两个代码</p><ul><li>RBG大神的caffe源码：<a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/py-faster-rcnn</a></li><li>Github上复现的pytorch源码：<a href="https://github.com/chenyuntc/simple-faster-rcnn-pytorch" target="_blank" rel="noopener">https://github.com/chenyuntc/simple-faster-rcnn-pytorch</a></li></ul><p>由于两种方法生成anchors的技巧不同，故分开讨论，并主要以RBG大神的代码为主，讲解anchors的生成原理与生成技巧。</p><h2 id="1-1-Caffe源码"><a href="#1-1-Caffe源码" class="headerlink" title="1.1 Caffe源码"></a>1.1 Caffe源码</h2><ul><li><p>首先，解释一下，重要的参数</p><ul><li>base_size=16，由于原图经过卷积池化后得到的特征图是原图的$\frac{1}{16}$，故用于采样anchor的特征图上的一个cell就相当于原图的$16 \times 16$区域。</li><li>ratios=[0.5, 1, 2]，固定anchor面积下的长宽比，即$[1:2 \quad 1:1 \quad 2:1]$</li><li>scales=[8, 16, 32]，即将anchors放大的倍数，具体在哪里用到会在后面详细解释</li></ul></li><li><p>其次，我们根据RBG大神的源码走一遍anchors生成的流程</p><ul><li><pre class=" language-lang-python"><code class="language-lang-python">def generate_anchors(base_size=16, ratios=[0.5, 1, 2],                     scales=2**np.arange(3, 6)):    """    Generate anchor (reference) windows by enumerating aspect ratios X    scales wrt a reference (0, 0, 15, 15) window.    """    base_anchor = np.array([1, 1, base_size, base_size]) - 1    ratio_anchors = _ratio_enum(base_anchor, ratios)    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)                         for i in xrange(ratio_anchors.shape[0])])    return anchors</code></pre><ul><li><strong>generate_anchors()</strong>函数是一切的开端，首先定义了base_anchor，由于图像的坐标以左上角为原点且值为(0, 0)，故base_anchor的坐标(xmin, ymin, xmax, ymax)为(0, 0, 15, 15)。</li><li>其次，调用<strong>_ratio_enum()</strong>函数如下</li></ul></li><li><pre class=" language-lang-python"><code class="language-lang-python">def _ratio_enum(anchor, ratios):    """    Enumerate a set of anchors for each aspect ratio wrt an anchor.    """    w, h, x_ctr, y_ctr = _whctrs(anchor)    size = w * h    size_ratios = size / ratios    ws = np.round(np.sqrt(size_ratios))    hs = np.round(ws * ratios)    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)    return anchors</code></pre><ul><li>为了计算w, h, x_ctr, y_ctr，又调用了<strong>_whctrs()</strong>函数，如下所示</li></ul></li><li><pre class=" language-lang-python"><code class="language-lang-python">def _whctrs(anchor):    """    Return width, height, x center, and y center for an anchor (window).    """    w = anchor[2] - anchor[0] + 1    h = anchor[3] - anchor[1] + 1    x_ctr = anchor[0] + 0.5 * (w - 1)    y_ctr = anchor[1] + 0.5 * (h - 1)    return w, h, x_ctr, y_ctr</code></pre><ul><li><strong>_whctrs()</strong>函数的功能就是传入参数为（左上角x，左上角y，右上角x，右上角y），将其转换为（宽， 高， 中心坐标x，中心坐标y）</li></ul></li><li><p>让我们回到<strong>_ratio_enum()</strong>函数</p><ul><li>得到base_anchor的（宽， 高， 中心坐标x，中心坐标y），经过计算值为（16, 16, 7.5, 7.5）</li><li>size = w x h = 16 x 16 = 256</li><li>size_ratios = $\frac{256}{[0.5 \quad 1 \quad 2]}$ = $[512, 256, 128]$</li><li>对size_ratios开根号，再四舍五入，得到 ws = [23, 16, 11]</li><li>ws和ratios相乘就得到了 hs = [12, 16, 22]</li><li><strong>ws和hs其实是相同面积下，anchor不同长宽比条件下，得到的长和宽。但由于四舍五入的缘故，ws x hs的面积值不一定相等</strong></li><li>得到上面的变量值后，又调用了<strong>_mkanchors()</strong>函数返回计算后的anchors，函数如下</li></ul></li><li><pre class=" language-lang-python"><code class="language-lang-python">def _mkanchors(ws, hs, x_ctr, y_ctr):    """    Given a vector of widths (ws) and heights (hs) around a center    (x_ctr, y_ctr), output a set of anchors (windows).    """    ws = ws[:, np.newaxis]    hs = hs[:, np.newaxis]    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),                         y_ctr - 0.5 * (hs - 1),                         x_ctr + 0.5 * (ws - 1),                         y_ctr + 0.5 * (hs - 1)))    return anchors</code></pre><ul><li><p>根据上面的代码，会得到如下的计算公式</p><script type="math/tex; mode=display">7.5 - \frac{1}{2}\left[\begin{matrix} 22 \\ 15 \\ 10 \end{matrix}\right] = \left[\begin{matrix} -3.5\\ 0\\ 2.5\end{matrix}\right]</script><script type="math/tex; mode=display">7.5 - \frac{1}{2}\left[\begin{matrix} 12\\ 16\\ 22\end{matrix}\right] = \left[\begin{matrix} 1.5\\ 0\\ -3\end{matrix}\right]</script><script type="math/tex; mode=display">7.5 + \frac{1}{2}\left[\begin{matrix} 22 \\ 15 \\ 10 \end{matrix}\right] = \left[\begin{matrix} 18.5\\ 15\\ 12.5\end{matrix}\right]</script><script type="math/tex; mode=display">7.5 + \frac{1}{2}\left[\begin{matrix} 12\\ 16\\ 22\end{matrix}\right] = \left[\begin{matrix} 13\\ 15\\ 18\end{matrix}\right]</script></li><li><p>最后anchors的值为$\left[\begin{matrix} -3.5 &amp; 1.5 &amp; 18.5 &amp; 13.5\\ 0 &amp; 0 &amp; 15 &amp; 15\\ 2.5 &amp; -3 &amp; 12.5 &amp; 18\end{matrix}\right]$</p></li><li><p><strong>这里得到的是，面积都为256下，以（7.5， 7.5）为中心坐标的，不同长宽比例下的anchor坐标。根据坐标的计算公式，可以发现，都是以7.5为中心坐标减去一半的长或宽，那么得到的是新的（左上角x，左上角y，右上角x，右上角y）形式的坐标值。为什么坐标会是负数，因为左上角坐标超出了图片范围，故为负数。</strong></p></li></ul></li><li><p>得到以上anchors后，我们直接返回到<strong>generate_anchors()</strong>函数</p><ul><li>通过一系列函数的调用，我们得到了ratio_anchors的值，即$\left[\begin{matrix} -3.5 &amp; 1.5 &amp; 18.5 &amp; 13.5\\ 0 &amp; 0 &amp; 15 &amp; 15\\ 2.5 &amp; -3 &amp; 12.5 &amp; 18\end{matrix}\right]$</li><li>最后一步，就是调用<strong>_scale_enum()</strong>函数，得到不同scale下，不同长宽比例的anchors。目前的scale为[8, 16, 32]，对于每一个scale都要调用<strong>_scale_enum()</strong>函数；传入不同长宽比、以(7.5, 7.5)为中心坐标的anchors（<strong>即ratio_anchors的每一行</strong>），每次返回3组变换尺度后的anchors，故最后会有9组anchors。<strong>_scale_enum()</strong>函数如下</li></ul></li><li><pre class=" language-lang-python"><code class="language-lang-python">def _scale_enum(anchor, scales):    """    Enumerate a set of anchors for each scale wrt an anchor.    """    w, h, x_ctr, y_ctr = _whctrs(anchor)    ws = w * scales    hs = h * scales    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)    return anchors</code></pre><ul><li>我们以$[-3.5 \quad 1.5 \quad 18.5 \quad 13.5]$为例</li><li>调用<strong>_whctrs()</strong>函数，得到中心坐标表示，w, h, x_ctr, y_ctr = $[23 \quad 12 \quad 7.5\quad 7.5]$</li><li><script type="math/tex">ws = 23 \times \left[\begin{matrix} 8\\ 16\\ 32\end{matrix}\right] = \left[\begin{matrix} 184\\ 368\\ 736\end{matrix}\right]</script>，其实是宽为23的情况下，放大宽的值</li><li><script type="math/tex">hs = 12 \times \left[\begin{matrix} 8\\ 16\\ 32\end{matrix}\right] = \left[\begin{matrix} 96\\ 192\\ 384\end{matrix}\right]</script>，其实是长为12的情况下，放大长的值</li><li>由于中心坐标都是(7.5, 7.5)不变，但宽和高的值变了，所以新得到的anchors坐标需要再次调用<strong>_mkanchors()</strong>对坐标进行调整。在新的长和宽下，仍然以(7.5, 7.5)为中心坐标。</li><li>最后计算得到的anchors坐标为$\left[\begin{matrix} -83 &amp; -39 &amp; 100 &amp; 56\\ -175 &amp; -87 &amp; 192 &amp; 104\\ -359 &amp; -183 &amp; 376 &amp; 200\end{matrix}\right]$</li></ul></li></ul></li><li><p>至此，RBG大神生成Anchors的方法就介绍完毕</p></li></ul><h2 id="1-2-Pytorch源码"><a href="#1-2-Pytorch源码" class="headerlink" title="1.2 Pytorch源码"></a>1.2 Pytorch源码</h2><ul><li><p>Pytorch版本就不详细解释了，直接上代码，简单易懂</p><pre class=" language-lang-python"><code class="language-lang-python">def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2],                         anchor_scales=[8, 16, 32]):    """    Returns:        ~numpy.ndarray:        An array of shape :math:`(R, 4)`.        Each element is a set of coordinates of a bounding box.        The second axis corresponds to        :math:`(y_{min}, x_{min}, y_{max}, x_{max})` of a bounding box.    """    py = base_size / 2.    px = base_size / 2.    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4),                           dtype=np.float32)    for i in six.moves.range(len(ratios)):        for j in six.moves.range(len(anchor_scales)):            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])            index = i * len(anchor_scales) + j            anchor_base[index, 0] = py - h / 2.            anchor_base[index, 1] = px - w / 2.            anchor_base[index, 2] = py + h / 2.            anchor_base[index, 3] = px + w / 2.    return anchor_base</code></pre><ul><li>参数和caffee一致，不同点在于，计算anchor_base的方式</li><li>这里的anchor_base没有-1</li><li>调用了两个循环，即遍历9次，每次得到一个anchors的坐标</li><li>计算的公式很奇怪，为何对ratios开根号，应该是有奇怪的转换公式的</li><li>最后，是直接求anchor_base的每一个坐标，以中心坐标为基准，计算(ymin, xmin, ymax, xmax)</li></ul></li></ul><p><strong>未完待续~~~</strong></p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ROI Align理解</title>
      <link href="/2019/08/30/roi-align/"/>
      <url>/2019/08/30/roi-align/</url>
      
        <content type="html"><![CDATA[<h1 id="1-几个问题"><a href="#1-几个问题" class="headerlink" title="1. 几个问题"></a>1. 几个问题</h1><ul><li>Mask R-CNN中为何要使用ROI Align取代Faster R-CNN中的ROI Pooling</li><li>何为线性插值，何为双线性插值？插值的意义？</li><li>ROI Align的具体步骤</li></ul><h1 id="2-ROI-Pooling"><a href="#2-ROI-Pooling" class="headerlink" title="2. ROI Pooling"></a>2. ROI Pooling</h1><ul><li><p>在Faster RCNN中（不懂Faster RCNN的点击<a href="https://godwriter.github.io/2019/08/27/Faster-RCNN/#more">这里</a>），通过Achor生成的RPN Boxes经过一系列筛选后，会得到一系列ROIs用于后面的识别等工作。但是用于识别的网络需要输入固定维度的图像特征，而ROIs的大小不一致，故抽取的图像特征也不一致。为此，在Faster RCNN中，使用ROI Pooling来解决这个问题。</p><p><img src="/2019/08/30/roi-align/ROI-pooling.png" alt></p></li><li><p>首先，我们理解一下ROI Pooling的基本思路，如上图所示</p><ul><li>假设我们有一个8*9的feature map，2个ROI，且要求输出的固定维度大小为2x2</li><li>我们经过<strong>坐标的变化</strong>，得到在特征图上的投影坐标（左上角x，左上角y，宽度，高度）为(0, 3, 3, 4)，(4, 1, 4, 4)，如图中的红框</li><li>既然需要得到2x2的固定特征维度，那么我们需要对红框内的特征图做Pooling，Pooling方式也如图中所示，以及最后的结果，都一目了然。</li><li><strong>有的特征图在划分的时候不能整除，那就只能采取图中做法，类似于加了个zero-pooling</strong></li></ul></li><li><p>其次，我们来说一下ROI Pooling存在的问题。我们在上面提到了”坐标变换“这四个字，我也特地加粗了，其实问题就出在这里。</p><ul><li>假设我们使用VGG16, feat_stride=32来提取图片特征。若原图大小为800x800，那么最后输出的特征图xxxxxxxxx……</li></ul></li></ul><p><strong>未完待续~~~</strong></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li><a href="https://blog.csdn.net/u011436429/article/details/80279536" target="_blank" rel="noopener">https://blog.csdn.net/u011436429/article/details/80279536</a></li><li><a href="https://www.cnblogs.com/wangyong/p/8523814.html" target="_blank" rel="noopener">https://www.cnblogs.com/wangyong/p/8523814.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo下Next主题的优化1</title>
      <link href="/2019/08/28/hexo-next-1/"/>
      <url>/2019/08/28/hexo-next-1/</url>
      
        <content type="html"><![CDATA[<h1 id="1-版本概览"><a href="#1-版本概览" class="headerlink" title="1. 版本概览"></a>1. 版本概览</h1><ul><li>Node.js版本：v10.16.3</li><li>Hexo版本：v3.9.0</li><li>Next版本：v5.1.4</li></ul><h1 id="2-首页不显示全文"><a href="#2-首页不显示全文" class="headerlink" title="2. 首页不显示全文"></a>2. 首页不显示全文</h1><ul><li><p>希望达到效果：首页显示文章列表，列表里的每一篇文章只显示预览，不显示全文。效果如下图</p><p><img src="/2019/08/28/hexo-next-1/notlong.jpg" alt></p></li><li><p>解决方法如下</p><ul><li><p>进入themes/next目录</p></li><li><p>打开_config.yml文件</p></li><li><p>搜索“auto_excerpt”，找到如下部分</p><pre class=" language-lang-yaml"><code class="language-lang-yaml">auto_excerpt:  enable: true  length: 250</code></pre></li><li><p>把enable改为true，length设置你想显示的长度</p></li></ul></li><li><p>问题即可迎刃而解</p></li></ul><h1 id="3-首页添加头像"><a href="#3-首页添加头像" class="headerlink" title="3. 首页添加头像"></a>3. 首页添加头像</h1><ul><li><p>希望达到效果：首页能够有自己的头像显示，效果如下图</p><p><img src="/2019/08/28/hexo-next-1/touxiang.jpg" alt></p></li><li><p>解决步骤如下</p><ul><li><p>进入themes/next目录</p></li><li><p>打开_config.yml文件</p></li><li><p>搜索”avatar”，修改成如下</p><pre class=" language-lang-yaml"><code class="language-lang-yaml">avatar: /images/avatar.gif</code></pre></li><li><p>将你的图像放到\themes\next\source\images文件夹下，命名为：avatar.gif，若已经有这个文件直接替换即可。</p></li></ul></li><li><p>问题即可迎刃而解</p></li></ul><h1 id="4-开启版权声明"><a href="#4-开启版权声明" class="headerlink" title="4. 开启版权声明"></a>4. 开启版权声明</h1><ul><li><p>希望达到效果：每篇文章末尾会有版权声明，效果如下图</p><p><img src="/2019/08/28/hexo-next-1/banquan.jpg" alt></p></li><li><p>解决步骤如下</p><ul><li><p>进入themes/next目录</p></li><li><p>打开_config.yml文件</p></li><li><p>搜索“post_copyright”，修改成如下</p><pre class=" language-lang-yaml"><code class="language-lang-yaml">post_copyright:  enable: true  license: CC BY-NC-SA 3.0  license_url: https://creativecommons.org/licenses/by-nc-sa/3.0/</code></pre></li></ul></li><li><p>问题即可迎刃而解</p></li></ul><h1 id="5-添加github标志"><a href="#5-添加github标志" class="headerlink" title="5. 添加github标志"></a>5. 添加github标志</h1><ul><li><p>希望达到效果：自己博客的右上角有一个github的小标志，点击可以直达主页，效果如下图</p><p><img src="/2019/08/28/hexo-next-1/icon.jpg" alt></p></li><li><p>实现步骤如下</p><ul><li><p>点击<a href="http://tholman.com/github-corners/" target="_blank" rel="noopener">这里</a>挑选自己喜欢的样式，并复制代码。 例如，我是复制的这一个：</p><p><img src="/2019/08/28/hexo-next-1/iconwebsite.jpg" alt></p></li><li><p>进入themes/next/layout/_layout.swig文件中，并搜索如下代码</p><pre class=" language-lang-html"><code class="language-lang-html"><div class="headband"></div></code></pre></li><li><p>将复制的代码放到该代码行下，并把href改为自己的github地址</p></li></ul></li><li><p>问题即可迎刃而解</p></li></ul><h1 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a>6. 参考资料</h1><ul><li><p><a href="https://www.jianshu.com/p/393d067dba8d" target="_blank" rel="noopener">https://www.jianshu.com/p/393d067dba8d</a></p></li><li><p><a href="https://jingyan.baidu.com/article/d5a880ebeb42f113f147cce5.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/d5a880ebeb42f113f147cce5.html</a></p></li><li><p><a href="https://blog.csdn.net/weixin_43971764/article/details/96478950" target="_blank" rel="noopener">https://blog.csdn.net/weixin_43971764/article/details/96478950</a></p></li><li><p><a href="https://blog.csdn.net/fly_wt/article/details/86674138" target="_blank" rel="noopener">https://blog.csdn.net/fly_wt/article/details/86674138</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> next </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
