<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,">










<meta name="description" content="1. RL几个应用 Learning to play AlphaGo   Supervised（监督学习的方式） 从老师那里学习，即通过打标签的方式学习 比如看棋谱学，但棋谱是人下的，人下的那一步就是最优的吗？   Reinforement Learning 从经验中学习，其实人可能都不知道哪一步是最优的   AlphaGo的学习方式 先做Supervised Learning，从高手棋谱中学习">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning 1">
<meta property="og:url" content="https://godwriter.github.io/2020/05/06/reinforcement-learning1/index.html">
<meta property="og:site_name" content="GodWriter">
<meta property="og:description" content="1. RL几个应用 Learning to play AlphaGo   Supervised（监督学习的方式） 从老师那里学习，即通过打标签的方式学习 比如看棋谱学，但棋谱是人下的，人下的那一步就是最优的吗？   Reinforement Learning 从经验中学习，其实人可能都不知道哪一步是最优的   AlphaGo的学习方式 先做Supervised Learning，从高手棋谱中学习">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2020-05-26T13:33:18.976Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning 1">
<meta name="twitter:description" content="1. RL几个应用 Learning to play AlphaGo   Supervised（监督学习的方式） 从老师那里学习，即通过打标签的方式学习 比如看棋谱学，但棋谱是人下的，人下的那一步就是最优的吗？   Reinforement Learning 从经验中学习，其实人可能都不知道哪一步是最优的   AlphaGo的学习方式 先做Supervised Learning，从高手棋谱中学习">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://godwriter.github.io/2020/05/06/reinforcement-learning1/">





  <title>Reinforcement Learning 1 | GodWriter</title>
  








<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <a href="https://github.com/GodWriter" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">GodWriter</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://godwriter.github.io/2020/05/06/reinforcement-learning1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="GodWriter">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GodWriter">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Reinforcement Learning 1</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-05-06T19:33:27+08:00">
                2020-05-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Reinforcement Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1-RL几个应用"><a href="#1-RL几个应用" class="headerlink" title="1. RL几个应用"></a>1. RL几个应用</h1><blockquote>
<p>Learning to play AlphaGo</p>
</blockquote>
<ul>
<li>Supervised（监督学习的方式）<ul>
<li>从老师那里学习，即通过打标签的方式学习</li>
<li>比如看棋谱学，但棋谱是人下的，人下的那一步就是最优的吗？</li>
</ul>
</li>
<li>Reinforement Learning<ul>
<li>从经验中学习，其实人可能都不知道哪一步是最优的</li>
</ul>
</li>
<li>AlphaGo的学习方式<ul>
<li>先做Supervised Learning，从高手棋谱中学习</li>
<li>通过Supervised Learning训练出两个模型，他们互相博弈</li>
</ul>
</li>
</ul>
<blockquote>
<p>Learning a Chat-Bot</p>
</blockquote>
<ul>
<li>如何评判生成句子的好坏是关键问题，总不能一直“See you”循环下去。</li>
<li>预测将来可能出现的做法<ul>
<li>在RL中引入生成对抗网络</li>
<li>判别生成的句子是否符合真实数据的分布</li>
</ul>
</li>
</ul>
<blockquote>
<p>Playing Video Game</p>
</blockquote>
<ul>
<li>两个常用的游戏平台<ul>
<li>Gym</li>
<li>Universe</li>
</ul>
</li>
<li>episode，一局游戏，从开始到结束</li>
</ul>
<h1 id="2-RL的概念"><a href="#2-RL的概念" class="headerlink" title="2. RL的概念"></a>2. RL的概念</h1><blockquote>
<p>RL的难点</p>
</blockquote>
<ul>
<li><p>Reward Delay 奖励延迟</p>
<p>比如，打飞机游戏中，只有开火才会拿到Reward，但左右操作不会。但左右操作又会影响最后得分的高低，毕竟就算一直开火而不躲避也会导致被子弹击中从而导致游戏结束。<strong>所以，这就需要机器能够合理规划以获得长期的利益。</strong></p>
</li>
<li><p>Agent采取的行为会影响之后的行为</p>
<p>Agent需要充分的探索它所处的环境。还是以打飞机游戏为例，若是Agent从来都没有尝试过开火，那么它永远无法得到高分。</p>
</li>
</ul>
<blockquote>
<p>RL常用的做法</p>
</blockquote>
<ul>
<li>Policy-Based，主要目的是学习一个Actor</li>
<li>Value-Based，主要目的是学习一个Critic</li>
<li>那现在比较流行的做法是：Actor + Critic</li>
</ul>
<h1 id="3-RL三个步骤"><a href="#3-RL三个步骤" class="headerlink" title="3. RL三个步骤"></a>3. RL三个步骤</h1><blockquote>
<p>Neural Network as Actor</p>
</blockquote>
<ul>
<li>Input：机器的观察，Vector或者是Matrix</li>
<li>OutPut：输出层每一个神经元代表一个动作</li>
</ul>
<blockquote>
<p>Goodness of Actor 1</p>
</blockquote>
<ul>
<li><p>给定一个Actor $\pi<em>\theta(s)$，神经网络就是Actor，参数为$\theta$ 。使用$\pi</em>\theta(t)$来玩游戏，那么总体的Reward如下：</p>
<script type="math/tex; mode=display">Total~Reward : R_\theta = \sum_{t=1}^Tr_t</script></li>
<li><p>由于环境是一直在变化的（输入会变），比如打游戏玩家不同的位置、画面等，所以就算是同一个Actor玩千百次游戏最后得到的$R<em>\theta$也会不一样。而强化学习的目的是最大化Reward从而选择出最佳的Actor，那么直接最大化$R</em>\theta$是没有意义的，而应该最大化所有$R_\theta$的期望。</p>
</li>
<li><p>定义$\bar{R}<em>\theta$为$R</em>\theta$的期望值，所以最终的目的应该是最大化$\bar{R}_\theta$。当玩的所有局获得的Reward期望最大时，就说明这个Actor整体上达到了较好的能力。（可用随机放回采样钞票来类比，若是整体期望很大，那么盒子里钞票值普遍都比较大，即Reward普遍比较大）。</p>
</li>
</ul>
<blockquote>
<p>Goodness of Actor 2</p>
</blockquote>
<ul>
<li><p>将一个episode考虑为一个$\tau$，即一场游戏的开始到结束这个过程。这个过程有千百种，有些经常出现，有些不太出现。$\tau$的定义如下：</p>
<script type="math/tex; mode=display">\tau = \{s_1, a_1, r_1, s_2, a_2, r_2, ..., s_T, a_T, r_T\}</script><p>其中$s_i$代表当前的状态，$a_i$代表采取的动作，$r_i$代表收获的Reward。</p>
</li>
<li><p>当一个Actor开始玩游戏的时候，每一个$\tau$都有一定的概率出现，我们用$P(\tau|\theta)$来代表不同的$\tau$出现的概率。那么所有$R_\theta$的期望为：</p>
<script type="math/tex; mode=display">\bar{R}_\theta = \sum_{\tau}R(\tau)P(\tau|\theta)</script><p>这个公式的理解方式类比于离散随机变量求期望：假设可以穷举所有的$\tau$，那么$\bar{R}_\theta$应该为每种$\tau$出现的机率乘上该$\tau$能够获取的总体Reward的总值。</p>
</li>
<li><p>对$\bar{R}<em>\theta$的进一步思考：由于总体的目标是希望选择的Actor在不同的$\tau$下都能够获得尽可能高的$Reward$，那么使得$\bar{R}</em>\theta$最大的那个Actor就是我们所需要的。</p>
</li>
<li><p>由于前穷举所有的$\tau$可能性很低，那么只能用一个Actor玩N场游戏得到$\{\tau^1, \tau^2, …, \tau^3\}$来类比所有的$\tau$。也就相当于从$P(\tau|\theta)$中采样N次，那么每个$\tau$出现的概率应该为$\frac{1}{N}$，所以$\bar{R}_\theta$近似为：</p>
<script type="math/tex; mode=display">\bar{R}_\theta = \sum_tR(\tau)P(\tau|\theta) \approx \frac{1}{N}\sum_1^NR(\tau^n)</script><p>相当于把N个$\tau$的$R(\tau)$求和并取平均。</p>
</li>
</ul>
<blockquote>
<p>Pick the best Actor 1</p>
</blockquote>
<ul>
<li><p>根据前文，已将问题转化为：</p>
<script type="math/tex; mode=display">\theta^* = arg~\underset{\theta}{max}\bar{R}_\theta = arg~\underset{\theta}{max}~\sum_{\tau}R(\tau)P(\tau|\theta)</script></li>
<li><p>那么就可以采用梯度上升来更新模型的参数</p>
<ul>
<li>Start with $\theta^0$</li>
<li>$\theta^1 = \theta^0 + \eta\nabla\bar{R}_{\theta^0}$</li>
<li>$\theta^2 = \theta^1 + \eta\nabla\bar{R}_{\theta^1}$</li>
<li>……</li>
</ul>
<p>其中$\nabla\bar{R}<em>\theta = \left[\begin{matrix} \part\bar{R}</em>\theta/\part W<em>1 \\ \part\bar{R}</em>\theta/\part W<em>2 \\ … \\ \part\bar{R}</em>\theta/\part b_1\end{matrix}\right]$</p>
</li>
</ul>
<blockquote>
<p>Pick the best Actor 2</p>
</blockquote>
<p>那么$\nabla\bar{R}_\theta$到底该怎么求呢？</p>
<ul>
<li><p>根据$\bar{R}_\theta$的公式，可以得到：</p>
<script type="math/tex; mode=display">\nabla\bar{R}_\theta = \sum_\tau R(\tau)\nabla P(\tau|\theta)</script><ul>
<li>其中，$R(\tau)$和$\theta$并没有关系，所以不需要求梯度。因为Reward是环境给的，比如游戏中的计分板，我们不需要计分的具体过程和规则，只要拿到值就可以了。</li>
<li>所以即使$R(\tau)$不可微，不知道其具体的$function$也没有关系，只要给了输入能够给输出就行。</li>
</ul>
</li>
<li><p>再进一步看上面的式子，可以推导出：</p>
<script type="math/tex; mode=display">\nabla\bar{R}_\theta = \sum_\tau R(\tau)\nabla P(\tau|\theta) \\ = \sum_\tau R(\tau)P(\tau|\theta)\frac{\nabla P(\tau|\theta)}{P(\tau|\theta)} \\ \approx \frac{1}{N}\sum_{n=1}^N R(\tau^n) \nabla\log P(\tau^n|\theta)</script><p>约等于已在上文解释，因为无法穷取所有的$\tau$，故只能采样类比。那么每个$\tau$出现的概率应该为$\frac{1}{N}$，可以提到求和前面。</p>
</li>
</ul>
<blockquote>
<p>Pick the best Actor 3</p>
</blockquote>
<p>根据上文的推导，问题就转化为了$\nabla\log P(\tau^n|\theta) = ?$，那么该怎么求呢？</p>
<ul>
<li><p>前文定义$\tau = \{s_1, a_1, r_1, s_2, a_2, r_2, …, s_T, a_T, r_T\}$，故可以得到：</p>
<script type="math/tex; mode=display">P(\tau|\theta) = P(s_1)P(a_1|s_1, \theta)P(r_1,s_2|s_1, a_1)P(a_2|s_2, \theta)...</script><ul>
<li>$P(s_1)$代表游戏开始画面出现的几率</li>
<li>$P(a_1|s_1, \theta)$代表$\theta_1$下的Actor，在$s_1$状态下，取动作$a_1$的机率</li>
<li>$P(r_1,s_2|s_1, a_1)$代表，在$s_1$状态下采取动作$a_1$，获取$r_1$并指向状态$s_2$的概率</li>
</ul>
</li>
<li><p>那么，上面的式子可以进一步写为：</p>
<script type="math/tex; mode=display">P(\tau|\theta) = P(s_1)\prod_{t=1}^TP(a_t|s_t, \theta)P(r_t, s_{t+1}|s_t, a_t) \\ = \log P(s_1) + \sum_{t=1}^T \log P(a_t|s_t,\theta) + \log P(r_t, s_{t+1}|s_t, a_t)</script><p>其中$\log P(s<em>1)$和$\log P(r_t, s</em>{t+1}|s_t, a_t)$都是与$\theta$无关的项，与游戏环境相关。</p>
</li>
<li><p>于是，可以得到：</p>
<script type="math/tex; mode=display">\nabla \log P(\tau|\theta) = \sum_{t=1}^T \nabla \log P(a_t|s_t, \theta)</script></li>
</ul>
<blockquote>
<p>Pick the best Actor 4</p>
</blockquote>
<p>根据上文的推导，最终的问题可以转化为如下式子：</p>
<script type="math/tex; mode=display">\nabla \bar{R}_\theta \approx \frac{1}{N}\sum_{n=1}^N R(\tau^n) \nabla\log P(\tau^n|\theta) \\ = \frac{1}{N}\sum_{n=1}^N R(\tau^n) \sum_{t=1}^{T_n} \nabla \log P(a_t^n|s_t^n, \theta) \\ = \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n)\nabla \log P(a_t^n|s_t^n, \theta)</script><p>最后得到的一项式子是非常直觉的，可以体现学习过程中很多重要的信息，将在下面一一列出。</p>
<ul>
<li><p>机器在$\tau^n$过程中，在$s_t^n$的状态下采取行动$a_t^n$导致</p>
<ul>
<li>$R(\tau^n)$为正，则希望这个$a_t^n$出现的机率越大越好，需调整$\theta$来增加$P(a_t^n|s_t^n)$</li>
<li>$R(\tau^n)$为负，则希望这个$a_t^n$出现的机率越小越好，需调整$\theta$来降低$P(a_t^n|s_t^n)$</li>
</ul>
</li>
<li><p>$P(a_t^n|s_t^n, \theta)$的意思是：在某一$\tau$过程中，在$t$时刻的所观测的状态$s_t^n$，采取动作$a_t^n$的概率。</p>
<ul>
<li>但$P(a_t^n|s_t^n, \theta)$乘以的是该$\tau$整体的Reward $R(\tau^n)$，而不是当前时刻$t$采取行动后的Reward $R(\tau^n_t)$</li>
<li>所以，训练完毕后的Actor每次采取的动作往往是为了最大化整体的利益，而不是局部的利益。这就可以避免前文提到的问题：打飞机只有开火才能获得Reward，而左移、右移不会有任何Reward，但会对最大化Reward有帮助，所以机器每次做的决策要考虑全局而不是一直开火。</li>
</ul>
</li>
<li><p>$\nabla \log P(a_t^n|s_t^n, \theta)$为什么要取$\log$?分母$P(a_t^n|s_t^n, \theta)$的作用是什么呢？</p>
<script type="math/tex; mode=display">\nabla \log P(a_t^n|s_t^n, \theta) = \frac{\nabla P(a_t^n|s_t^n, \theta)}{P(a_t^n|s_t^n, \theta)}</script><ul>
<li>举个例子，如有以下的$\tau$过程<ul>
<li>$\tau^{13}$，采取行动a，$R(\tau^{13}) = 2$</li>
<li>$\tau^{15}$，采取行动b，$R(\tau^{15}) = 1$</li>
<li>$\tau^{17}$，采取行动b，$R(\tau^{17}) = 1$</li>
<li>$\tau^{33}$，采取行动b，$R(\tau^{33}) = 1$</li>
</ul>
</li>
<li>虽然动作a会有更高的Reward，但由于动作b出现次数很多，在求和所有R之后，b动作带来的总Reward会更高；这就导致机器将b出现的机率调高，以获得总体的Reward更高。故虽然a可以获取更高的Reward，机器也会限制它的发生机率。</li>
</ul>
</li>
<li><p>在看了上述例子之后，我们再回头解释分母$P(a_t^n|s_t^n, \theta)$存在的意义，其实$\frac{\nabla P(a_t^n|s_t^n, \theta)}{P(a_t^n|s_t^n, \theta)}$相当于对各种action出现的次数做了normalize，从而使得机器不会偏好出现次数过高的action，而是综合评价出现次数和单次行动获得的Reward。</p>
</li>
</ul>
<p>通过上述对目标函数直观的解释和理解，我们可以发现目标函数不仅保证了全局利益，也能保证得到做出合理决策的Actor。</p>
<blockquote>
<p>Add a Baseline 1</p>
</blockquote>
<p>若是$R(\tau)^n$一直为正，会发生什么事情？</p>
<ul>
<li><p>理想状态下，不会有什么问题。若有三个动作a、b、c，通过充分的采样，三个动作都发生了，那么三个动作在进行梯度下降的时候都会有相应的提升。</p>
</li>
<li><p>但若采样不充分，导致某个动作没有发生如动作a没有发生，就会使得动作b、c的机率增加，而a的机率减小。所以，我们希望$R(\tau^n)$有正有负，那么可以添加一项$b \approx E(R(\tau^n))$。</p>
<script type="math/tex; mode=display">\nabla \bar{R}_\theta = \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^{T_n} (R(\tau^n) - b)\nabla \log P(a_t^n|s_t^n, \theta)</script></li>
</ul>
<blockquote>
<p>Add a BaseLine 2</p>
</blockquote>
<ul>
<li><p>假设有两个$\tau$的片段如下：</p>
<ul>
<li>$(s_a, a_1, r=5),~(s_b, a_2, r=0),~(s_c, a_3, r=-2), ~ R=3$</li>
<li>$(s_a, a_1, r=-5),~(s_b, a_2, r=0),~(s_c, a_3, r=-2), R=-7$</li>
</ul>
</li>
<li><p>在上面两个例子中，执行$a_2$并不会获得太差的Reward。但在原始的Policy Gradient中，乘上的权重为$R(\tau^n)$，即一场游戏的总Reward，这虽然能考虑全局收益，但同时拉低了某些好的action出现的概率，从而使得当前Actor出现偏好。</p>
</li>
<li><p>如在上述的例子中，$a_2$之前采取的动作其实和$a_2$将产生的结果并没有任何关系，$a_2$之后产生的结果只和$a_2$之后的动作相关。故可以修改Policy Gradient为：</p>
<script type="math/tex; mode=display">\nabla\bar{R_\theta} \approx \frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{T_n}[\sum_{t^{'}=t}^{T_n}r_{t^{'}}^n - b]\nabla logP_\theta(a_t^n|s_t^n)</script><p>即将权重从全局的Reward修改为，采取动作$a_t$之后所有Reward的总和</p>
</li>
<li><p>再进一步，由于当前的action对后续动作的影响会随着时间的推移而逐渐减弱，故需要乘上一个衰减因子$\gamma &lt; 1$，于是式子更新为：</p>
<script type="math/tex; mode=display">\nabla\bar{R_\theta} \approx \frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{T_n}[\sum_{t^{'}=t}^{T_n}\gamma^{t^{'}-t} r_{t^{'}}^n - b]\nabla logP_\theta(a_t^n|s_t^n)</script></li>
<li><p>中间的那一项称之为Advantage Function，即</p>
<script type="math/tex; mode=display">A^\theta(s_t, a_t) = \sum_{t^{'}=t}^{T_n}\gamma^{t^{'}-t} r_{t^{'}}^n - b</script></li>
</ul>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    GodWriter
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://godwriter.github.io/2020/05/06/reinforcement-learning1/" title="Reinforcement Learning 1">https://godwriter.github.io/2020/05/06/reinforcement-learning1/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/05/02/statistic-learning5/" rel="next" title="Statistic Learning 5">
                <i class="fa fa-chevron-left"></i> Statistic Learning 5
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/05/15/statistic-learning6/" rel="prev" title="Statistic Learning 6">
                Statistic Learning 6 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="GodWriter">
            
              <p class="site-author-name" itemprop="name">GodWriter</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-RL几个应用"><span class="nav-text">1. RL几个应用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-RL的概念"><span class="nav-text">2. RL的概念</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-RL三个步骤"><span class="nav-text">3. RL三个步骤</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">GodWriter</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
